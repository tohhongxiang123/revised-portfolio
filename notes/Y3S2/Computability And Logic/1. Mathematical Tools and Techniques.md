# Mathematical Tools and Techniques

# Logic and Proofs

Logic involves **propositions**
- Propositions have truth values (`true`/`false`)
- Examples of propositions: "0 = 1", "Peanut butter is a source of protein"

Propositions can involve a **free variable**, and the value of the proposition will depend on the free variable
- $x - 1$ is prime

Propositions can be combined using **logical connectives**
- Conjunction (p and q)
- Disjunction (p or q)
- Negation (not p)
- Conditional (if p, then q)
- Biconditional (p if and only if q)

A **tautology** is a proposition that is always true ($p \lor \neg p$), a **contradiction** is a proposition that is always false ($p \land \neg p$)

Useful logical identities
- Commutative laws 
    - $p \lor q \iff q \lor p$
    - $p \land q \iff q \land p$
- Associative laws
    - $p \lor (q \lor r) \iff (p \lor q) \lor r$
    - $p \land (q \land r) \iff (p \land q) \land r$
- Distributive laws
    - $p \lor (q \land r) \iff (p \lor q) \land (p \lor r)$
    - $p \land (q \lor r) \iff (p \land q) \lor (q \land r)$
- De Morgan's laws
    - $\neg (p \land q) \iff \neg p \lor \neg q$
    - $\neg (p \lor q) \iff \neg p \land \neg q$
- Involving the conditional and biconditional
    - $(p \implies q) \iff (\neg p \lor q)$ (Implication)
    - $(p \implies q) \iff (\neg q \implies \neg p)$ (Contrapositive)
    - $(p \iff q) \iff (p \implies q) \land (q \implies p)$ (Equivalence)

More logical identities can be found [here](https://www.cs.odu.edu/~toida/nerzic/content/logic/prop_logic/identities/identities.html)

We can attach logical quantifiers to the beginning of propositions. There are 2 logical quantifiers:
- Universal quantifier (for every, $\forall$)
- Existential quantifier (for some, $\exists$)

$$
\forall x (x - 1 \text{ is prime}) \\
\exists x (x - 1 \text{ is prime})
$$

$x$ is no longer a free variable, it is now bound by the logical connective

The negation of logical quantifiers are as follows:
- $\neg (\forall x (P(x))) \iff \exists x (\neg P(x))$
- $\neg (\exists x (P(x))) \iff \forall x (\neg P(x))$

# Sets

A finite set can be described by listing its elements

$$
A = \{1, 2, 3, 4\}
$$

An infinite set can be described with a formula

$$
B = \{x | x \text{ is a nonnegative integer multiple of } 3\}
$$

For any set $A$
- $x \in A$ describes an element $x$ in the set $A$
- $x \not \in A$ describes an element $x$ not in the set $A$
- $A \subseteq B$ describes that $A$ is a subset of $B$
- $A \not \subseteq B$ describes that $A$ is nota  subset of $B$
- $\emptyset$ is the empty set

For 2 sets $A$ and $B$
- $A \cup B = \{ x | x \in A \text{ or } x \in B \}$
- $A \cap B = \{ x | x \in A \text{ and } x \in B \}$
- $A - B = \{ x \in A \text{ and } x \not \in B \}$

If $A$ and $B$ are both subsets of some "universal" set $U$, then

- $A' = U - A = \{ x | x \in U \text{ and } x \not \in A \}$

## Set Identities

- De Morgan's
    - $(A \cup B)' = A' \cap B'$
    - $(A \cap B)' = A' \cup B'$
- Associativity
    - $(A \cup B) \cup C = A \cup (B \cup C)$
    - $(A \cap B) \cap C = A \cap (B \cap C)$


For a set $A$, the set of all subsets of $A$ is called the **power set**, written $2^A$, because if there are $n$ elements in $A$, there are $2^n$ elements in $2^A$

$$
2^{\{a, b, c\}} = \{\empty, \{a\},  \{b\},  \{c\},  \{a, b\}, \{a, c\}, \{b, c\}, \{a, b, c\}\}
$$

From 2 sets $A$ and $B$, the **Cartesian Product** is defined as follows:

$$
A \times B = \{(a, b) \ | \ a \in A \text{ and } b \in B \} \\
\{0, 1\} \times \{1, 2, 3\} = \{(0, 1), (0, 2), (0, 3), (1, 1), (1, 2), (1, 3)\}
$$

Elements in $A \times B$ are called **ordered pairs**

# Languages

An **alphabet** is a finite set of symbols, usually denoted by $\Sigma$
- $\Sigma = \{ 1, 2, 3 \}$
- $\Sigma = \{ A, B, C, \cdots, Z \}$

A **string** over $\Sigma$ is a finite sequence of symbols in $\Sigma$.
- For some string $x$, $|x|$ is the length of the string
- For a string $x$ over $\Sigma$ and some symbol $\sigma \in \Sigma$, $n_\sigma(x) = \text{number of occurences of } \sigma \text{ in string } x$
- The null string $\Lambda$ is a string over $\Sigma$, regardless of what $\Sigma$ is. By definition, $|\Lambda| = 0$

The set of all strings over $\Sigma$ is written as $\Sigma^*$. For the alphabet $\Sigma = \{ a, b \}$,

$$
\Sigma^* = \{ \Lambda, a, b, aa, \cdots \}
$$

A **language** over $\Sigma$ is a subset of $\Sigma^*$. Examples of languages over $\Sigma$:
- The null language $\empty$
- $\{ \Lambda, a, b \}$, another finite language
- $\{x \in \{a,b\}∗ \ | \ n_a(x)>n_b(x)\}$

Consider 2 strings $x$ and $y$ over $\Sigma$. $xy$ is a concatenation of $x$ and $y$
- $x = ab$
- $y = ba$
- $xy = abba$
- $yx = baab$
- $|xy| = |x| + |y|$

Concatenating any arbitrary string with $\Lambda$ is itself: $x\Lambda = \Lambda x = x$

For a string $s = tuv$
- $t$ is a prefix of $s$
- $u$ is a substring of $s$
- $v$ is a suffix of $s$

We can concatenate languages as well. For 2 languages $L_1$ and $L_2$:
- $L_1 L_2 = \{ xy | x \in L_1 \text{ and } y \in L_2 \}$
- $\{a, aa\}\{\Lambda, b, ab\} = \{a, ab, aab, aa, aaab\}$

For concatenation of a symbol $a$, string $x$, or language $L$, if we concatenate multiple times, we use an exponent symbol:
- $a^i$
- $x^j$
- $L^k$

Note that
- $a^0 = x^0 = \Lambda$
- $L^0 = \{ \Lambda \}$

For a language $L$ over an alphabet $\Sigma$, $L^*$ denotes the language of all strings that can be obtained by concatenating 0 or more strings in $L$ (This operation is known as the Kleene star, or Kleene closure)

$$
L^* = \bigcup \{ L^k | k \in \N \}
$$

# Recursive Definitions

Consider the recursive definition of $\{ a, b \}^*$
1. $\Lambda \in \{a, b \}^*$
2. For each $x \in \{ a, b \}^*$, $xa, xb$ are both in $\{a, b \}^*$

Consider the language $Pal$, the language of all palindromes over $\{a, b\}$. We can recursively define $Pal$ by writing:

1. $\Lambda, a, b \in Pal$
2. For every $x \in Pal$, $axa, bxb$ are in $Pal$

Consider the language $Balanced$, the language of balanced strings of parentheses. (For example, "()", "(()(()))", etc). We can recursively define this language with

1. $\Lambda \in Balanced$
2. For every $x$ and every $y$ in $Balanced$, $xy$ is in $Balanced$
3. For every $x \in Balanced$, $(x)$ is in $Balanced$

# Structural Induction

Mathematical induction involves the following steps:
1. Basis ($P(0)$): Prove that the base case is true
2. Induction hypothesis ($P(k)$): Assume that the statement you are trying to prove is true for some value $k$
3. Statement to prove ($P(k + 1)$)
4. Proof ($P(k) \implies P(k + 1)$): Using the induction hypothesis $P(k)$, prove that it leds to $P(k + 1)$

## Example

Consider the language $Expr$, where
1. $a \in Expr$
2. For every $x$ and every $y$ in $Expr$, $x + y$, $x * y$, $(x)$ are in $Expr$

We will prove
- $|a|$ is odd
- For every $x$ and $y$ in $Expr$, if $|x|$ and $|y|$ are odd, then $|x+y|$, $|x∗y|$, and $|(x)|$ are
odd.

The proof:
1. Basis: $|a|$ is odd (by definition of the basis step, $|a| = 1$)
2. Induction hypothesis: Assume, for every $x$ and every $y$ in $Expr$, $|x|$ and $|y|$ is odd
3. Statement to prove: $|x + y|$, $|x * y|$ and $(x)$ are all odd
4. Proof: $|x + y| = (2k + 1) + 1 + (2j + 1) = 2(k + j + 1) + 1$, which is odd. Similarly for $|x * y|$. Also, $|(x)| = 2 + 2k + 1 = 2(k + 1) + 1$, which is odd

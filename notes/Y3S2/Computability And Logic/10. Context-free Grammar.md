# Context Free Grammars: Derivation Trees and Ambiguity

# Inductive Definitions

Inductive definitions define the smallest set satisfying some conditions:

-   $AnBn = \{ a^n b^n \ | \ n \in \mathbb{N} \}$
    -   $\Lambda \in AnBn$
    -   If $w \in AnBn$, then $awb \in AnBn$
-   $Pal = \{ w \in \{a, b\}^* \ | \ w = w^r \}$
    -   $\Lambda, a, b \in Pal$
    -   If $w \in Pal$, $awa \in Pal$ and $bwb \in Pal$

# Examples of Grammar

-   Alphabet: $\{1, 2, 3, +, \times\}$
-   Variables: $\{S, T\}$
-   Expression: $1 + 2 \times 3$

Let us specify the language of all correct expressions

$$
G_3 = \begin{cases}
S \rightarrow T + S \ | \ T \\
T \rightarrow 1 \ | \ 2 \ | \ 3 \ | \ T \times T
\end{cases}
$$

# Context-Free Grammar

Context-Free Grammar (CFG) is a 4-tuple: $\langle V, \Sigma, P, S \rangle$:

-   $V$ is a finite set of variables
-   $\Sigma$ is a finite set of alphabet which is disjunct with $V$ ($V \cap \Sigma = \empty$)
-   $P \subseteq V \times (V \cup \Sigma)^*$ is a finite set of rules (aka productions)
-   $S \in V$ is the start symbol

Note:

-   A rule is a pair $(A, w)$ with $A \in V$ and $w \in (V \cup \Sigma)^*$
    -   If $(A, w_1), (A, w_2), ... \in P$, then $A \rightarrow w_1 \ | \ w_2 \ | \ ...$
    -   Note that $(A, \Lambda) \in P$
-   Covention:
    -   Variables are usually capitalised ($A, B, C, ...$)
    -   Alphabets are usually lower-case ($a, b, c, ...$)

For example:

-   $V = \{S, A \}$, $\Sigma = \{a, b \}$
    -   $S \rightarrow AA$
    -   $A \rightarrow AAA \ | \ bA \ | \ Ab \ | \ a$

## Derivability

When a string $y$ can be derived from a string $x$, we write $x \implies y$

-   $x \implies y$ iff $x = uAv$ and $y = uwv$ for some rule $A \rightarrow w \in P$

Take a CFG $\langle V, \Sigma, P, S \rangle$ and a string $v = (V \cup \Sigma)^*$. The set of derivable strings $Der(v)$ is the smallest set such that

-   $v \in Der(v)$
-   If $u \in Der(v)$ and $u \implies w$, then $w \in Der(v)$

Note that

-   $v \stackrel{*}{\implies} w$: "w is derivable from v"
-   $v \stackrel{n}{\implies} w$: "w is derivable from v in $n$ steps"

## Sentential Forms, Sentences, and Languages

The language of a CFG:

Given a CFG $G = \langle V, \Sigma, P, S \rangle$:

-   If $S \stackrel{*}{\implies} w$ then $w$ is called a sentential form
-   If additionally, $w \in \Sigma^*$, then $w$ is a sentence
-   The language of $G$ is defined as the set of all sentences: $\mathcal{L}(G) = \{ w \in \Sigma^* \ | \ S \stackrel{*}{\implies} w \}$

A set of words $X \subseteq \Sigma^*$ is a context-free language if and only if there exists a CFG $G$ such that $\mathcal{L}(G) = X$

We sometimes call these languages context-free languages (CFL)

## Context-Free Languages that were not regular

1. A number of $a$'s followed by the same number of $b$'s

    $L = \{ a^n b^n \ | \ n \in \mathbb{N} \}$

    $S \rightarrow a S b \ | \ \Lambda$

2. Palindromes over $a, b$

    $L =  \{ w \in {a, b}^âˆ— \ | \ w = w^r \}$

    $S \rightarrow aSa \ | \ bSb \ | \ a \ | \ b \ | \ \Lambda$

# Regular Grammars

A grammar is regular if every rule has on of the following forms (with $A, B \in V$ and $a \in \Sigma$):

-   $A \rightarrow a$
-   $A \rightarrow aB$
-   $A \rightarrow \Lambda$

-   Are all regular languages context-free?
-   Are all context-free languages regular?
-   If we have 2 CFGs $G_1$ and $G_2$, $G_1 \equiv G_2 \iff \mathcal{L}(G_1) = \mathcal{L}(G_2)$. How do we check the equivalence of 2 CFGs?
-   Are all languages context-free?

## Finite Automata to Regular Grammar

Given an NFA $M = (Q, \Sigma, q_0, F, \delta)$, we define $G = (Q, \Sigma, P, q_0)$ with

-   For all $q \in Q$ and $q' \in \delta(q, a)$, we add $(q \rightarrow aq') \in P$
-   For all $q \in F$, add $(q \rightarrow \Lambda) \in P$

Then $\mathcal{L}(G) = \mathcal{L}(M)$

Intuition behind the conversion

-   Every state in the NFA becomes a variable in the CFG
-   Every transition $q \stackrel{a}{\rightarrow} q'$ becomes a rule $q \rightarrow aq'$
-   Every accepting state $q$ becomes a rule $q \rightarrow \Lambda$
-   Paths in the automaton correspond to the derivations

## Regular Grammar to Finite Automata

Given a CFG $G = (V, \Sigma, P, S)$, we define $M = (V \cup \{Z\}, \Sigma, S, F, \delta)$

-   $Z$ is a new special variable not in $V$
-   $B \in \delta(A, a)$ for all $A \rightarrow aB$
-   $Z = \delta(A, a)$ for all $A \rightarrow a$
-   $A \in F$ (final states) for all $A \rightarrow \Lambda$, and $Z \in F$

Then, $\mathcal{L}(M) = \mathcal{L}(G)$

Intuition behind the conversion

-   Every variable of the grammar becomes a state
-   Special accepting state $Z$ for terminal rules
-   Rules $A \rightarrow aB$ become transitions $A \stackrel{a}{\rightarrow} B$
-   Rules $A \rightarrow a$ become transitions $A \stackrel{a}{\rightarrow} Z$ (with $Z \in F$)
-   Rules $A \rightarrow \Lambda$ result in $A \in F$

# Derivation Trees

![Derivation tree](https://www.gatevidyalay.com/wp-content/uploads/2018/08/Example-of-Leftmost-Derivation-Tree.png)

Take a CFG $G = (V, \Sigma, P, S)$ and a derivation $S \stackrel{*}{\implies} w$

The ordered derivation tree of $S \stackrel{*}{\implies} w$ is constructed as follows:

-   Internal nodes are labelled in $V$, leaves are labelled in $\Sigma \cup \{\Lambda\}$
-   Create a root node labelled $S$
-   For a node labelled $A$, corresponding to an application of the rule $A \rightarrow x_1, ..., x_n$ (with all $x_i \in V \cup \Sigma$) in $S \stackrel{*}{\implies} w$:
    -   Create its $n$ children and label them $x_1, ... x_n$
-   At the application of the rule $A \rightarrow \Lambda$, add $\Lambda$ as its only child of $A$

At any time during the construction, the derived word is equal to the symbols of the leaves, in the order of the tree

# Ambiguity

A CFG $G$ is **ambiguous** if there is a word $w \in \mathcal{L}(G)$ with at least **two different derivation trees**

Note

-   Ambiguity is a property of grammars, not of languages
-   A language can have both ambiguous and unambiguous grammar

    $$
    \begin{aligned}
    G_1 &: S \rightarrow aS \ | \ Sa \ | \ a \\
    G_2 &: S \rightarrow aS \ | \ a
    \end{aligned}
    $$

    $\mathcal{L}(G_1) = \mathcal{L}(G_2) = a^+$ holds, but $G_1$ is ambiguous, and $G_2$ is not

-   There are languages that only have ambiguous grammars. For example: $L = \{a^m b^m c^n \ | \ m,n \geq 0\} \cup \{a^m b^n c^n \ | \ m,n \geq 0\}$
-   For regular languages, there always exists unambiguous grammars
-   Programming languages should have unambiguous grammars

## Ambiguity in Natural and Programming Languages

I saw a man on a hill with a telescope

-   I saw (a man on a hill) with a telescope
-   I saw (a man (on a hill) with a telescope)
-   I saw (a man (on a hill with a telescope))

# Verification of Grammar

For some CFG $G$ and some language $X \subseteq \Sigma^*$, is $\mathcal{L}(G) = X$?

To prove that $\mathcal{L}(G) = X$:

1. Prove $X \subseteq \mathcal{L}(G)$
    - For all $w$, $w \in X \implies w \in \mathcal{L}(G)$
    - Prove that $\forall w \in X, S \stackrel{*}{\implies} w$
    - This is shown by constructing every word $w \in X$ using rules from $G$
2. Prove $\mathcal{L}(G) \subseteq X$
    - For all $w$, $w \in \mathcal{L}(G) \implies w \in X$
    - $\forall w \in \mathcal{L}(G), (S \stackrel{*}{\implies} w) \implies (w \in X)$
    - Proof by induction on length $n$
        - Assume $S \stackrel{0}{\implies} w$, and show that $w \in X$
        - Induction hypothesis: Assume that $S \stackrel{k}{\implies} w$ then $w \in X$
        - Show that $S \stackrel{k+1}{\implies} w$ then $w \in X$
            - From the induction hypothesis, $S \stackrel{k}{\implies} v, v \in X$
            - $S \stackrel{k}{\implies} v \implies w$
            - Prove that $w \in X$ using $v \implies w$ and rules from $G$

Both parts done with induction

# Parsing

Checking membership and computing parse trees:

-   Given a grammar $G = (V, \Sigma, P, S)$ and a word $w \in \Sigma^*$,
    -   Is $w \in \mathcal{L}(G)$? (Decision problem)
    -   Compute a derivation tree for $w$ in grammar $G$

## Naive Parsing

-   A parsing of a sentence is the derivation tree of that sentence
-   A parsing algorithm for a grammar is an algorithm that produces a parsing for each sentence

Naive parsing algorithm: Backtracking

1. If the sentential form consists only of $S$, we are done
2. Search the right side of a rule that matches a part of the sentence 1. On success, replace the right side by the corresponding left side, and repeat step 2 2. Else, go back to the previous phase and try the next possibility (backtrack)

E.g.

$$
G = \begin{cases}
S \rightarrow aSb \ | \ a A \\
A \rightarrow aA \ | \ a
\end{cases}
$$

We want to parse $aaab$

$$
\begin{aligned}
aaab & \leftarrow Aaab & \leftarrow AAab & \leftarrow AAAb \\
& & \leftarrow AaAb & \leftarrow AAAb \\
& & & \leftarrow AAb \\
& & & \leftarrow ASb \\
& \leftarrow aAab & \leftarrow AAab \\
& & \leftarrow aAAb & \leftarrow \cdots \\
& \leftarrow aaAb & \leftarrow aSb & \leftarrow S

\end{aligned}
$$

Naive parsing is bad, time complexity is too high (exponential)

# Chomsky Normal Form

A language can have several grammars, some are more suitable for parsing than others

Normal form: Restriction to the right side of the rules

-   Example: Never $aAb$ but always $AB$; never $\Lambda$
-   Simplier to analyse
-   More efficient parsing algorithms

For e.g., regular grammars. However, an equivalent regular grammar does not always exist for all grammars.

Automatic transformation of a grammar to its normal form:

-   Through step-by-step transformation of the grammar
-   The order in which these steps are applied can be important

## Transformation of Grammar

Let $G = (V, \Sigma, P, S)$ be a grammar

1. Adding derivations as "shortcut" rules:

    Let $G_2 = (V, \Sigma, P \cup \{A \rightarrow w \}, S)$. If $A \implies w$ then $\mathcal{L}(G_2) = \mathcal{L}(G)$

2. Inlining rules:

    Suppose $A \rightarrow u B v$ and $B \rightarrow w_1 \ | \ \cdots \ | \ w_n$ in $P$.

    Let $G_3 := (V, \Sigma, P_3, S)$ with $P_3 = (P - \{A \rightarrow uBv \}) \cup (\{ A \rightarrow u w_1 v \ | \ \cdots \ | \ u w_n v \})$

    Then $\mathcal{L}(G_3) = \mathcal{L}(G)$

## Chomsky Normal Form

$G = (V, \Sigma, P, S)$ is in chomsky-normal form if all rules are of one of the following forms:

-   $A \rightarrow BC$ (with $B, C \in V$); or
-   $A \rightarrow a$ (with $a \in \Sigma$)

For example:

$$
G = \begin{cases}
S \rightarrow a S b \ | \ a A \\
A \rightarrow a A \ | \ a
\end{cases}
$$

has an equivalent Chomsky-Normal form:

$$
G' = \begin{cases}
S \rightarrow X_1 X_2 \ | \ X_1 A \\
X_1 \rightarrow a \\
X_2 \rightarrow S X_3 \\
X_3 \rightarrow b \\
A \rightarrow X_1 A \ | \ a
\end{cases}
$$

Advantages of chomsky-normal form:

-   Derivations are non-contracting (words do not get shorter)
-   Derivation of $w$ at most $2n - 1$ steps (where $n = |w|$)
-   Checking whether $w \in \mathcal{L}(G)$ is decidable
-   Derivation tree of $w$ is a binary tree, with a depth of at most $n$
-   Efficient parsing algorithm (CYK):
    -   Polynomial in $|w|$ for fixed $G$: $O(n^3)$ for $|w| = n$

To convert a grammar to chomsky-normal form:

1. Eliminate $\Lambda$-rules
2. Eliminate chain-rules ($A \rightarrow B$)
3. Finalize rules $A \rightarrow w$ with $|w| \geq 2$ and $w \neq BC$
4. Eliminate non-terminating variables
5. Eliminate unreachable variables

# Conversion of Grammar to Chomsky-Normal Form

## 1. Non-contracting grammar - Eliminate null-rules

Grammar with null-rule:

$$
G = \begin{cases}
S \rightarrow SaB \ | \ aB \\
B \rightarrow bB \ | \ \Lambda
\end{cases}
$$

Equivalent grammar without null-rule:

$$
G = \begin{cases}
S \rightarrow SaB \ | \ Sa \ | \ aB \ | \ a \\
B \rightarrow bB \ | \ b
\end{cases}
$$

Transformation idea:

1. Collect all variables $Null$ with a derivation to $\Lambda$
2. For all rules $A \rightarrow w$:
    - If $w$ is equal to $w_1 A_1 w_2 A_2 w_3 A_3 \dots w_k A_k w_{k+1}$ with $A_i \in Null$, add a rule $A \rightarrow w_1 w_2 \dots w_k w_{k+1}$
3. Remove all null-rules

### 1.1 Collecting All Null-Rules

1. Initialise a set `null` where $null = \{ X \in V \ | \ (X \rightarrow \Lambda) \in P \}$
    - Any variable with $A \rightarrow \Lambda$ is added to `null`
2. Initialise an empty set `prev = {}` set to keep track of changes to `null`
3. For all variables $A$, if $A \rightarrow w$ and $w \in null^*$, add $A$ into `null`

$$
G = \begin{cases}
S \rightarrow A C A \\
A \rightarrow a A a \ | \ B \ | \ C \\
B \rightarrow b B \ | \ b \\
C \rightarrow c C \ | \ \Lambda
\end{cases}
$$

-   Initially, the `null` set contains only $\{C\}$, since $C \rightarrow \Lambda$
-   We check for any rule containing $C$. We can see that $A \rightarrow C$ and $S \rightarrow ACA$, hence both of them are added to `null`
-   There is no more expansion, no other rules contain some combination of $\{A, C, S\}$

### 1.2 Remove all Combinations of "Nullable" Variables

Continuing from above, we remove all combinations of variables $\{A, C, S\}$

$$
G' = \begin{cases}
S \rightarrow \Lambda \ | \ A \ | \ C \ | \ A C \ | \ C A \ | \ A A \ | \ A C A \\
A \rightarrow a a \ | \ a A a \ | \ B \ | \ Î› \ | \ C \\
B \rightarrow b B \ | \ b \\
C \rightarrow c \ | \ c C \ | \ \Lambda
\end{cases}
$$

### 1.3 Remove the $\Lambda$-Rules

We remove any rule $A \rightarrow \Lambda$

$$
G'' = \begin{cases}
S \rightarrow A \ | \ C \ | \ A C \ | \ C A \ | \ A A \ | \ A C A \\
A \rightarrow a a \ | \ a A a \ | \ B \ | \ Î› \ | \ C \\
B \rightarrow b B \ | \ b \\
C \rightarrow c \ | \ c C
\end{cases}
$$

Note that $\mathcal{L}(G'') = \mathcal{L}(G) - \{ \Lambda 
\}$

## 2. Eliminate Chain-Rules

For the grammar $G_1$

$$
G_1 = \begin{cases}
S \rightarrow A \ | \ aB \\
A \rightarrow Ab \ | \ B \\
B \rightarrow AA \ | \ b
\end{cases}
$$

Chain rules are of the form $A \rightarrow B$, where $A, B \in V, A \neq B$. In $G_1$, the chain rules are:

-   $S \rightarrow A$
-   $A \rightarrow B$

The equivalent grammar to $G_1$ without chain rules:

$$
G_1' = \begin{cases}
S \rightarrow Ab \ | \ AA \ | \ b \ | \ aB \\
A \rightarrow Ab \ | \ AA \ | \ b \\
B \rightarrow AA \ | \ b
\end{cases}
$$

Transformation idea:

1. Assume that the grammar is non-contracting (No $\Lambda$-rules)
2. Collect all reachable variables $\text{Chain}(A) = \{ B \in V \ | \ A \stackrel{*}{\implies} B \}$
    - Note that $A \in \text{Chain}(A)$ because $A \stackrel{0}{\implies} A$
3. Create a rule $A \rightarrow w$ for all $B \in \text{Chain}(A)$ and $B \rightarrow w$ ($w \not \in V$)

### 2.1 Collecting all Reachable Variables

Continuing our example from $G''$:

$$
G'' = \begin{cases}
S \rightarrow A \ | \ C \ | \ A C \ | \ C A \ | \ A A \ | \ A C A \\
A \rightarrow a a \ | \ a A a \ | \ B \ | \ Î› \ | \ C \\
B \rightarrow b B \ | \ b \\
C \rightarrow c \ | \ c C
\end{cases}
$$

We can see that

-   $\text{Chain}(S) = \{ S, A, B, C \}$
-   $\text{Chain}(A) = \{ A, B, C \}$
-   $\text{Chain}(B) = \{ B \}$
-   $\text{Chain}(C) = \{ C \}$

### 2.2 Creating New Rules

Introduce a new rule $A \rightarrow w$ for all $B \in \text{Chain}(A)$ and $B \rightarrow w$ , with $w \not \in V$

Consider the rules for $S$. We know $\text{Chain}(S) = \{ S, A, B, C \}$

-   For $S$ which is the first element of $\text{Chain}(S)$, we can see that $S \rightarrow A C \ | \ C A \ | \ A A \ | \ A C A$. These are all rules of the form $B \rightarrow w, w \not \in V$
-   For $A$ which is the second element of $\text{Chain}(S)$, we can see that $A \rightarrow aa \ | \ aAa$. Hence we add to the rules for $S$:
    -   $S \rightarrow A C \ | \ C A \ | \ A A \ | \ A C A \ | \ aa \ | \ aAa$
-   For $B$ which is the third element of $\text{Chain}(S)$, we have $B \rightarrow b B \ | \ b$. We add these rules again to $S$
    -   $S \rightarrow A C \ | \ C A \ | \ A A \ | \ A C A \ | \ aa \ | \ aAa \ | \ b B \ | \ b$
-   For $C$ which is the last element of $\text{Chain}(S)$, we have $C \rightarrow c \ | \ c C$. Hence, $S$ is finally:
    -   $S \rightarrow A C \ | \ C A \ | \ A A \ | \ A C A \ | \ aa \ | \ aAa \ | \ b B \ | \ b \ | \ c \ | \ c C$

Repeat these steps for the remaining rules, and we get a new $G'''$

$$
G''' = \begin{cases}
S \rightarrow A C \ | \ C A \ | \ A A \ | \ A C A \ | \ a a \ | \ a A a \ | \ b B \ | \ b \ | \ c \ | \ c C \\
A \rightarrow a a \ | \ a A a \ | \ b B \ | \ b \ | \ c \ | \ c C \\
B \rightarrow b B \ | \ b \\
C \rightarrow c \ | \ c C
\end{cases}
$$

## 3. Finalising Right-hand Sides

Transformation idea: Replace each rule $A \rightarrow w$ with $|w| \geq 2$:

1. Suppose that $w = a_1 a_2 ... a_n$ with $a_i \in V \cup \Sigma$ for all $i \in [1, n]$ and
    - For $a_i \in V$: name $D_i = a_i$
    - For $a_i \in \Sigma$: choose a new $D_i$, and define $D_i = a_i$
2. Name $C_1 = A$ and choose new variables $C_2, C_3, ..., C_{n-1}$
3. Define $C_i \rightarrow D_i C_{i+1}$ for $i \in [1, n - 2]$ and $C_{n-1} = D_{n-1}D_n$

This is saying

-   Eliminate terminals from RHS if they exist with any other terminals or non-terminals. Any rule $A \rightarrow wB$, where $w \in \Sigma, B \in V$ can be decomposed into
    -   $A \rightarrow XB, X \rightarrow w$
-   Eliminate RHS with more than 2 non-terminals. Any rule $A \rightarrow BCD$ can be decomposed into:
    -   $A \rightarrow XD, X \rightarrow BC$

Continuing from $G'''$

$$
G''' = \begin{cases}
S \rightarrow A C \ | \ C A \ | \ A A \ | \ A C A \ | \ a a \ | \ a A a \ | \ b B \ | \ b \ | \ c \ | \ c C \\
A \rightarrow a a \ | \ a A a \ | \ b B \ | \ b \ | \ c \ | \ c C \\
B \rightarrow b B \ | \ b \\
C \rightarrow c \ | \ c C
\end{cases}
$$

Let us create $T_1 \rightarrow a, T_2 \rightarrow b, T_3 \rightarrow c$. Then, replace terminals that exist with non-terminals in $G'''$ appropriately

$$
G''' = \begin{cases}
S \rightarrow A C \ | \ C A \ | \ A A \ | \ A C A \ | \ T_1 T_1 \ | \ T_1 A T_1 \ | \ T_2 B \ | \ b \ | \ c \ | \ T_3 C \\
A \rightarrow T_1 T_1 \ | \ T_1 A T_1 \ | \ T_2 B \ | \ b \ | \ c \ | \ T_3 C \\
B \rightarrow T_2 B \ | \ b \\
C \rightarrow c \ | \ T_3 C \\
T_1 \rightarrow a \\
T_2 \rightarrow b \\
T_3 \rightarrow c
\end{cases}
$$

Now we replace any non-terminal mapping to more than 2 non-terminals

$$
G''' = \begin{cases}
S \rightarrow A C \ | \ C A \ | \ A A \ | \ D_1 A \ | \ T_1 T_1 \ | \ D_2 T_1 \ | \ T_2 B \ | \ b \ | \ c \ | \ T_3 C \\
D_1 \rightarrow AC \\
D_2 \rightarrow T_1 A \\
A \rightarrow T_1 T_1 \ | \ D_2 T_1 \ | \ T_2 B \ | \ b \ | \ c \ | \ T_3 C \\
B \rightarrow T_2 B \ | \ b \\
C \rightarrow c \ | \ T_3 C \\
T_1 \rightarrow a \\
T_2 \rightarrow b \\
T_3 \rightarrow c
\end{cases}
$$

# Construction of Chomsky-Normal Form

For every grammar $G$ there exists a grammar $G'$ which is in Chomsky-Normal form such that $L(G') = L(G) - \{ \Lambda \}$

## Optimisations

### 1. Remove non-terminating variables

Consider

$$
G = \begin{cases}
S \rightarrow AC \ | \ BS \ | \ B \\
A \rightarrow aA \ | \ aF \\
B \rightarrow CF \ | \ b \\
C \rightarrow cC \ | \ D \\
D \rightarrow aD \ | \ BD \ | \ C \\
E \rightarrow aA \ | \ BSA \\
F \rightarrow bD \ | \ b
\end{cases}
$$

Note that $C$ and $D$ never terminate (will never reach a terminal). Hence, we remove them, and any production rule that contains them

$$
G' = \begin{cases}
S \rightarrow BS \ | \ B \\
A \rightarrow aA \ | \ aF \\
B \rightarrow b \\
E \rightarrow aA \ | \ BSA \\
F \rightarrow b
\end{cases}
$$

## 2. Remove Non-Reachable Variables

From the above, $A, E, F$ are not reachable from $S$. Hence, we remove them

$$
G'' = \begin{cases}
S \rightarrow BS \ | \ B \\
B \rightarrow b
\end{cases}
$$

Algorithm to detect termination (backwards):

1. Initialise a set `term`, where $term := \{ X \in V \ | \ \exists w \in \Sigma^*: (X \rightarrow w) \in P \}$ (All variables which map to a terminal word)
    - Obviously, all variables that end with a terminal word can terminate
2. Initialise an empty set `prev` ($prev = \empty$)
3. while `term != prev`
    - `prev = term`
    - $term = term \cup \{ X \in V \ | \ \exists w \in (term \cup \Sigma)^*: (X \rightarrow w) \in P \}$

Algorithm to detect reachability (forwards):

1. $reach = \{ S \}$
2. $new = \{ S \}$
3. while $new \neq \empty$
    - $prev = reach$
    - $reach = reach \cup \{ Z \in V \ | \ \exists Y \in new : (Y â†’ w) \in P, Z âˆˆ var(w) \}$
    - $new = reach - prev$

Clean up $V - term$, then $V' - reach$

# CYK Parsing (Cocke-Younger-Kasami)

Starting point: A grammar $G$ is CNF, and a word $w$

-   Let $w = a_1 ... a_n$ where $n \geq 0$ and $a_i \in \Sigma$
-   Notation: A subword $w_{ij} = a_i ... a_j$ with $i \leq i \leq j \leq n$
-   We will compute $X_{ij} = \{ V \ | \ V \stackrel{*}{\implies} w_{ij}\}$
-   If $S \in X_{1n}$ then $S \stackrel{*}{\implies} w_{1n} = w$, so $w \in \mathcal{L}(G)$

Basic recursive idea:

-   If $i = j$, then $X \stackrel{*}{\implies} w_{ij} \iff X \rightarrow a_i$ is a rule
-   If $i < j$
    -   $X \stackrel{*}{\implies} w_{ij} \iff$ for some $k, X \rightarrow AB$ and $A \stackrel{*}{\implies} w_{ik}$ and $B \stackrel{*}{\implies} w_{(k+1)j}$

```
for len in [1..n]:
	for i in [1..n-len+1]:
		j := i + len - 1
		if len == 1:
			x[i,j] := {A if A -> a_i in P}
		else:
			x[i,j] := {}
			for k in [i..j-1]:
				x[i,j] = x[i,j] + {A if B in X[i, k] and C in X[k+1, j], A -> BC in P}
```

# Resources

-   [Generating Grammars](https://stackoverflow.com/questions/15126824/tips-for-creating-context-free-grammar)
-   [Non-context-free grammars](https://en.wikipedia.org/wiki/Context-free_grammar#Examples_of_languages_that_are_not_context_free)
-   [Order of precedence of operators in a Grammar](https://stackoverflow.com/questions/26471876/how-to-tell-the-precedence-of-operators-in-a-context-free-grammar)

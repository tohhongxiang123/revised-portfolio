# Data Mining

- Discover new information in terms of patterns or rules from vast amounts of data
- Process of finding interesting structures in data through applications of various algorithms

# Types of Discovered Knowledge

- Association rules
- Classification models and predictions
- Sequential patterns
- Clustering: Grouping related objects together

E.g., Basket data analysis

- When users shop, what are items that are commonly grouped together?

# Rule Measures: Support and Confidence

We would like to express how interesting rules are:

- Support: Percentage of transactions that contain all items in the rule (Prevalence)
- Confidence: Fraction of transactions that contain RHS items of the rule, to transactions that contain LHS items of the rule (Strength)

# Mining Frequent ItemSets

We want to find all rules that exceed 2 user specified thresholds:

- Minimum support
- Minimum confidence

We do so by finding items that exceed minimum support threshold first

- From these we generate later rules

Naive frequent itemset mining algorithm

- Count the frequency for all possible subsets of items $I$ in the database
- Too expensive, $2^{|I|}$ possible subsets

The apriori principle (monotonicity)

- Any subset of a frequent itemset must be frequent
- E.g. if $\{\text{butter, milk}\}$ is frequent, then $\{\text{butter}\}$ is frequent as well
- Use the opposite to exclude possible sets: Any set that is not frequent cannot be the subset of a frequent one (downward closure)

# Apriori Algorithm

Concept on apriori principle:

1. Count the $1$-itemsets, then $2$-itemssets, etc.
   - $L_k$: Frequent itemssets of size $k$
2. When counting $k+1$-itemsets, only consider those $k+1$-itemsets where all subsets of length $k$ are found to be frequent in the previous steps
   - Else, cannot be frequent (due to downward closure)

Algorithm:

1. $L_k$: Itemsets of length $k$, $C_k$: Candidate itemset of length $k$
2. Scan the database to produce $L_1$ itemset
3. Repeat:
   1. Join frequent itemsets to produce candidates of length increased by 1
      - Frequent $k-1$ itemsets $p$ and $q$ are joined, if they share the same first $k-2$ items
      - E.g. $\{\text{butter, sugar}\}$, $\{\text{butter, milk}\}$ are combined to form $\{\text{butter, sugar, milk}\}$
   2. Prune candidate itemset which contain a non-frequent $k-1$-itemset ($s \not \in L_{k-1}$)
      - E.g. If $\{\text{salt, sugar}\}$ is not frequent, then we discard $\{\text{butter, salt, sugar}\}$
   3. Repeat until no more self-joins are possible

# Generating Rules from Frequent Itemsets

For each frequent itemset $X$

- For each subset $A$ of $X$, form a rule $A \to X - A$
- Delete those rules that do not satisfy the minimum confidence

Computation of confidence of rule $A \to X - A$:

$$
\text{confidence}(A \to (X - A)) = \frac{\text{support}(X)}{\text{support}(A)}
$$

# Clustering

Class labels are unknown, group objects into sub-groups

- Similarity function used to measure similarity between objects
- Objective: Maximise intra-class similarity, minimise inter-class similarity

Clustering is an unsupervised algorithm

## K-Means Clustering

Objective: For some given $k$, form $k$-groups so that the sum of their square distances between the mean of the groups and the element is minimal

Algorithm:

1. Initialize: Partition objects into $k$ nonempty subsets
2. Compute centroids of each cluster of the current partition (Centroid is the mean point of each cluster)
3. Assign each object to the cluster with the nearest representative
4. Go back to step 2 and repeat, until representatives stop changing

## Basic Notation for K-Means Clustering

- Objects $x$ are points in a $d$-dimensional vector space
- Centroid $\mu_C$ is the mean of all points in a cluster $C$:

  $$
      \mu_C = \frac{1}{|C|} \sum_{x_i \in C} x_i
  $$

- Measure of "compactness" (Total distance) of a cluster $C_i$:
  $$
      TD(C_i) = \sqrt{\sum_{x \in C_i} \text{dist}(x, \mu_{C_i})^2}
  $$
  - Distance of each point in the cluster $x$, from the centroid of the cluster $\mu_{C_i}$
  - Sum up all the square distances, and then square root (Similar to mean-square error)
- Measure for the compactness of a whole clustering:
  $$
      TD = \sqrt{\sum_{C_i \in C} TD^2(C_i)}
  $$

# Classification

Having a model learn to be able to describe different classes of data

- Supervised, as the classes have to be predetermined
- Class labels are known for a small set of "training" data, and use model to predict new unseen data

## Decision Trees

Decision trees are flowchart-like models

- Nodes are attributes, branches are outcomes, leaves are class predictions

Idea: Make use of training data

- Historical data which we know the outcome of

To build the tree

- Start from top-down at the root
  - Group data according to values from different attributes
  - Attribute at the root should be the one that separates training data the best
  - Greedily reduce our uncertainty about class outcome

Training examples $T$ recursively partitioned into $T_1, ..., T_n$

- Entropy for $k$ classes with frequency $p_i$

  $$
  \text{entropy}(T) = - \sum_{i=1}^{k} p_i \log_2(p_i)
  $$

- Compute information gain of a split using an attribute, such as humidity, by comparing entropy before the split, and entropy after the split
- Always pick the split that reduces uncertainty the most (Highest information gain)

  $$
      \text{InformationGain(T, A)} = \text{entropy}(T) - \sum_{i=1}^{m} \frac{|T_i|}{|T|}\text{entropy}(T_i)
  $$

## Evaluation of Decision Trees

Accuracy

- Predict class labels of each object $o$
- Determine fraction of correctly predicted labels: $A = \frac{\text{count(correct)}}{\text{count(all)}}$

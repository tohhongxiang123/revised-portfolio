# Concurrency Control and Recovery

# Multiple Granularity Locking

A lockable unit of data defines its granularity

-   Granularity can be coarse (Entire database) or fine (an attribute of a relation)

Data item granularity significantly impacts concurrency control performance

-   Concurrency is low for coarse granularity
-   Concurrency is high for fine granularity

## Lock Hierarchy

There are 5 locks for multiple granularity control

1. Read (Shared) lock: Indicates that this item is being read
2. Write (Exclusive) lock: Indicates that this item is being written
3. Intention-Shared (IS) lock: Indicates that a shared lock will be requested by some descendant node(s)
4. Intention-Exclusive (IX) lock: Indicates that an exclusive lock will be requested by some descendant nodes(s)
5. Shared-Intention-Exclusive (SIX): Indicates that current node is locked in shared mode, and exclusive locks will be requested on some descendant node(s)

## Lock Compatibility

| ""  | IS  | IX  | S   | SIX | X   |
| --- | --- | --- | --- | --- | --- |
| IS  | y   | y   | y   | y   | n   |
| IX  | y   | y   | n   | n   | n   |
| S   | y   | n   | y   | n   | n   |
| SIX | y   | n   | n   | n   | n   |
| X   | n   | n   | n   | n   | n   |

Note

-   IS and IS are compatible, because multiple child nodes can be shared locked from different transactions
-   IS and IX are compatible, because multiple child nodes can be read or written at the same time
-   IX and IX are compatible, because multiple child nodes can be written to at the same time
-   S and IS are compatible, because a node itself can be read, while its children are being read too (and vice versa)
-   S and IX are not compatible. Imagine a row-level granularity, and its children attribute level granularity. If we edit the attributes while being able to read the row, then we would have performed a dirty read. This is actually solved using the SIX lock
-   S and S are compatible, because a row can be read by multiple transactions at the same time
-   IS and SIX are compatible, similar to how IS and IX are
-   IX and SIX are incompatible, similar to how S and IX are incompatible.
-   S and SIX are incompatible, similar to how S and IX are incompatible
-   SIX and SIX are incompatible, similar to how S and IX are incompatible
-   X is incompatible with every other lock, as it is meant to be exclusive

## Producing Serializable Schedules

The set of rules to follow to generate a serializable schedule with multiple granularity locking:

1. Lock compatibility must be adhered to
2. Root of tree must be locked first in any mode
3. Node $N$ can be locked by transaction $T$ in S or IS mode only if the parent node of $N$ is already locked by $T$ in IS or IX
4. Node $N$ can be locked by transaction $T$ in X, IX or SIX mode only if the parent of $N$ is already locked by $T$ in either IX or SIX mode
5. $T$ can only lock a node if it has not unlocked another node (enforce growing/shrinking phase from 2PL)
6. $T$ can only unlock a node $N$ only if none of the children of $N$ are currently locked by $T$

# Validation (Optimistic) Concurrency Control

Perform operations optimistically (Go ahead and do the work, assume everything will work out), but on local copies only. Before committing, check if there are any potential issues with committing. If committing causes issues, abort and restart. If committing does not cause issues, then write changes to the database

In this technique, serializability is only checked at commit time. Transactions are aborted in case of non-serializable schedules

There are 3 phases to validation concurrency control

1. Read phase (where work happens, both read and write)
2. Validation phase (where serializability is checked)
3. Write phase (where local changes are fully committed and written to the database if validation succeeds)

## Validation Phase

-   Each transaction is assigned a timestamp at the beginning of its validation phase
-   Check the timestamp ordering with all other running transaction
-   If $TS(T_i) < TS(T_j)$, then one of the following 3 conditions must hold:
    1. $T_i$ completes all 3 phases (read, validate, write) before $T_j$ begins
    2. $T_j$ completes before $T_i$ starts its write phase, and $T_j$ does not write to any object read by $T_i$
        - $write_set(T_j) \cap read_set(T_i) = \emptyset$
        - This is to ensure any reads by $T_i$ are not dirty, or prevent cascading rollbacks
    3. $T_j$ completes its read phase before $T_i$ completes its read phase, and $T_j$ does not write to any object read/written by $T_i$
        - $write_set(T_j) \cap read_set(T_i) = \emptyset$
        - $write_set(T_j) \cap write_set(T_i) = \emptyset$

## Validation Strategy

When validating $T_i$,

-   For each $T_j$ in committed or validation phase,
    -   Check condition (1) first, because it is the easiest condition to check
    -   If (1) is false, then check (2)
    -   If (2) is false, then check (3)
-   If none of (1), (2), (3) holds, the validation fails, and $T_i$ is aborted

The 3 conditions all try to check to see if the transactions worked in a conflict-free manner

-   Check if each transaction read/write from the same variable in the database caused a conflict
-   If there is a conflict, then serializability is not guaranteed

## Validation CC Implementation

-   Record read and write set while each transaction is running, and write results into a private workspace (similar to "local" scope)
-   Execute validation and write phase inside a protected critical section at the end of the transaction's lifetime
-   Optimistic CC works well when the number of conflicts is low
    -   Large databases with balanced workflows, low probability of conflict, hence locking is wasteful
-   High overhead from copying data locally
-   Validation/Write phase bottlenecks
-   Aborts are more wasteful than locks, because they only occur after the entire transaction is executed

# Snapshot Isolation

-   Transactions see data items based on committed values of items in the database snapshot (the current database state) when the transaction begins
-   Any changes after the transaction begins is not seen by the current transaction (transaction snapshot is local)
-   No read locks, only write locks
-   Writes create an older version in a temporary version store (tempstore) along with its creation timestamp
    -   Other transactions that started before the current transaction can read the respective older version
    -   Usually implemented as pointers from item to list of versions
-   No phantom reads, dirty reads or non-repeatable reads, because only committed values are seen
    -   Rare anomalies may still occur (with extremely low probability)
        -   These anomalies are very complex to detect
        -   Usually these issues are ignored, resolved in program (albeit cumbersome), or we use a variant of snapshot isolation (Serializable snapshot isolation, used in postgres)
    -   Tradeoff between rare anomalies and runtime performance

# Recovery

What should a transaction do after an error occurs?

-   Repeating the execution of a transaction may lead to incorrect results
-   Not repeating the execution of a transaction may also lead to incorrect results

## Log-Based Recovery

General idea: Keep track of **modifications** to data (Only care about writes)

-   Log manager to record important events in logs
    -   When transaction $T$ starts, then `<T start>` written to logs
    -   When $T$ modifies data by `write`, `<T, X, old: V, new: W>` written to logs
        -   `T`: Transaction ID
        -   `X`: Data object modified
        -   `V`: Original value
        -   `W`: Updated value
    -   Important: Logs must be stored in stable storage. If not, when the program crashes, all the logs are lost as well. This defeats the purpose of logging
-   When $T$ reaches the last statement, `<T commits>` written to logs
    -   Commit precisely when commit entry output to log

## Buffering of Log Entries and Data Items

We assumed so far that logs are only put into stable storage. However, it is possible to have logs stored in cache, for better performance. We achieve this by imposing some restrictions:

-   Transaction $T$ cannot commit before `<T commits>` is written to stable storage
-   `<T commits>` cannot be written to stable storage before all other entries pertaining to $T$ are put into stable storage
-   A block of data items cannot be outputted to stable storage, until all log entries pertaining to these data items are outputted into stable storage as well

## Recovery from Failures

When failure occurs, use the logs to execute the following operations:

-   Undo: Restore database to state prior execution
    -   E.g. aborted transactions, active transactions at the time of crash
-   Redo: Perform database changes again
    -   E.g. committed transaction whose data has not been written to the database

In the case of a failure:

1. Redo all transactions which logged both `start` **and** `committed`
2. Undo all transactions with `start` but not `committed`

Remarks:

-   In a multi-tasking system, more than 1 transaction may need to be undone
-   If a system crashes during recovery, the new recovery must still give the correct results (idempotent)
-   In this algorithm, a large amount of transactions need to be redone, since we do not know what data items are actually on the disk

## Undo/Redo (Immediate Database Modification)

Maximise efficiency during normal operation

-   Some extra work required during recovery

Allows maximum flexibility of buffer manager

-   Database outputs are entirely asynchronous, other than having to happen after the corresponding log entry

More complex at recovery time

-   Must implement undo and redo

Note: Requires before and after images in logs or only after images if an initial before image is written before first write

## No-Undo/Redo (Deferred Database Modification)

Algorithm:

-   Do not output values to disk until commit log entry is on stable storage
-   All writes will go to logs or database cache
-   Sometimes after commits, cached values are outputted to disk

Advantages:

-   Faster recovery, since no undo is required
-   No before images required in logs

Disadvantages:

-   Database outputs must wait
-   A lot of extra work at commit time

## Undo/No-redo

Algorithm:

-   All changed data items output to a disk before committing
    -   Requires that write entries first be output to a stable log
-   At commit:
    -   Output (flush) all changed data items to cache
    -   Add commit entry to log

Advantages:

-   No after images required in the logs
-   No transactions need to be redone

Disadvantages:

-   Hot spot data requires a flush for each committed write
    -   Implies a lot of I/O traffic

## No-Undo/No-Redo

Algorithm

-   No-undo: Do not change database during a transaction
-   No-redo: On commit, write changes to database in a single atomic action

Advantages:

-   Recovery is instantaneous
-   No recovery code needs to be written

Atomic database writes of many pages is accomplished using "shadow-paging" technique

-   Create a "shadow page" where we copy all the data of a page. The "master" page still points to the original copy of data
-   During the transaction, we will write updates to the new page
-   When transaction is committed, we point the "master" from the old existing page to the new shadow page. Now master is the new page, and the old page can be deleted

For no-undo/no-redo,

-   Commits require writing on disks of 1 bit
-   Restart is very fast: Only 1 disk read

Disadvantages

-   Access to stable storage is indirect (though directories may possibly be stored in main memory)
-   Garbage collection of stable storage is required
-   Original layout of data is destroyed
-   Concurrent transactions are difficult to support

## Checkpointing

Checkpointing speeds up recovery, by flushing dirty pages to disk

-   During the execution in addition to activities of previous methods, periodically perform checkpointing
    1. Output log buffers to the log
    2. Force database buffers to disk
    3. Output an entry `<checkpoint>` on the log
-   During recovery
    1. Undo all transactions that are not yet committed
    2. Redo all transactions that have committed after the checkpoint

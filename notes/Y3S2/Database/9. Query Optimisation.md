# Query Optimisation

# Using Heuristics for Query Optimisation

Process

1. Parser for high-level query generates an initial internal representation
2. Apply heuristic rules to optimise internal representation
3. Query execution plan generated

The main heuristic is to apply first the operations that reduce the size of intermediate results

- Apply `SELECT` and `PROJECT` operations before the `JOIN` or other binary operations

# Query Trees

A tree data structure corresponding to the relational algebra expressoin

- Input relations as leaf nodes
- Operations as internal nodes

Execution of query tree

- Executing an internal node operation whenever its operands are available, then replacing that internal node by the relation that results from executing the operation
- Apply selection early to reduce size of intermediate results
- Apply more restrictive selections early (selections that reduce the size of the result the most)
- Apply projections as early as possible

# Outline of Heuristic Algebraic Optimisation Algorithm

1. Break up any select operation with conjunctive conditions in to a cascade
   - $\sigma_{c_1 \land c_2 \land ... \land c_n}(R) = \sigma_{c_1} (\sigma_{c_2} (... (\sigma_{c_n}(R))))$
2. Move each select operation as far down the query tree as possible, permitted by attributes involved in the select condition
3. Rearrange leaf nodes so that leaf node relations with the most restrictive select operation executed first
4. Transform cartesian product with subsequent select operation into join
5. Break down and move lists of projection attributes down the tree as far as possible by creating new projection operations as required
6. Identify subtrees that represent group of operations that can be executed by a single operation

# General Transformation Rules

1. Cascading $\sigma$: Break up conjunctive selection to sequence of $\sigma$: $\sigma_{c_1 \land c_2 \land ... \land c_n}(R) = \sigma_{c_1} (\sigma_{c_2} (... (\sigma_{c_n}(R))))$
2. Commutativity of $\sigma$: $\sigma_{c_1}(\sigma_{c_2}(R)) = \sigma_{c_2}(\sigma_{c_1}(R))$
3. Cascade of $\pi$: In cascades of $\pi$, all except the last one can be ignored: $\pi_{c_1} (\pi_{c_2} (... (\pi_{c_n}(R)))) = \pi_{c_1} (R)$
4. Commuting $\sigma$ and $\pi$: If selection condition only involves attributes in projection list, the 2 operations can be commuted: $\pi_{A_1, ..., A_n} (\sigma_C (R)) = \sigma_C (\pi_{A_1, ..., A_n}(R))$
5. Commutativity of $\bowtie$ and $\times$: $R \bowtie_C S = S \bowtie_C R, R \times S = S \times R$
6. Commuting $\sigma$ with $\bowtie$: If all attributes in selection involve only attributes of 1 condition, the 2 operations can be commuted: $\sigma_C (R \bowtie S) = (\sigma_C (R) \bowtie S)$
   - If selection condition can be split up such that one involves only attributes of $R$, while the other involves only attributes of $S$: $\sigma_C (R \bowtie S) = (\sigma_{C_1} (R) \bowtie \sigma_{C_2}(S))$
7. Commuting $\pi$ with $\bowtie$: If $L = \{A_1, ..., A_n, B_1, ..., B_m \}$ where $A_i$ are attributes of $R$, and $B_i$ are attributes of $S$. If join condition $C$ involves only attributes in $L$, then the 2 operations can be commuted: $\pi_L (R \bowtie_C S) = \pi_{A_1, ..., A_n} (R) \bowtie_C \pi_{B_1, ..., B_m} (S)$
   - If join condition contains additional attributes not in $L$, these must be added to the projection list, and a final $\pi$ operation is required
8. Set operations $\cup$ and $\cap$ are commutative, but $-$ is not
9. $\bowtie, \times, \cup, \cap$ are associative: $(R \theta S) \theta T = R \theta (S \theta T)$
10. Commuting $\sigma$ with set operations: If $\theta \in \{ -, \cup, \cap \}$, $\sigma$ is commutative: $\sigma_C (R \theta S) = \sigma_C (R) \theta \sigma_C(S)$
11. $\pi$ is commutative with $\cup$: $\pi_L (R \cup S) = (\pi_L(R)) \cup (\pi_L(S))$
12. If the condition $c$ of a $\sigma$ that follows a $\times$ is a join condition, we can convert $\times$ to $\bowtie_C$: $\sigma_C (R \times S) = R \bowtie_C S$

# Cost Based Query Optimisation

- Estimate and compare costs of executing a query with different strategies, and choose the strategy with the lowest cost estimate
- Can "learn" from data stored in database
  - How large is each relation?
  - What is the selectivity of each operation?
- However, need to consider a large amount of strategies
- Need to define a cost function

## Approach in Cost-Based Optimisation

- Transformations to generate multiple candidate query trees from initial tree
- Statistics on the input of each operator required
  - Statistics of leaf relations stored in catalog
  - Statistics of intermediate results must be estimated; most important is the relations' cardinality
- Cost formulas estimate the cost of executing each operation in the candidate query tree
  - Parameterised by statistics of the input relations
  - Dependent on specific algorithm used by the operator
  - Cost could be: CPU time, IO time, memory usage, or a combination
- Query tree with the least cost is executed

## Statistics used as Input to Cost

Catalog information used in cost function:

- Information about file size
  - Number of records
  - Record size
  - Number of blocks
  - Blocking factor (number of records per block)
- Information about indexes and indexing attributes of each file
  - Number of levels of each multilevel index (How many levels to traverse before reaching data)
  - Number of first level index blocks
  - Number of distinct values of an attribute
    - Can be used to estimate selectivity
  - Selectivity of an attribute

# Comparison Between Heuristic-Based and Cost-Based

Heuristic-Based Optimisation

- Sequence of single query plans
- Each plan is presumably more efficient than previous
- Search is linear

Cost-Based Optimisation

- Many query plans generated
- Cost of each plan is estimated, with most efficient chosen

Heuristics optimisation is more efficient to generate, however may not always be the most optimal query execution plan

Cost-based optimisation relies on statistics gathered about the relations

# Calculating Costs

For a select:

$$
C_\sigma = b_X + \lceil sel \cdot b_X \rceil
$$

- $b_X$ is the number of blocks loaded by relation $X$.
- $sel$ is the selectivity. It is the proportion of data remaining after the selection
- $\lceil sel \cdot b_X \rceil$ is the number of blocks to write after the selection is complete

For a projection:

$$
C_\pi = 2 b_X
$$

- The initial $b_X$ is for the loading of blocks from $X$ onto main memory
- The next $b_X$ is for writing the result of the projection back onto the disk

For a join $R \bowtie_C X$:

$$
C_\bowtie = b_R + b_R b_S + \left \lceil \frac{js |R| |S|}{bfr_{RS}} \right \rceil
$$

- We assume we have 3 buffers to write data on
- $b_R$ is loading relation $R$ onto the first buffer
- For each block in $R$, we have to go through the all $b_S$ blocks, and write it on the second buffer
- $js$ is the join selectivity. It is defined as $\frac{|R \bowtie_C S|}{|R||S|}$. When we are doing an equijoin, we can rewrite this as $\frac{1}{\max(NDV(a, R), NDV(a, S))}$ (NDV = Non-distinct values, and $a$ is the key we join on).
- $bfr_{RS}$ is the blocking factor of $RS$. Blocking factor is the number of records per block for a relation. For a joined table, the blocking factor can be calculated with $bfr_{RS} = \lfloor \frac{1}{bfr_R^{-1} + bfr_S^{-1}} \rfloor$

If we use pipelining, we do not need to consider loading intermediate results, and we do not need to consider writing intermediate results

- Only consider loading cost for first instruction
- Only consider writing cost for last instruction

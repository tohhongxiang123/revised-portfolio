# Optimisation

Before producing the final executable code, many code generators perform optimisations to make the code faster and more compact. There are 2 kinds of optimisations

1. Platform-independent optimisations: Applies for any target platform
2. Platform-specific optimisations: Only applies to a particular target platform

Here we only consider simple platform-independent optimisations. Such optimisations are usually implemented on intermediate representations like Jimple

# Static Analysis

-   Optimisations need to be behavior-preserving: Optimised code should run faster/take less space but behave exactly the same as the original program
-   An optimising compiler has to reason statically (without running code) about all possible behaviors of the program
-   It follows from some basic results of computability theory that it is impossible to statically predict precisely the possible behaviors of a program
-   So optimisers have to be conservative: Only apply an optimisation if we can be certain that it is behavior-preserving

Examples of optimisations performed by modern compilers

-   Register allocation
-   Common subexpression elimination: If an expression has already been evaluated, reuse its previously computed value
-   Dead code detection: Remove code that cannot be executed
-   Loop invariant code motion: Move computation outside of loop body
-   Loop fusion: Combine 2 loops into 1
-   Reduction in strength: Replace slow operations with faster operations
-   Bounds check elimination: In java, if an array index can be shown to be in range, we do not need to check it at runtime
-   Function inlining: Replace a call to a function by its function body
-   Devirtualisation: Turn a virtual method call into a static one

## Intra-Procedural vs Inter-Procedural

-   Many optimisations only concern the code within a single method or function, this is intra-procedural
-   More ambitious optimisations try to optimise several methods/functions at once: this is inter-procedural or whole-program optimisations
-   There is a similar distinction between intra-procedural static analylsis and inter-procedural static analysis
-   Whole-program optimisations have the potential to deliver greater performance improvements, but they are more difficult to apply and it is harder to reason about their safety

# Intra-Procedural Analysis

## Control Flow Graphs

A control flow graph (CFG) is a representation of all instructions in a method, and the possible flow of execution through these instructions, it is used by many optimisations to reason about the possible behaviors of a method

-   Nodes of the CFG represent the instructions to be executed. There is an edge from node $n_1$ to $n_2$ if $n_2$ could be executed immediately after $n_1$
-   We add distringuised `ENTRY` and `EXIT` nodes representing the beginning and end of the method execution
-   In Jimple, most instructions only have a single successor except conditional jumps, which has 2 (we ignore exceptions)

## Data Flow Analysis

Many analyses employed by compiler optimisations are data flow analyses: They gather information about the values computed and assigned to variables at different points in the program

-   Data flow analyses work on the CFG, considering all possible paths from `ENTRY` node to a certain node, or from that node to the `EXIT` node
-   A typical example of data flow analysis is **liveness** analysis, determining which local variables are live at what point in the program

## Liveness Analysis

-   A local variable is live at a program point if its value may be read before it is reassigned
-   In terms of a CFG, a variable `x` is live after a CFG node $n$ if there is a path $p$ from $n$ to `EXIT` such that there is another node $r$ on $p$ that reads `x`, and $r$ is not preceded on $p$ by any other node that writes to `x`
-   A variables can be live before or after a node
-   For some node $n$, we write $in_L(n)$ for the set of variables that are live **before** $n$, and $out_L(n)$ for the set of variables that are live after $n$. These sets are known as flow sets
-   The goal of liveness analysis is to compute $in_L(n)$ and $out_L(n)$ for all CFG nodes

Note that, if we already known $out_L(n)$ for some node, $in_L(n)$ is easy to compute. A variable $x$ is live before $n$ if

1. Either $n$ reads $x$
2. Or $x$ is live after $n$ and $n$ does not write to $x$

By convention, the set of local variables a node $n$ reads is denoted as $use(n)$, and the set of variables it writes is $def(n)$. Hence, we have

$$
in_L(n) = out_L(n) - def(n) \cup use(n)
$$

The set of live variables going into $n$ is the set of live variables coming out of $n$, minus the variables written by $n$ (since $n$ overwrites these variables, these variables become dead), plus the variables read by $n$. This equation is known as the **transfer function** for $in_L(n)$

-   A data flow analysis where $in_L(n)$ is computed from $out_L(n)$ is called a **backward flow analysis** - an analysis where $out_L(n)$ is computed from $in_L(n)$ is called **forward flow** analysis

How to compute $out_L(n)$? There are 2 cases

1. Node $n$ is the `EXIT` node: $out_L(n) = \emptyset$: No variables are live at the end of a method
2. Node $n$ has at least 1 successor node (There is more code to execute after $n$):
    - Let $succ(n)$ be the set of successor nodes of $n$
    - Note that a variable $x$ is live after $n$ if it is live before any successor of $n$
    - We then define $out_L(n) = \bigcup \{in_L(m) \ | \ m \in succ(n) \}$
    - The live variables coming out of $n$ is the union of all live variables going into all child nodes

A data flow analysis where union is used to combine results from successor nodes is called a **may** analysis: If intersection is used, the analysis is a **must** analysis

## Computing Transfer Functions

The system of equations for the transfer functions cannot be solved directly, because the definitions are circular. Howver, the equation system can be solved by iteration

1. For all nodes $n$, set $in_L(n) = out_L(n) = \emptyset$
2. For every node $n$, recompute $out_L(n)$ based on the values we have computed in the previous iteration, and then recompute $in_L(n)$ from $out_L(n)$
3. Keep repeating step 2 until values do not change any further

This algorithm terminates and computes a solution for the equation system (Proof of this result is beyond the scope of the course)

This method can be used for many other data flow analyses

## Simplifying Transfer Equations

To make it easier to solve these equations, we substitute $out_L(n)$ away, so we only have to solve for $in_L(n)$

## Common Subexpression Elimination

Useful optimisation: Identify expressions that have been computed before, and resuse the previously computed result

```
t = z*2; # computed first time
if t > y goto l2;
r = r*r;
z = z*2; # reused
```

Since the last `z*2` has been computed before, we replace it with the assignment `z = t`

## Correctness Condition

There are 2 conditions that need to hold before we can eliminate a common subexpression

1. Expression must have been computed previously on **every** execution path, not just one

    ```
    if z > 0 goto l;
    x = y + z;
    r = y + z; # l
    ```

2. None of the variables involved in the computation was updated in the meantime

    ```
    x = y + z;
    y = y + 1; // updated
    z = y + z;
    ```

## Available Expression

We formalise these conditions in terms of the CFG via the concept of an available expression:

> An expression $e$ is available before a CFG node $n$ if:
>
> 1. $e$ is computed on every path from `ENTRY` to $n$
> 2. No variable in $e$ is overwritten between the computation of $e$ and $n$

-   We can compute the set $in_A(n)$ of available expressions before a node $n$ using data flow analysis
-   The set of possible expressions is infinite, however we only need to consider the expressions that actually occur in the method
-   For simplicity, we only consider expressions computed from local variables using arithmetic operators

## Available Expression Analysis

An expression $e$ is available after node $n$ if

1. $n$ computes $e$, or
2. $e$ is available before $n$, and $n$ does not overwrite any variables in $e$

Let us define $vars(e)$ for the variables in $e$, and $comp(n)$ for the expressions computed by $n$. Also recall $def(n)$ is the set of variables that $n$ writes to

We have

$$
\begin{aligned}
out_A(n) = in_A(n) &- \{ \ e \ | \ vars(e) \cap def(n) \neq \emptyset \ \} \\ & \cup \ comp(n)
\end{aligned}
$$

-   The available expressions after a node $n$ is the set of available expressions before node $n$ ($in_A(n)$), excluding any expressions which include at least 1 variable written to by $n$ ($\{ e \ | \ vars(e) \cap def(n) \neq \emptyset \}$), and adding the expressions computed by $n$ ($comp(n)$)

An expression is available before $n$ if it is available after every predecessor $m$ of $n$; No expression is available before `ENTRY`

$$
\begin{aligned}
   in_A(n) &= \bigcap \{ out_A(m) \ | \ m \in pred(n) \} \\
   in_A(ENTRY) &= \emptyset
\end{aligned}
$$

# Other Data Flow Analysis

-   Liveness is a backward-may analysis: whether a variable is live before a node depends on whether it is live after a node (but not vice versa), and a variable is live after a node if it is live before any successor node
-   Available Expressions is a forward-must analysis: Whether an expression is available after a node depends on whether it is available before the node (but not vice versa), and an expression is available before a node if it is available after every predecessor node
-   The are also backward-must analyses (very busy expressions) and forward-may analysis (reaching definitions)

# General Characteristics

Clearly, liveness and available expressions (and many other dataflow analyses) share some basic similarities

-   They compute flow sets $in(n)$ and $out(n)$ for before and after every node $n$ in the CFG
-   The analysis results are always sets of data flow facts: In the case of liveness, sets of variables that are live. In the case of available expressions, sets of available expressions
-   There are also functions that compute either $out(n)$ from $in(m)$ for the successors $m$ of $n$, or $in(n)$ from $out(m)$ for the predecessors $m$ of $n$, again depending on whether the analysis is forwards or backwards

# Worklist Algorithm (Forward)

For forward analyses, $out(n)$ can only change if $in(n)$ changes, and $in(n)$ only if $out(m)$ changes for some predecessor $m$ of $n$

We can avoid unnecessary recomputation by keeping a `worklist` of nodes for which $in(n)$ has changed

```
worklist = [all nodes]
while worklist is not empty:
   m = removeFirst(worklist)
   recompute out(m)
   if out(m) has changed:
      for each successor n of m:
         compute in(n)
         if in(n) has changed:
            add n to worklist if not already in worklist
```

# Worklist Algorithm (Backward)

For backward analyses, $in(n)$ can only change if $out(n)$ changes, and $out(n)$ can change only if $in(m)$ changes for some successor $m$ of $n$

Similar to the forward algorithm, we avoid unnecessary recomputation by keeping a worklist of nodes for which $out(n)$ has changed

```
worklist = [all nodes]
while worklist is not empty:
   m = removeFirst(worklist)
   recompute in(m)
   if in(m) has changed:
      for each predecessor n of m:
         compute out(n)
         if out(n) has changed:
            add n to worklist if not already in worklist
```

# Worklist Algorithm Initialisation

Forward analysis

-   For all nodes, if the join operator is a union, then $out(n) = in(n) = \emptyset$, otherwise $U$, except $in(ENTRY)$, which is defined by the analysis
-   We initialise the worklist to contain all nodes in the CFG to ensure that each node is evaluated at least once
-   The order in which the worklist is initialised is important
    -   As far as possible, the nodes should be ordered so that a node is only evaluated after all its predecessors have updated their values (but this can be difficult when the CFG has cycles)

Backward analysis

-   This is similar to forward analysis, except the roles of $in(n)$ and $out(n)$, `ENTRY` and `EXIT`, predecessor and successor are reversed

# Conflicts between Optimisations

Sometimes different optimisations may be in conflict with each other

-   For instance, common subexpression elimination increases the live range of local variables
-   This makes register allocation harder, so the performance gain of avoiding recomputation may be lost because more memory accesses are required
-   Deciding which optimisations to apply and when to apply them is difficult, there is no general recipe

# Inter-Procedural Optimisations

We consider 2 inter-procedural optimisations

-   Inlining: Function calls incur overhead due to parameter passing and pipeline stalls. Inlining replaces a call with the body of the invoked function, which avoids overhead but increases code size
-   Devirtualisation: In Java, the method invoked by an expression often depends on the runtime type
    -   We invoke a call `e.m()`
    -   If `e` has type `C` at compile time, then the object it evaluates to at runtime could have a more specific type `D`, so the JVM needs to look up `m` on the runtime type `D` in order to work out which method to invoke
    -   This is known as a virtual call. The aim of this optimisation is to determine at compile time which method could be invoked, turning it into a static call

Both optimisations need a call graph indicating for each call all the possible call targets

For example,

```java
interface Shape { double area(); }

class Rectangle implements Shape {
   // …
   public double area() { return width*height; }
}

class Circle implements Shape {
   // …
   public double area() { return Math.PI*radius*radius;}
}

class Test {
   public static void main(String[] args) {
      Shape[] shapes = { new Rectangle(), new Circle() };
      for(Shape shape : shapes) shape.area();
   }
}
```

The call to `shape.area()` could invoke either `Rectangle.area()` or `Circle.area()`, cannot be inlined

```java
class Test {
   public static void main(String[] args) {
      Shape[] shapes = { new Rectangle() };
      for(Shape shape : shapes) shape.area();
   }
}
```

We now know that `shape` is definitely a `Rectangle`, hence the call for `shape.area()` definitely invokes a `Rectangle.area()`, hence this function call can be inlined

# Computing Call Graphs

A call graph is a set of call edges $(c, m)$ where $c$ is a call site (method call) and $m$ is a call target (a method)

-   Call site $c$ may invoke call target $m$ at runtime (one call site can have multiple call targets)
-   Language features like method overriding and function points make call graph computations difficult; in fact, computing a precise call graph is impossible (undecidable)
-   We can, however, compute an overapproximate call graph: If at runtime, $c$ may invoke $m$, then the call graph contains the edge $(c, m)$
-   However, the call graph may contain edges $(c', m')$, where call site $c'$ can never actually invoke $m'$: This is called a **spurious call edge**

# Call Graph Algorithms

For object-oriented programming languages, there are 3 popular call graph construction algorithms which all yield overapproximate call graphs

1. Class Hierarchy Analysis (CHA)
2. Rapid Type Analysis (RTA)
3. Control Flow Analysis (CFA), also known as Pointer Analysis

CHA is the fastest of these algorithms, but it yields the least precise call graphs (many spurious edges); CFA gives the best call graphs (few spurious edges), but is quite slow in practice

CFA is also applicable to other languages

-   Much research has gone into making CFA faster, and most modern compilers now use CFA-like analyses for inter-procedural optimisation

## CHA

The idea of CHA:

1. For a call `e.m()`, determine the static type `C` of `e`
2. Then look up method `m` in class `C` or its ancestors; this yields some method definition `md`
3. The possible call targets of the call are `md` (unless it is abstract) and any (non-abstract) methods `md'` that override `md`

In our earlier example, CHA would determine that the call targets of `shape.area()` are `Rectangle.area()` and `Circle.area()`, which is correct for the first example, but imprecise for the second example. Thus, CHA could be be used for inlining that call in the second example

## RTA

Note that in the second example, class `Circle` is never instantiated, hence `Circle.area()` can never be invoked

-   RTA improves on CHA by keeping track of which classes are instantiated somewhere in the program; call these **live classes**
-   If a method is neither declared in a live class, nor inherited by a live class, then it clearly can never be a call target
-   RTA thus can build precise call graphs for both examples

## CFA

RTA is easily fooled. Consider the following example:

```java
class Test {
   public static void main(String[] args) {
      new Circle();
      Shape[] shapes = { new Rectangle() };
      for(Shape shape : shapes) shape.area();
   }
}
```

-   RTA sees that both `Circle` and `Rectangle` are live, hence it thinks that `shape.area()` could invoke either `Circle.area()` or `Rectangle.area()`, but clearly only `Rectangle.area()` would be invoked
-   CFA flow analysis keeps track of possible runtime types of every variable; It can tell that elements of the shapes array can only be of type `Rectangle`, hence shape must be of type `Rectangle`, yielding a precise call graph

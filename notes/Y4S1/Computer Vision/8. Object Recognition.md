# Object Recognition

Object detection: Where is a specific object in the image?

- Can be at categorisation level or instance level

Caltech101:

- Dataset with 101 object categories, and 40-800 images per category

# Bag-of-Words Model

Given features, we do classification. However, what kind of features do we want to detect, and how to organize/use them?

1. Data collection: A set of sentences
2. Design vocabulary: Get unique words from data
3. Create document vectors: Each index represents a specific word, and the value is the count of each word

Bag-of-Words work well on image-level recognition tasks such as image classification

Scale invariant feature transform (SIFT) widely used for robust feature detection and description

1. Extract features from images
   1. Many feature detectors are available, such as SIFT
2. Learn "Visual vocabulary"
   1. From features, cluster them to form a visual vocabulary
3. Quantize features using visual vocabulary
4. Represent images by frequencies of visual words
5. Train SVM to classify new images based on features detected

# Supervised Learning

- Learn from labelled data. Labelled classification model assigns class labels to unseen recrods

Given some linearly separable dataset $\{ \bold{x}_p, y_p \}_{p=1}^{N}, \bold{x} \in \mathbb{R}^D, y \in \{-1, +1\}$, support vector machines (SVMs) find a separating hyperplane which satisfies

$$
\begin{cases}
    \bold{w}^T \bold{x}_i + b \geq 1 & y_i = +1 \\
    \bold{w}^T \bold{x}_i + b \leq -1 & y_i = -1
\end{cases}
$$

- A hyperplane too close to the training examples will be sensitive to noise, and less likely to generalise well for data unseen during training
- Optimal separating hyperplane will be the one with the largest margins

Hyperplane equation: $\bold{w}^T \bold{x} + b = 0$

- Canonical hyperplane: $|\bold{w}^T \bold{x} + b| = 1$
- Margin: $m = \frac{2}{\lVert \bold{w} \rVert}$

To find the hyperplane, we want to do the following:

$$
\min J(\bold{w}) = \frac{1}{2} \lVert \bold{w} \rVert \ s.t. \ y_i (\bold{w}^T \bold{x}_i + b) \geq 1
$$

- $J(\bold{w})$ is a quadratic function, which means that there exists a single global minimum, and no local maximum
- Convert to Langrangian primal problem:

$$
\min L_p(\bold{w}, b, \bold{\alpha}) =  \frac{1}{2} \lVert \bold{w} \rVert - \sum_{i=1}^{N} \alpha_i [y_i (\bold{w}^T \bold{x}_i + b) - 1]
$$

Precision, Recall and F-measure

- Suppose the cutoff threshold is chosen to be 0.8 (Any instance with posterior probability > 0.8 is classified positive, else negative)
- Precision, $P = TP / (TP + FP)$ (Accuracy of positive predictions)
- Recall, $R = TP / (TP + FN)$ (Completeness of positive predictions)
- F-measure: $F = 2PR / (P + R)$

# Unsupervised Learning

Clustering: Finding groups of objects

- Objects in a group are similar
- Objects in different groups are unrelated
- Intra-cluster distances are minimised, inter-cluster distances are maximised

K-means clustering: Data grouping and cluster centroid update

Density-based spatial clustering (DBSCAN) has a number of desired properties

- Does not require cluster number information
- Performs well with arbitrary shaped clusters
- Robust to outliers and able to detect outliers
- Handle data distributions of different sizes

2 key parameters of DBSCAN:

1. eps: The distance that specifies the neighborhoods: 2 points are considered neighbors if distance between them are less than or equal to eps
2. minPts: Minimum number of data points to define a cluster

With these 2 parameters, points are classified into 3 categories

1. Core point: At least minPts points (including the point itself) in its surrounding area with radius eps (Basically the centroid)
2. Border point: If it is reachable from a core point, and less than minPts points in its surrounding area
3. Outlier: Not a core point, and unreachable from any core point

![](https://www.researchgate.net/publication/342082665/figure/fig2/AS:903773622898690@1592487831444/The-DBSCAN-algorithm-and-two-generated-clusters-There-are-three-types-of-points-as.png)

1. With an arbitrary unvisited, its neighborhood is retrieved from the eps.
2. If it has MinPts within eps neighborhood, cluster formation starts. Or labeled as noise.
3. If itâ€™s a core point, the points in its eps neighborhood is part of the cluster. All points in the eps neighborhood are added, along with their own eps neighborhood, if being core points.
4. The above process continues until density connected cluster is completely found.
5. The process restarts with a new point which can be a part of a new cluster or noise.

Constraints of density-based clustering:

- Cannot handle varying densities
- Sensitive to parameters

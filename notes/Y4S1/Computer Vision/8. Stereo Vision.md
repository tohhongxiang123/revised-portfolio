# Stereo Vision

# Stereo Vision and Parallax

Computer stereo vision is the extraction of 3D information from digital images, such as those obtained by a CCD camera. By comparing information about a scene from two vantage points, 3D information can be extracted by examining the relative positions of objects in the two panels. This is similar to the biological process of stereopsis.

Parallax is a displacement in the apparent position of an object viewed along 2 different lines of sight. It is measured by the angle/semi-angle of inclination between those 2 lines

![](https://www.researchgate.net/publication/257922664/figure/fig7/AS:668220231008265@1536327526891/Concept-of-stereoscopic-viewing-By-fusing-left-and-right-images-the-brain-perceives.ppm)

Disparity and Depth Maps: Each pixel is associated witha disparity value and a depth value

Assume a camera translates by $T$ along the x-axis in the camera frame. With camera perspectives:

$$
Z = \frac{fT}{d}
$$

- Disparity $d = x_l - x_r$
- Focal length $f$: Distance of camera from image frame

Using $w$ for disparity (to avoid confusion in differentiation), triangulation accuracy is affected by:

$$
\begin{aligned}
Z &= \frac{fT}{w} \\
\frac{dZ}{dw} &= -f\frac{T}{w^2} \\
|\delta Z| &= |\frac{dZ}{dw} \delta w| \\
&= |\frac{fT}{w^2} \delta w| \\
&= |\frac{Z^2}{fT} \delta d|
\end{aligned}
$$

- $T \to 0 \implies \delta Z \to \infty$: If 2 cameras are too close, accuracy is bad
- $Z \to \infty \implies \delta Z \to \infty$: Lower accuracy for faraway 3D points

# Simple 2D Triangulation

1. Compute rectification homography to re-project the image planes to be parallel to each other
2. Depth map can then be computed based on pixel disparity

# Point Matching

From 2 points in the left and right image, we can reconstruct the 3D point

- This assumes that the 2 2D points correspond to the same 3D point
- Correspondence problem: For each important image point in the first image, find the corresponding point in the second image

2 approaches

1. Appearance based matching: Match points with similar appearances in 2 images
2. Feature based matching: Match similar features (edges, corners) in 2 images

## Point Matching: Appearance Based

Assumptions: Corresponding points in 2 images have image patches that look identical

- Minimal geometric distortion
- Lambertian reflectance
- No occlusion

These assumptions are reasonable for stereo cameras with a small baseline

Appearance-based matching: SSD

- For a small image patch $g[(2N+1) \times (2N+1)]$ in the first image, find the corresponding image patch center location $(x, y)$ with the smallest sum-of-square difference (SSD) in the second image $I$ such that

$$
(x^*, y^*) = \argmin_{(x, y)} \sum_{u=-N}^{N} \sum_{v=-N}^{N} [I(x + u, y + v) - g(u, v)]^2
$$

- Correspondence by appearance matching is most accurate when cameras are close to each other: Small baseline
- 3D estimation is more accurate when cameras are far apart: Large baseline
- In real 3D stereo applications, we need a reasonable tradeoff
- SSD based correspondence search is not robust

## Point Matching: Feature Based

Matching image points that have large differences in viewpoints

- To extract image points with local properties that are approximately invariant to larger changes of viewpoint
- Need to select feature points as 3D reconstruction requires sparse point correspondences only
- E.g. corners (angle or corner), curves (maximum radius of curvature)

Match feature points across images directly by finding similar properties

- E.g. matching corner points in 2 images with the same angle
- Corner points can have very different orientations

Even with feature properties, there may be multiple candidates. Heuristics can be used to select the correct correspondence, though they are empirical and could be unreliable

- Proximity: Match feature point to candidate with nearest $(x, y)$ position in second image
- Ordering: Feature points on a contour in 1st image must match those on a contour in the 2nd image on the same order

Appearance based point matching allows greater baseline/larger variation in viewpoints. Method:

- Extract feature points with properties which are invariant to viewpoint changes
- Match feature points with the same properties across images
- May require heuristics to help find the correct matching, e.g. proximity ordering etc

Scale invariant feature transform (SIFT) has been widely used for robust feature point detection and description

# 3D Reconstruction

By triangulation with calibrated cameras

- For each image point pair in the 2 cameras, we know the associated 3D ray
- Find intersection of rays from corresponding image points

Assume we have projection matrices from camera calibration

Left camera:

$$
\begin{bmatrix}
    kx_l \\ ky_l \\ k
\end{bmatrix} = \begin{bmatrix}
    a_1 & a_2 & a_3 & a_4 \\
    a_5 & a_6 & a_7 & a_8 \\
    a_9 & a_{10} & a_{11} & 1
\end{bmatrix} \begin{bmatrix}
    X \\ Y \\ Z \\ 1
\end{bmatrix}
$$

Right camera:

$$
\begin{bmatrix}
    mx_r \\ my_r \\ m
\end{bmatrix} = \begin{bmatrix}
    b_1 & b_2 & b_3 & b_4 \\
    b_5 & b_6 & b_7 & b_8 \\
    b_9 & b_{10} & b_{11} & 1
\end{bmatrix} \begin{bmatrix}
    X \\ Y \\ Z \\ 1
\end{bmatrix}
$$

We solve for unknowns $X, Y, Z$, to get the following equations:

$$
\begin{aligned}
    (a_9 x_l - a_1) X + (a_{10} x_l - a_2) Y + (a_{11} x_l - a_3)Z &= (a_4 - x_l) \\
    (a_9 y_l - a_5) X + (a_{10} y_l - a_6) Y + (a_{11} y_l - a_7)Z &= (a_8 - y_l) \\
    (b_9 x_l - b_1) X + (b_{10} x_l - b_2) Y + (b_{11} x_l - b_3)Z &= (b_4 - x_l) \\
    (b_9 y_l - b_5) X + (b_{10} y_l - b_6) Y + (b_{11} y_l - b_7)Z &= (b_8 - y_l) \\
\end{aligned}
$$

Writing this in matrix form:

$$
\begin{aligned}
\begin{bmatrix}
    a_9 x_l - a_1 & a_{10} x_l - a_2 & a_{11} x_l - a_3 \\
    a_9 y_l - a_5 & a_{10} y_l - a_6 & a_{11} y_l - a_7 \\
    b_9 x_l - b_1 & b_{10} x_l - b_2 & b_{11} x_l - b_3 \\
    b_9 y_l - b_5 & b_{10} y_l - b_6 & b_{11} y_l - b_7 \\
\end{bmatrix} \begin{bmatrix}
    X \\ Y \\ Z
\end{bmatrix} &= \begin{bmatrix}
    a_4 - x_l \\ a_8 - y_l \\ b_4 - x_r \\ b_8 - y_r
\end{bmatrix} \\
\bold{Wx} &= \bold{q}
\end{aligned}
$$

Compute 3D world coordinates $X, Y, Z$ using pseudo-inverse

$$
\begin{bmatrix}
    X \\ Y \\ Z
\end{bmatrix} = \bold{W}^+ \bold{q} = (\bold{W}^T \bold{W})^{-1} \bold{W}^T \bold{q}
$$

Procedure for 3D reconstruction

1. Fix the pose of cameras: E.g. place them on tripods
2. Use a calibration chart to calibrate your cameras using known correspondences of 3D points to 2D image poitns
3. For an arbitrary scene in front of your cameras, your system should automatically
   - Find corresponding image points between camera images
   - Compute 3D coordinates for each correspondence

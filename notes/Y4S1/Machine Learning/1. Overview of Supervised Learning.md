# Overview of Supervised Learning

In supervised learning, computers are given (input, output) pairs, and the goal is for the model to learn the mapping between inputs and outputs

More specifically, given a set of $\{x_i, y_i\}$ for $i = 1, ..., N$, where $x_i = [x_{i1}, ..., x_{im}]$ and $y_i$ is a scalar, the goal is to learn the mapping $f: x \to y$ by requiring $f(x_i) = y_i$

-   The learned mapping $f$ is expected to be able to make precise predictions for unseen values $x^*$ as $f(x^*)$

# Classification vs Regression

For classification, $y$ is discrete

-   If $y$ is binary (only 2 values), then it is a binary classification
-   If $y$ is nominal (more than 2 possible values), then it is a multi-class classification

For regression, $y$ is continuous.

# Typical Learning Procedure

Consists of 2 phases

1. Training phase
    - Given labelled training data $\{x_i, y_i\}$ for $i = 1, ..., N$
    - Apply supervised learning algorithms on training data to learn a model such that $f(x_i) = y_i$
2. Testing/prediction phase
    - Given unseen test data $x_i^*$ for $i = 1, ..., T$
    - Use trained model $f$ to predict $f(x_i^*)$s

# Inductive Learning

How to learn a predictive model $f(\cdot)$

-   Bayesian classifiers
-   Decision trees
-   Artificial neural networks
-   Support vector machines
-   Regularised regression models

# Evaluation of Supervised Learning

The whole training data is split into "train" and "test" sets, with training set used to learn the predictive model, and the test set used to validate the performance of the model

Common evaluation metrics include

-   For classification:
    -   Accuracy
    -   Error rate
    -   F-score
-   For regression
    -   Mean absolute error
    -   Root mean square error

## Cross Validation

![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)

$k$-fold cross validation:

-   Partition data into $k$ subsets of the same size
-   For each fold, set aside one group for testing, and use the rest for training the model

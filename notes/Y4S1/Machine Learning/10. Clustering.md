# Clustering

Cluster analysis is finding groups of data instances such that the data instances in a group are

- Similar to one another
- Different from data instances in other groups

Cluster analysis is unsupervised

## Types of Clustering

- A clustering is a set of clusters
- Important distinction between hierarchical and partitional set of clusters
- Partitional clustering
  - Divide data instances into non-overlapping subsets (clusters) so that each data instance is in exactly 1 subset
- Hierarchical clustering
  - A set of nested clusters organised as a hierarchical tree

## Other Distinctions Between Clustering

Exclusive vs Non-exclusive

- In non-exclusive clustering, instances may belong to multiple clusters

Fuzzy vs non-fuzzy

- In fuzzy clustering, a point belongs to every cluster with some weight between 0 and 1
- Weights must sum up to 1

Partial vs complete

- In partial clustering, only some of the instances are clustered

# Clustering Algorithms

- K-means and its variants
- Hierarchical clustering

## K-Means

A partitional clustering approach

- The number of clusters $K$ must be specified
- Each cluster is associated with a centroid
- Each point is assigned to the cluster with the closest centroid

The basic algorithm:

1. Select $K$ random data instances as initial centroids
2. Repeat
   1. Form $K$ clusters by assigning all data instances to the closest centroid
   2. Recompute the centroid of each cluster
3. Until the centroids do not change

Details

- Initial centroids are often chosen randomly
- The centroid is typically the mean of the data instances in the cluster
- Closeness is measured by proximity
  - Distance: Euclidean, manhattan etc
  - Similarity: Cosine similarity, correlation, etc
- K-means will converge for common similarity measures mentioned above
  - In practice, it converges within the first few iterations
  - Often the stopping condition is changed to "Until relatively few instances change clusters"

### Evaluating K-Means Clusters

Most common measure is sum of squared errors (SSE)

- For each data instance, the error is the disteance to the nearest cluster that is represented by a centroid
- To get an overall SSE, we square these errors and sum them

$$
SSE = \sum_{i=1}^{K} \sum_{\bold{x} \in C_i} dist(\bold{c}_i, \bold{x})^2
$$

- $\bold{c}_i$ is the centroid of the cluster
- The cluster SSE for cluster $C_i$ is $SSE_{C_i} = \sum_{\bold{x} \in C_i} dist(\bold{c}_i, \bold{x})^2$

Given 2 differnt runs of K-means, we choose the one with the smallest total SSE

- One way to reduce SSE is to increase $K$, the number of clusters
- However, a good clustering with a smaller $K$ can have a lower SSE than a poor clustering with higher $K$

### Initial Centroids Issue

- Multiple runs
- Postprocessing
  - Decompose a cluster with high Cluster SSE
  - Merge clusters with low Cluster SSE, which are close to each other
- Bisecting K-means

### Pre and Post-processing

Preprocessing

- Normalise data
- Eliminate outliers

Postprocessing

- Eliminate small clusters that may represent outliers
- Split loose clusters (clusters with relatively high SSE)
- Merge clusters that are close and that have relatively low SSE

### Empty Clusters Issue

Basic K-means algorithm can yield empty clusters

- Several strategies to choose a replacement centroid
  - Choose the point that contributes most to the SSE
  - Choose a point from the cluster with the highest cluster SSE
  - If there are several empty clusters, the above can be repeated multiple times

### Bisecting K-means

Basic algorithm

1. Initialise a list of clusters to be a single cluster that contains all points
2. Repeat
   1. Select a cluster with highest SSE from the list of clusters
   2. From i = 1 to T:
      1. Bisect the selected cluster using basic K-means
   3. End
   4. Add the 2 clusters from the bisection with the lowest SSE to the list of clusters
3. Until the list of clusters contains $K$ clusters
4. (After bisecting) run K-means with the K clusters found so far

### Limitations of K-means

K-means has problems when clusters are of differing

- Sizes
- Density
- Non-globular shapes

K-means has problems when the data contains outliers

## Hierarchical Clustering

Produces a set of nested clusters, organised as a hierarchical tree

- Can be visualised as a dendrogram: A tree like diagram that records the sequences of merges/splits

Strengths of Hierarchical clustering

- Do not have to assume any particular number of clusters
  - Any desired number of clusters can be obtained by "cutting" the dendrogram at the appropriate level
- They may correspond to meaningful taxonomies
  - E.g. document organisation, biological sciences

2 types of hierarchical clustering

1. Agglomerative (bottom-up)
   - Start with instances as individual clusters
   - At each step, merge the closest pair of clusters, until only 1 cluster (or K clusters) are left
2. Divisive (top-down)
   - Start with one all-inclusive cluster
   - At each step, split a cluster until each cluster contains a point (or there are K clusters)

Traditional hierarchical algorithms use a proximity matrix (similarity or distance) to merge/split one cluster at a time

### Agglomerative Clustering Algorithm

1. Compute proximity matrix (similarity or distance)
2. Let each data instance be a cluster
3. Repeat
   1. Merge 2 closest clusters
   2. Update proximity matrix
4. Until only a single cluster remains

Key operation is to compute the proximity of 2 clusters

- Different approaches to defining proximity between clusters may lead to different clustering results

We can define inter-cluster similarity as:

- MIN or Single Link
  - The proximity between the closest 2 data points in different clusters
  - The shortest edge (single link) between 2 nodes in different subsets (using graph terms)
- MAX or Complete Link
  - The proximity between the farthest 2 points that are in different clusters
  - The longest edge (complete link) between 2 nodes in different subsets
- Group Average

  - The average pairwise proximities of all pairs of points from different clusters
  - Or average length of edges between nodes in different subsets

  $$
    Proximity(C_i, C_j) = \frac{\sum_{\bold{x}_i \in C_i, \bold{x}_j \in C_j} Proximity(\bold{x}_i, \bold{x}_j)}{|C_i| |C_j|}
  $$

Limitations of Agglomerative Clustering

- Once a decision is made to combine 2 clusters, it cannot be undone
- No objective function is directly minimised
- Different schemes have problems with one or more of the following:
  - Sensitivity to noise/outliers
  - Difficulty in handling different sized clusters

## Divisive Hierarchical Clustering

1. Generate a minimum spanning tree to connect all data instances as a single cluster
2. Repeat
   1. Create a new cluster by breaking the link corresponding to the largest distance (smallest similarity)
3. Until only singleton clusters remain

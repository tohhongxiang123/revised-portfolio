# Density Estimation

Purpose of machine learning

-   Model uncertainty (Density estimation)
-   We want to figure out shape of probability distribution

Recall Naive Bayes

$$
\begin{aligned}
y_i &= \argmax_c P(\bold{x} | y = c) P(y = c) \\
&= \argmax_c \prod_{i=1}^{d} P(x_i | y = c) P(y = c)
\end{aligned}
$$

# Discrete Probability Distributions

-   Usually finite number of outcomes
-   6-sided die: 6 possible outcomes
-   Distribution can be described with 6 numbers
    -   Non-negative
    -   Sums to 1

# Continuous Probability Distributions

-   Outcome is a real number in a range (e.g. $[0, 1], (-\infty, \infty)$)
-   Infinite number of outcomes between any 2 real numbers

# Probability Density Function

Probability is area under the curve

$$
\begin{aligned}
    y &= p(x) \\
    \int_{-\infty}^{\infty} p(x) dx &= 1 \\
    P(\bold{x}^*) &= \int_{\bold{x}^*}^{\bold{x}^* + \epsilon} p(x) dx \\
    &\approx p(\bold{x}^*) \times \epsilon
\end{aligned}
$$

# Density Estimation

Density estimation aims to estimate an unobservable underlying probabilty density function, based on observed data

-   Denote by $\mathcal{D} = \{ \bold{x}_1, ..., \bold{x}_N \}$ the set of observed data points, drawn from an unknown probability distribution $P(\bold{x})$

    $$
        \bold{x}_i \sim P(\bold{x}), \ i = 1, ..., N
    $$

-   Goal is to estimate the associated probability density function $p(\bold{x})$ of the distribution

# State-of-the-Art Density Estimation

Generative Adversarial Networks

-   Competition drives the counterfeiter to learn the distribution of real data

Diffusion models

-   Learn a sequence of transformations that create images from pure noise
-   E.g. DALL-E V2, 2022, sampling from conditional distribution $P(image | text)$

# Density Estimation Approaches

-   Parametric density estimation
    -   Assume a form for $p(\bold{x}; \bold{\theta})$, defined up to parameters, $\theta$
    -   E.g. Gaussian distribution $\mathcal{N}(\mu, \sigma^2), \bold{\theta} = \{ \mu, \sigma^2 \}$
    -   Estimate $\bold{\theta}$ from observed data points (Maximum likelihood estimation)
-   Nonparametric density estimation

# General Principle

Observed data points $\mathcal{D} = \{ \bold{x}_1, ..., \bold{x}_N \}$ are assumed to be a sample of $N$ random variables, independent and identically distributed (iid)

-   Identically distributed: for any $\bold{x}_i \in \mathcal{D}$, it is sampled from the same probability distribution
-   Independent: All data points $\bold{x}_i \in \mathcal{D}$ are independent events

# Parametric Density Estimation

Assume that $\mathcal{D} = \{ \bold{x}_1, ..., \bold{x}_N \}$ are drawn from some known probability density family $p(\bold{x} | \bold{\theta})$, defined up to parameters $\bold{\theta}$

-   We seek $\bold{\theta}$ that makes $\bold{x}_i$ as likely as possible under $p(\bold{x} | \bold{\theta})$
-   Approach: Maximum likelihood estimation

# Maximum Likelihood Estimation

The likelihood of sample $\mathcal{D}$ given parameter $\bold{\theta}$: $\mathcal{L}(\mathcal{D}; \bold{\theta})$

-   As $\mathcal{D}$ is IID, the above likelihood is the product of the likelihood of the individual points

$$
    \mathcal{L}(\mathcal{D}; \bold{\theta}) \triangleq P(\mathcal{D} | \bold{\theta}) = \prod_{i=1}^{N} P(\bold{x}_i | \bold{\theta}) \propto \prod_{i=1}^{N} p(\bold{x}_i | \bold{\theta})
$$

In MLE, we aim to find $\bold{\theta}$ that makes $\mathcal{D}$ the most likely to be drawn from. Mathematically, we aim to search for $\hat{\bold{\theta}}$ such that

$$
\bold{\hat{\theta}} = \argmax_\theta \mathcal{L}(\mathcal{D}; \bold{\theta})
$$

# Log-Likelihood

Typically, we maximise the log-likelihood

$$
\mathcal{L}(\mathcal{D}; \bold{\theta}) \triangleq \ln \mathcal{L}(\mathcal{D}; \bold{\theta})
$$

-   $\ln$ function converts product into sum

$$
\ln \mathcal{L}(\mathcal{D}; \bold{\theta}) = \ln \left( \prod_{i=1}^{N} p(\bold{x}_i | \bold{\theta}) \right) = \sum_{i=1}^{N} \ln p(\bold{x}_i | \bold{\theta})
$$

-   $\ln$ is also a strictly increasing function, hence one can maximise the log likelihood without changing the value where it can take its maximum

$$
\bold{\hat{\theta}} = \argmax_\bold{\theta} \mathcal{L}(\mathcal{D}; \bold{\theta}) = \argmax_\bold{\theta} \ln \mathcal{L}(\mathcal{D}; \bold{\theta})
$$

# Solving MLE

Suppose $\bold{\theta} \in \mathbb{R}^p$ contains $p$ parameters

$$
\bold{\theta} = [\theta_1, ..., \theta_p]^T
$$

Then, $\argmax_\bold{\theta} \ln \mathcal{L}(\mathcal{D}; \bold{\theta})$ is an unconstrained optimisation problem

-   To solve it, we first set the derivative of $\ln \mathcal{L}(\mathcal{D}; \bold{\theta})$ wrt $\bold{\theta}$

$$
\nabla_\bold{\theta} \ln \mathcal{L}(\mathcal{D}; \bold{\theta}) = \nabla_\bold{\theta} \left( \sum_{i=1}^{N} \ln p(\bold{x}_i | \bold{\theta}) \right) = \sum_{i=1}^{N} \nabla_\bold{\theta} \ln p(\bold{x}_i | \bold{\theta}) = 0
$$

-   Recall $\nabla_\bold{\theta} = \left[ \frac{\partial}{\partial \theta_1}, \cdots, \frac{\partial}{\partial \theta_P}\right]$
-   We obtain a solution for $\hat{\bold{\theta}}$ by solving the above system of equations

# Univariate Gaussian

Suppose $\mathcal{D} = \{x_1, ..., x_N\}$. Each data point $x_i$ is a scalar, and is drawn from a Gaussian distribution with unknown $\mu$ and $\sigma^2$

$$
p(x | \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \frac{(x - \mu)^2}{2 \sigma^2}}
$$

where

$$
\begin{aligned}
    \mu &= \mathbb{E}[x] \\
    &= \int p(x; \mu, \sigma^2) x \ dx \\
    \sigma^2 &= Var(x) \\
    &= \mathbb{E}[(x - \mathbb{E}[x])(x - \mathbb{E}[x])^T]
\end{aligned}
$$

Log-likelihood is:

$$
\begin{aligned}
    \ln \mathcal{L}(\mathcal{D}; \bold{\theta}) &= \sum_{i=1}^{N} \ln p(\bold{x}_i | \bold{\theta}) \\
    &= \sum_{i=1}^{N} \ln \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left({- \frac{(x - \mu)^2}{2 \sigma^2}} \right) \right) \\
    &= \sum_{i=1}^{N} \ln \left( \exp \left( -\frac{(x_i - \mu)^2}{2 \sigma^2} \right) \right) - \sum_{i=1}^{N} \ln \sqrt{2 \pi \sigma^2} \\
    &= - \frac{\sum_{i=1}^{N} (x_i - \mu)^2}{2 \sigma^2} - \frac{N}{2} \ln (2\pi) - \frac{N}{2} \ln \sigma^2
\end{aligned}
$$

Now we differentiate:

$$
\begin{aligned}
    \nabla_\bold{\theta} \ln \mathcal{L}(\mathcal{D}; \bold{\theta}) &=  \sum_{i=1}^{N} \nabla_\bold{\theta} \ln p(x_i | \bold{\theta}) \\
    &= \begin{bmatrix}
        \sum_{i=1}^{N} \nabla_\mu \ln p(x_i | \bold{\theta}) \\
        \sum_{i=1}^{N} \nabla_{\sigma^2} \ln p(x_i | \bold{\theta})
    \end{bmatrix} \\
    &= \begin{bmatrix}
        \frac{1}{\sigma^2} \sum_{i=1}^{N} (x_i - \mu) \\
        \frac{\sum_{i=1}^{N} (x_i - \mu)^2}{2 (\sigma^2)^2} - \frac{N}{2 \sigma^2}
    \end{bmatrix}
\end{aligned}
$$

Now we set the derivatives to be 0

$$
\begin{cases}
    \frac{1}{\sigma^2} \sum_{i=1}^{N} (x_i - \mu) = 0 \\
    \frac{\sum_{i=1}^{N} (x_i - \mu)^2}{2 (\sigma^2)^2} - \frac{N}{2 \sigma^2} = 0
\end{cases}
$$

Solving,

$$
\begin{cases}
    \hat{\mu} &= \frac{1}{N} \sum_{i=1}^{N} x_i \\
    \hat{\sigma^2} &= \frac{1}{N} \sum_{i=1}^{N} (x_i - \hat{\mu})^2
\end{cases}
$$

# Unbiased Estimator

An estimator is a rule for estimating a quantity based on observations

-   MLE estimator for gaussian mean is $\hat{\mu} = \frac{1}{N} \sum_{i=1}^{N} x_i$
-   Estimator is unbiased if its expectation is the same as the true quantity

$$
\mathbb{E}[\hat{\mu}] = \mu \\
$$

However $\hat{\sigma}^2$ is biased:

$$
\mathbb{E}[\hat{\sigma}^2] = \frac{N - 1}{N} \sigma^2
$$

Note [this](https://www.probabilitycourse.com/chapter8/8_2_2_point_estimators_for_mean_and_var.php#:~:text=Although%20the%20sample%20standard%20deviation,a%20biased%20estimator%20of%20%CF%83.) and [this](https://math.stackexchange.com/questions/149723/why-is-the-expected-value-ex2-neq-ex2)

To correct bias,

$$
\tilde{\sigma}^2 = \frac{N}{N-1}\hat{\sigma}^2 = \frac{1}{N-1} \sum_{i=1}^{N} (x_i - \hat{\mu})^2
$$

# Multivariate Gaussian

Suppose $\mathcal{D} = \{x_1, ..., x_N\}$ and each data point $\bold{x}_i$ has $d$ dimensions, drawn from a Gaussian distribution with unknown $\bold{\mu}$ and $\bold{\Sigma}$

-   $\bold{\mu} \in \mathbb{R}^d$ is a $d$-dimensional mean vector
-   $\bold{\Sigma} \in \mathbb{R}^{d \times d}$ is a $d \times d$ covariance matrix

Then,

$$
p(\bold{x} | \bold{\mu}, \bold{\Sigma}) = \frac{1}{(2\pi)^{\frac{d}{2}}} \frac{1}{|\bold{\Sigma}|^{\frac{1}{2}}} \exp \left( - \frac{1}{2} (\bold{x} - \bold{\mu})^T \bold{\Sigma}^{-1}(\bold{x} - \bold{\mu})\right)
$$

The estimators are defined:

$$
\begin{aligned}
    \bold{\mu} &= \mathbb{E}[\bold{x}] \\
    &= \int p(\bold{x}; \bold{\mu}, \Sigma) \bold{x} \ d\bold{x} \\
    \Sigma &= \mathbb{E}[(\bold{x} - \mathbb{E}[\bold{x}])(\bold{x} - \mathbb{E}[\bold{x}])^T]
\end{aligned}
$$

Note that the covariance matrix $\bold{\Sigma}$ is defined as:

$$
\bold{\Sigma} = \begin{pmatrix}
    Var(x^{(1)}) & Cov(x^{(1)}, x^{(2)}) & \cdots & Cov(x^{(1)}, x^{(d)}) \\
    Cov(x^{(2)}, x^{(1)}) & Var(x^{(2)}) & \cdots & Cov(x^{(2)}, x^{(d)}) \\
    \vdots & \vdots & \ddots & \vdots \\
    Cov(x^{(d)}, x^{(1)}) & Cov(x^{(d)}, x^{(2)}) & \cdots & Var(x^{(d)})
\end{pmatrix}
$$

where

$$
\begin{aligned}
    Var(x^{(i)}) &= \mathbb{E}\left[(x^{(i)} - \mathbb{E}[x^{(i)}])^2\right] \\
    Cov(x^{(i)}, x^{(j)}) &= \mathbb{E}\left[(x^{(i)} - \mathbb{E}[x^{(i)}])(x^{(j)} - \mathbb{E}[x^{(j)}])\right] \\
    &= \int (x^{(i)} - \mathbb{E}[x^{(i)}])(x^{(j)} - \mathbb{E}[x^{(j)}]) p(x^{(i)}) p(x^{(j)}) dx^{(i)} dx^{(j)}
\end{aligned}
$$

The log-likelihood of multivariate Gaussian:

$$
\ln \mathcal{L}(\mathcal{D}; \bold{\theta}) = \sum_{i=1}^{N} \ln p(\bold{x}_i; \bold{\mu}, \bold{\Sigma}) = -\frac{Nd}{2} \ln (2\pi) - \frac{N}{2} \ln |\bold{\Sigma}| - \frac{1}{2} \sum_{i=1}^{N} (\bold{x}_i - \bold{\mu})^T \bold{\Sigma}^{-1} (\bold{x}_i - \bold{\mu})
$$

The derivative of log-likelihood wrt $\bold{\mu}$:

$$
\frac{\partial}{\partial \bold{\mu}} \ln \mathcal{L}(\mathcal{D}; \bold{\theta}) = \sum_{i=1}^{N} \bold{\Sigma}^{-1} (\bold{x}_i - \bold{\mu})
$$

Set the derivative to 0, and we can solve that:

$$
\hat{\bold{\mu}} = \frac{1}{N} \sum_{i=1}^{N} \bold{x}_i
$$

$\hat{\bold{\mu}}$ is an unbiased estimator of $\bold{\mu}$: $\mathbb{E}[\hat{\bold{\mu}}] = \bold{\mu}$

Similarly, by computing the derivative of the log-likelihood wrt $\bold{\Sigma}$ and setting to be zero, we have

$$
\hat{\bold{\Sigma}}_{MLE} = \frac{1}{N} \sum_{i=1}^{N} (\bold{x}_i - \hat{\bold{\mu}})(\bold{x}_i - \hat{\bold{\mu}})^T
$$

However, the above estimation is biased:

$$
\mathbb{E}[\hat{\bold{\Sigma}}] = \frac{N - 1}{N} \bold{\Sigma}
$$

Hence, we need to correct the bias by timing $\hat{\bold{\Sigma}}$ with $\frac{N}{N - 1}$:

$$
\tilde{\bold{\Sigma}}_{\text{unbiased}} = \frac{1}{N - 1}\sum_{i=1}^{N} (\bold{x}_i - \hat{\bold{\mu}})(\bold{x}_i - \hat{\bold{\mu}})^T
$$

In conclusion:

Suppose $\mathcal{D} = \{x_1, ..., x_N\}$ and each data point $\bold{x}_i$ has $d$ dimensions, drawn from a Gaussian distribution with unknown $\bold{\mu}$ and $\bold{\Sigma}$

$$
p(\bold{x} | \bold{\mu}, \bold{\Sigma}) = \frac{1}{(2\pi)^{\frac{d}{2}}} \frac{1}{|\bold{\Sigma}|^{\frac{1}{2}}} \exp \left( - \frac{1}{2} (\bold{x} - \bold{\mu})^T \bold{\Sigma}^{-1}(\bold{x} - \bold{\mu})\right)
$$

The unbiased estimators for $\bold{\mu}$ and $\bold{\Sigma}$ are:

$$
\begin{aligned}
    \hat{\bold{\mu}} &= \frac{1}{N} \sum_{i=1}^{N} \bold{x}_i \\
    \tilde{\bold{\Sigma}} &= \frac{1}{N - 1}\sum_{i=1}^{N} (\bold{x}_i - \hat{\bold{\mu}})(\bold{x}_i - \hat{\bold{\mu}})^T
\end{aligned}
$$

## Summary: MLE for General Distributions

Suppose $\mathcal{D} = \{x_1, ..., x_N\}$ and each data point $\bold{x}_i$ has $d$ dimensions, drawn from a distribution with unknown parameter $\bold{\theta}$

$$
\bold{x}_i \sim p(\bold{x} | \bold{\theta})
$$

1. Compute the likelihood of $\bold{\theta}$ given sample $\mathcal{D}$:

$$
    \mathcal{L}(\mathcal{D}; \bold{\theta}) \triangleq P(\mathcal{D} | \bold{\theta}) = \prod_{i=1}^{N} P(\bold{x}_i | \bold{\theta}) \propto \prod_{i=1}^{N} p(\bold{x}_i | \bold{\theta})
$$

2. Compute the log-likelihood of $\bold{\theta}$ given $\mathcal{D}$: $\ln \mathcal{L}(\mathcal{D}; \bold{\theta})$

3. Compute derivative of $\ln \ln \mathcal{L}(\mathcal{D}; \bold{\theta})$ wrt $\bold{\theta}$, and set it to 0

$$
\nabla_{\bold{\theta}} \ln \mathcal{L}(\mathcal{D}; \bold{\theta}) = 0
$$

4. Solve the above system of equations, to obtain the maximum likelihood estimate $\hat{\bold{\theta}}$

# Is MLE the Best Choice?

-   What if you only have a single data point $\bold{x}_1$?
-   The MLE $\bold{\mu} = \bold{x}_1$
-   Estimating the entire distribution from a single data point is unwise

## Bayesian Approach

We may introduce a prior distribution $P(\bold{\theta})$ that represents our belief about $\bold{\theta}$ in the absence of data

$$
P(\bold{\theta} | \mathcal{D}) = \frac{P(\mathcal{D} | \bold{\theta}) P(\bold{\theta})}{P(\mathcal{D})} = \frac{P(\mathcal{D} | \bold{\theta}) P(\bold{\theta})}{\int P(\mathcal{D} | \bold{\theta}) P(\bold{\theta}) d\bold{\theta}}
$$

-   $P(\bold{\theta})$ is our prior: Our current belief about $\bold{\theta}$ in the absence of any data
-   $P(\mathcal{D} | \bold{\theta})$ is our likelihood: The probability that we get our dataset $\mathcal{D}$ with our belief of $\bold{\theta}$ being the underlying distribution
-   $P(\bold{\theta} | \mathcal{D})$ is our posterior: Our updated beliefs on the underlying distribution because of the evidence from new data
-   $\int P(\mathcal{D} | \bold{\theta}) P(\bold{\theta}) d\bold{\theta}$ is our normalisation constant
-   If the likelihood is overly concentrated, the prior provides some smoothing effect

# Density Estimation Approaches: Nonparametric Density Estimation

-   Without assuming any form for the underlying density
-   Assume that similar inputs have similar outputs: if $\bold{x}_i$ and $\bold{x}_j$ are similar, then $P(\bold{x}_i)$ and $P(\bold{x}_j)$ are similar
-   Approaches
    -   Histogram estimator
    -   Naive Estimator/Parzen Windows/Kernel Estimator
    -   KNN Estimator

Assume that $\mathcal{D} = \{\bold{x}_1, ..., \bold{x}_N\}$ is drawn from some unknown probability density function $p(\bold{x})$

-   We want to learn the estimator $\hat{p}(\bold{x})$ for $p(\bold{x})$
-   We first focus on the univariate case ($x_i$ is a scalar), and this can be generalised easily to the multivariate case

## Histogram Estimator

-   Partition $x$ into distinct bins of fixed width $\Delta$
-   Count the number $N_t$ of data points falling into bin $t$
-   Turn this count into a normalised probability density, by dividing by the total number of observed data points $N$ and by the width $\Delta$ of the bins:

$$
p_t = \frac{N_t}{N \Delta}
$$

-   The model for the density p(\bold{x})$ is constant over the width of each bin: Find the bin where $\bold{x}$ is in (e.g. bin $t$), then

$$
\hat{p}(\bold{x}) = \frac{\# \{ \bold{x}_i | \bold{x}_i \text{ in the same bin as } \bold{x} \}}{N \Delta} = p_t
$$

For a bin $t$, given a density function, the probability that a data instance falls into bin $t$ is:

$$
P_t = \int_{\Delta} p(\bold{x}) d\bold{x} = p_t(\bold{x}) \Delta
$$

On the other hand,

$$
P_t = \frac{\# \{ \bold{x}_i | \bold{x}_i \in bin_t \}}{N} = \frac{N_t}{N}
$$

Therefore

$$
p_t(\bold{x}) \Delta = \frac{N_t}{N} \implies p_t(\bold{x}) = \frac{N_t}{N \Delta}
$$

Choosing width $\Delta$:

-   When $\Delta$ is very small, resulting density is quite spiky, and hallucinates a lot of structure not present in the true density
-   When $\Delta$ is very big, the resulting density is quite smooth, and consequently fails to capture enough detail of the true density

### Analysis on Histogram Estimator

Advantages

-   Simple to evaluate and simple to use
-   One can throw away $\mathcal{D}$ once the histogram is computed
-   Can be updated incrementally

Disadvantages

-   The estimated density has discontinuities due to the bin edges, rather than any property of the underlying density
-   Scales poorly to multivariate case: We would have $m^d$ bins if we divided each feature (dimension) in a $d$-dimensional space into $m$ bins

## Naive Estimator: An Alternative to Histogram Estimator

In histogram estimator, besides $\Delta$, we have to choose an origin $x_0$ aswell, the bins are the intervals defined as

$$
[\bold{x}_0 + m\Delta, \bold{x}_0 + (m + 1)\Delta), m \in \mathbb{Z}
$$

The Naive Estimator does not need to set an origin

$$
\hat{p}(\bold{x}) = \frac{\# \{\bold{x}_i | \bold{x}_i \text{ in the same bin as } \bold{x}\}}{N \Delta}
$$

For Naive Estimator, given $\bold{x}$, we use $\bold{x}$ as the center to create a bin of length $\Delta$

$$
\hat{p}(\bold{x}) = \frac{\# \{ \bold{x}_i | \bold{x} - \frac{\Delta}{2} \leq \bold{x}_i \leq \bold{x} + \frac{\Delta}{2} \}}{N \Delta}
$$

The Naive Estimator can also be written as:

$$
\hat{p}(\bold{x}) = \frac{1}{N \Delta} \sum_{i=1}^{N} w \left( \frac{\bold{x}_i - \bold{x}}{\Delta} \right)
$$

-   where $w$ is a windowing function:

$$
w(u) = \begin{cases}
    1 & -\frac{1}{2} \leq u < \frac{1}{2} \\
    0 & \text{otherwise}
\end{cases}, \ u = \frac{\bold{x}_i - \bold{x}}{\Delta}
$$

This is also known as the Parzen windows Estimator

### Generalisation to Multivariate

Suppose the observed data points $\mathcal{D} = \{\bold{x}_1, ..., \bold{x}_N\}$ are $d$-dimensional

-   In a $d$-dimensional space, we define $\mathcal{R}$ as a $d$-dimensional hypercube with $h$ being the length of each edge. Then the volume of the hypercube is $V = h^d$
-   The windowing function is now defined as:

$$
w(\bold{u}) = \begin{cases}
    1 & -\frac{1}{2} \leq u_j < \frac{1}{2}, \ \forall j \in \{1, ..., d\} \\
    0 & \text{otherwise}
\end{cases}
$$

-   We define a hypercube of length $h$ centered at $\bold{x}$. If another data point $\bold{x}_i$ falls within the hypercube, the count is increased by 1

We now rewrite the estimator to generalise to multivariate

$$
\hat{p}(\bold{x}) = \frac{1}{NV} \sum_{i=1}^{N} w\left( \frac{\bold{x}_i - \bold{x}}{h} \right)
$$

# K-Nearest Neighbors Estimator

In previous approaches $V$ or $\Delta$ for univariate, is fixed for different queries $\bold{x}$'s

-   KNN Estimator adapts the amount of smoothing to the local density of the data, and the degree of smoothing is controlled by $K$, the number of neighbors

$$
p(\bold{x}) = \frac{K}{NV_x}
$$

-   $K$: Consider $K$ nearest neighbors of $\bold{x}$
-   $N$: Total data points
-   $V_x$: Volume of space centered at $\bold{x}$ that exactly contains $K$ nearest neighbors of $\bold{x}$

Note that the number of nearest neighbors is typically much smaller than the number of training poitns $K << N$

## Univariate Case

$$
\hat{p}(\bold{x}) = \frac{K}{N (2 d_K(\bold{x}))}
$$

-   Fix $K$, the number of observed data points to fall in the bin, and compute the bin size

With a predefined $K$, for a particular data point $\bold{x}$

1. Compute distance between $\bold{x}$ and all observed data, e.g. Euclidean distance $\lVert \bold{x} - \bold{x}_i \rVert_2$
2. Sort observed data points based on the distances in ascending order

    $$
    d_1(\bold{x}) \leq d_2(\bold{x}) \leq \cdots \leq d_j(\bold{x}) \leq \cdots \leq d_N(\bold{x})
    $$

    - $d_1(\bold{x})$ is the distance of $\bold{x}$ to its nearest observed instance
    - $d_j(\bold{x})$ is the distance of $\bold{x}$ to the $j$-th nearest observed instance

3. The KNN density estimate is

    $$
        \hat{p}(\bold{x}) = \frac{K}{NV_{d_K(\bold{x})}}
    $$

    - $V_{d_K(\bold{x})}$: the volume of the d-ball (A ball in $d$-dimensional Euclidean space) of the radius $d_K(\bold{x})$
    - [Look-up table for a volume of an $n$-ball](https://en.wikipedia.org/wiki/Volume_of_an_n-ball)

# Appendix: Kernel Estimator

To get a smooth estimate, we use a smooth weight function (kernel function) e.g. the Gaussian kernel

-   For the univariate case, i.e. each data point is 1-dimensional, the Gaussian kernel is defined as

$$
k(u) = \frac{1}{\sqrt{2 \pi}} \exp \left( - \frac{u^2}{2} \right)
$$

-   The kernel estimator is computed via

$$
\hat{f}(x) = \frac{1}{N \Delta} \sum_{i=1}^{N} k\left( \frac{x_i - x}{\Delta} \right)
$$

-   For the multivariate case (i.e. each data point is $d$-dimensional) the Gaussian kernel is defined as

$$
k(\bold{u}) = \frac{1}{(2\pi)^{d/2}} \exp \left( - \frac{\lVert \bold{u} \rVert_2^2}{2} \right)
$$

-   The kernel estimator is computed via

$$
\hat{f}(x) = \frac{1}{NV} \sum_{i=1}^{N} k\left( \frac{\bold{x}_i - \bold{x}}{h} \right)
$$

# Dimensionality Rediction

High-level idea: Summarise observed high-dimensional data points with low-dimensional vectors

$$
\mathbb{R}^{N \times D} \xrightarrow{g: \bold{X} \to \bold{Z}} \mathbb{R}^{N \times K}, D >> K
$$

# Why Dimensionality Reduction

Avoid curse of dimensionality

- Decision boundary in SVM: $\bold{w} \cdot \bold{x} + b = 0$
- Linear regresion: $f(x) = \bold{x} \cdot \bold{x}$
- Perceptron: $f(x) = sign(\bold{w} \cdot \bold{x})$
- One parameter to learn for every input dimension
- Difficult to accurately estimate the best parameters when the number of data points is small

Dimensionality reduction is used to identify features/transformations of features that capture the most important data characteristics

- All measurements contain error or noise:

$$
x = \tilde{x} + e, e \sim \mathcal{N}(0, \sigma I)
$$

Valid hypothesis: Data resides on a straight line, but noise is isotropic (equal amount of variation) in all 3 dimensions

Dimensionality reduction identifies important variations in the data, and discards the noise

Benefits:

- Reduce storage requirements
- Allows visualisation in 2D/3D
- Reduces noise and improves the performance of machine learning

# Dimensionality Reduction Approaches

Feature selection

- Select a subset of $K$ features from the original $D$ features, to represent each data instance
  - Brute-force approach
  - Greedy search

Feature extraction

- To learn $K$ new features from the original $D$ features, to represent each data instance
  - Linear combination of original features
    - Principal component analysis
  - Nonlinear combination of original features

# Principal Component Analysis

One of the most widely used unsupervised dimensionality reduction methods

- Takes a data matrix of $N$ data points by $D$ features, summarizes its principal components that are linear combinations fo the original $D$ variables
- The first $K$ components display as much as posible of the variation among data instances

# Linear Algebra Review

## Change of Basis

Consider a projection of a data point $\bold{x}$ onto a vector $\bold{v}$

- The projection of $\bold{x}$ onto $\bold{v}$ is:

$$
\frac{\bold{v}^T \bold{x}}{\lVert \bold{v} \rVert_2} = \frac{\lVert \bold{v} \rVert_2 \lVert \bold{x} \rVert_2 \cos \theta}{\lVert \bold{v} \rVert_2} = \lVert \bold{x} \rVert_2 \cos \theta
$$

For simplicity, we consider $\bold{v}$ such that $\lVert \bold{v} \rVert_2 = 1$

## Orthogonal and Orthonormal Vectors

2 vectors $\bold{v}_1, \bold{v}_2$ are orthogonal if they are perpendicular to each other

$$
\bold{v}_1 \cdot \bold{v}_2 = 0
$$

A set of vectors $\{ \bold{v}_1, ..., \bold{v}_D\}$ are mutually orthogonal if every pair of vectors $i \neq j, \bold{v}_i \cdot \bold{v}_j = 0$

A set of vectors $\{ \bold{v}_1, ..., \bold{v}_D\}$ are mutually orthonormal if

- Every pair of vectors $i \neq j, \bold{v}_i \cdot \bold{v}_j = 0$
- $\lVert \bold{v}_i \rVert_2 = 1$
- A set of orthogonal vectors can be normalized to orthonormal with $\frac{\bold{v}_i}{\lVert \bold{v}_i \rVert_2}$

## Eigenvalues and Eigenvectors

Given a $D \times D$ square matrix $\bold{A}$, if there exists a non-zero $D$-dimensional vector $u$ such that $\bold{Au} = \lambda\bold{u}$, then $\bold{u}$ is an eigenvector of $\bold{A}$, and the scalar $\lambda$ is the corresponding eigenvalue

- There are $D$ eigenvectors and eigenvalues
- An eigenvalue can be positive, negative or zero
- A eigenvector cannot be a zero vector
- For real symmetric matrices, eigenvectors are orthogonal to each other

In general, if $\bold{A} \in \mathbb{R}^{D \times D}$ can be written as $\bold{A} = \bold{X}^T \bold{X}, \bold{X} \in \mathbb{R}^{N \times D}$, then the eigenvectors and eigenvalues of $\bold{A}$ can be computed by performing singular value decomposition on $\bold{X}$

# Principal Component Analysis

2 views

1. The variance preservation view
   - The first $K$ components display as much of the variation in the dat instances as possible
2. The minimum reconstruction error view
   - The first $K$ components preserve maximum information of the original data instances

## PCA: Geometric Rationale

Data may only occupy a small subspace of the high-dimensional $\mathbb{R}^D$ space

- We want to pick a few $(< D)$ basis vectors taht the data are projected onto
- How to pick basis vectors?

Goal: Find a projection or rotation of the original $D$-dimensional coordinate system to capture the largest amount of variation in the data

- Ordered such that the 1st principal component has the highest variance, the 2nd principal component has the next highest variance, until the $D$-th component which has the lowest variance
- Principal components are orthogonal to each other

Math outline

- Write down the data variance after projecting data to a single direction $\bold{u}$
- Find the direction that maximises the variance after projection
- This direction is the eigenvector of the data covariance matrix with the maximum eigenvalue

## PCA: Preprocessing

Consider input $\mathcal{D} = \{\bold{x}_1, ..., \bold{x}_N\}$ a set of observed data

1. Center the data point such that the mean is 0:

   $$
       \hat{\mu} = \frac{1}{N} \sum_{i=1}^{N} \bold{x}_i \\
       \tilde{\bold{x}}_i = \bold{x}_i - \hat{\bold{\mu}}
   $$

2. Compute sample covariance matrix

   $$
       \tilde{\bold{\Sigma}} = \frac{1}{N - 1} \sum_{i=1}^{N} \tilde{\bold{x}}_i \tilde{\bold{x}}_i^T = \frac{1}{N - 1} \tilde{\bold{X}}^T \tilde{\bold{X}}
   $$

   where $\bold{X} \in \mathbb{R}^{N \times D}$

3. Project $\tilde{\bold{x}}_i$ onto a new direction $\bold{u}$ such that $\lVert \bold{u} \rVert_2 = 1$

   $$
       \tilde{\bold{x}}_i^T \bold{u} = (\bold{x}_i - \hat{\bold{\mu}})^T \bold{u}
   $$

4. After projection onto $\bold{u}$, the mean of the data points is still 0

   $$
       \frac{1}{N} \sum_{i=1}^{N} \tilde{\bold{x}}_i^T \bold{u} = 0
   $$

5. The variance of the projected data points is

   $$
       \frac{1}{N - 1} \sum_{i=1}^{N} (\tilde{x}_i^T \bold{u} - 0)^2 = \frac{1}{N - 1} \sum_{i=1}^{N} \bold{u}^T \tilde{\bold{x}}_i \tilde{\bold{x}}_i^T \bold{u} = \bold{u}^T \tilde{\bold{\Sigma}} \bold{u}
   $$

## Variance Maximisation

Let us find $\bold{u}$ that maximises the variance $\bold{u}^T \tilde{\bold{\Sigma}} \bold{u}$. The optimisation problem is:

$$
\bold{u}^* = \argmax_\bold{u} \bold{u}^T \tilde{\bold{\Sigma}} \bold{u} \ s.t. \ \lVert \bold{u} \rVert_2^2 = 1
$$

It can be solved using the langrangian:

$$
\bold{u}^T \tilde{\bold{\Sigma}} \bold{u} + \lambda (1 - \bold{u}^T \bold{u})
$$

Differentiating and setting the gradient wrt $\bold{u}$ to be 0, we have

$$
2 \tilde{\bold{\Sigma}} - 2 \lambda \bold{u} = 0 \implies \tilde{\bold{\Sigma}} \bold{u} = \lambda \bold{u}
$$

- The desired direction $\bold{u}$ is an eigenvector of $\tilde{\bold{\Sigma}}$
- The largest postprojection variance is therefore

  $$
      \bold{u}^T \tilde{\bold{\Sigma}} \bold{u} = \bold{u}^T \lambda \bold{u} = \lambda \lVert \bold{u} \rVert_2^2 = \lambda
  $$

- To maximise the post-projection variance, we should pick the eigenvector with the largest eigenvalue

After finding the first eigenvector $\bold{u}_1$, we look for the second direction $\bold{u}_2$:

$$
    \bold{u}_2^* = \argmax_{\bold{u}_2} \bold{u}_2^T \tilde{\bold{\Sigma}} \bold{u}_2 \ s.t. \ \lVert \bold{u}_2 \rVert_2^2 = 1 \ and \ \bold{u}_1^T \bold{u}_2 = 0
$$

- $\lVert \bold{u}_2 \rVert_2^2 = 1$: Make sure $\bold{u}_2$ is normal
- $\bold{u}_1^T \bold{u}_2 = 0$: Make sure $\bold{u}_1$ and $\bold{u}_2$ are orthogonal

The complete solution to PCA is the $K$ eigenvectors of $\tilde{\bold{\Sigma}}$ with the largest eigenvalues

## Algorithm

Consider input $\mathcal{D} = \{\bold{x}_1, ..., \bold{x}_N\}$ a set of observed data, and $\bold{X} = \begin{pmatrix} \bold{x}_1^T \\ \vdots \\ \bold{x}_N^T \end{pmatrix}$. Also, note the SVD decomposition of $\bold{X} = \bold{V} \bold{D} \bold{U}^T$, where $\bold{V}$ and $\bold{U}$ are orthonormal matrices, $\bold{D}$ is a rectangular diagonal matrix

The procedure for PCA is as follows:

1. Center the data point such that the mean across each feature is 0:

   $$
   \begin{aligned}
        \hat{\mu} &= \frac{1}{N} \sum_{i=1}^{N} \bold{x}_i \\
       \tilde{\bold{x}}_i &= \bold{x}_i - \hat{\bold{\mu}}
   \end{aligned}


   $$

2. Compute sample covariance matrix

   $$
   \begin{aligned}
        \tilde{\bold{\Sigma}} &= \frac{1}{N - 1} \sum_{i=1}^{N} \tilde{\bold{x}}_i \tilde{\bold{x}}_i^T \\
        &= \frac{1}{N - 1} \tilde{\bold{X}}^T \tilde{\bold{X}}
   \end{aligned}
   $$

   where $\bold{X} \in \mathbb{R}^{N \times D}$

3. Compute eigenvectors of $\tilde{\bold{\Sigma}}$, $\{\bold{u}_1, ..., \bold{u}_D \}$ which are sorted based on their eigenvalues in non-decreasing order: $\lambda_1 \geq \lambda_2 \cdots \geq \lambda_D$ by performing SVD on $\bold{X}$, and then calculating $\bold{U} \tilde{\bold{D}}$

   $$
       \begin{aligned}
           \tilde{\bold{\Sigma}} &= \frac{1}{N - 1} \tilde{\bold{X}}^T \tilde{\bold{X}} \\
           &= \frac{1}{N - 1} (\bold{VDU}^T)^T(\bold{VDU}^T) \\
           &= \bold{U} \tilde{\bold{D}} \bold{U}^T \\
           \tilde{\bold{\Sigma}} \bold{U} &= \bold{U} \tilde{\bold{D}} \bold{U}^T \bold{U} = \bold{U} \tilde{\bold{D}}
       \end{aligned}
   $$

   Note that $\tilde{\bold{D}} = \frac{1}{N - 1} \bold{D}^T \bold{D}, \bold{V}^T \bold{V} = \bold{I}$

   The $i$-th column of the matrix $\bold{U} \tilde{\bold{D}}$ is the $i$-th largest eigenvector, multiplied by the square of its eigenvalue: $\lambda_i^2 \bold{u}_i$

   $$
       \bold{U} \tilde{\bold{D}} = \begin{pmatrix}
           \vert & \vert & & \vert \\
           \lambda_1^2 \bold{u}_1 & \lambda_2^2 \bold{u}_2 & \cdots & \lambda_D^2 \bold{u}_D \\
           \vert & \vert &  & \vert
       \end{pmatrix}
   $$

4. Select the first $K$ eigenvectors (columns) to construct the principal components. This is obtained from the first $K$ columns from $\bold{U}$ after SVD decomposition

With the $K$ eigenvectors, we form the matrix $\bold{U}_k = [\bold{u}_1, ..., \bold{u}_K]$, where the $i$-th column of $\bold{U}_k$ is the eigenvector with the $i$-th largest eigenvalue. Then we can compute

$$
\bold{Z} = \bold{X} \bold{U}_k
$$

## Minimum Reconstruction Error

We know that the length of projection of $\bold{x}$ onto $\bold{v}$ (where $\bold{v}$ is a normal vector) is:

$$
\frac{\bold{v}^T \bold{x}}{\lVert \bold{v} \rVert_2} = \bold{v}^T \bold{x}
$$

Hence, the vector after the projection is $(\bold{v}^T \bold{x}) \bold{v}$

We also know that for any vector $\bold{x} \in \mathbb{R}^D$, we can perfectly represent $\bold{x}$ using $D$ orthonormal vectors $\bold{v}_1, ..., \bold{v}_D$ which form an orthonormal basis

Given any orthonormal basis $\bold{v}_1, ..., \bold{v}_D$, a centered data point $\tilde{\bold{x}}_i$ can be written as:

$$
\tilde{\bold{x}}_i = \sum_{j=1}^{D} \alpha_{ij} \bold{v}_j \ , \alpha_{ij} = \bold{v}_j^T \tilde{\bold{x}}_i
$$

However, if we only use $K$ ($K < D$) basis vectors, there will be some reconstruction error:

$$
\hat{\bold{x}_i} = \sum_{j = 1}^{K} \alpha_{ij} \bold{v}_j \\
\lVert \tilde{\bold{x}}_i - \hat{\bold{x}_i} \rVert_2^2 = \left\lVert \sum_{j = K + 1}^{D} \alpha_{ij} \bold{v}_j \right\rVert_2^2
$$

For simplicity, consider $K = D - 1$ (We only omit 1 basis vector $\bold{v}_j$), which one should we omit?

The reconstruction error, summed over all data points is:

$$
\begin{aligned}
    \sum_{i=1}^{N} \lVert \alpha_{ij} \bold{v}_j \rVert_2^2 &= \sum_{i=1}^{N} (\bold{v}_j^T \tilde{\bold{x}}_i^T \bold{v}_j)^T (\bold{v}_j^T \tilde{\bold{x}}_i^T \bold{v}_j) \\
    &= \sum_{i=1}^{N} (\bold{v}_j^T \tilde{\bold{x}}_i)(\tilde{\bold{x}}_i^T \bold{v}_j) \bold{v}_j^T \bold{v}_j \\
    &= \sum_{i=1}^{N} \bold{v}_j^T (\tilde{\bold{x}}_i \tilde{\bold{x}}_i^T) \bold{v}_j \\
    &= (N - 1) \bold{v}_j^T \tilde{\bold{\Sigma}} \bold{v}_j
\end{aligned}
$$

- Note that $\tilde{\bold{\Sigma}} = \frac{1}{N - 1} \sum_{i=1}^{N} \tilde{\bold{x}}_i \tilde{\bold{x}}_i^T$

The optimisation problem is:

$$
\bold{v}_1^* = \argmin_{\bold{v}_1} \bold{v}_1^T \tilde{\bold{\Sigma}} \bold{v}_1 \ s.t. \ \lVert \bold{v}_1 \rVert_2^2 = 1
$$

By setting the gradient of the Lagrangian wrt $\bold{v}$ to be 0, we have

$$
2 \tilde{\bold{\Sigma}} \bold{v}_1 - 2 \lambda \bold{v}_1 = 0 \implies \tilde{\bold{\Sigma}} \bold{v}_1 = \lambda \bold{v}_1
$$

We can now rewrite:

$$
\bold{v}_1^T \tilde{\bold{\Sigma}} \bold{v}_1 = \bold{v}_1^T \lambda \bold{v}_1 = \lambda
$$

Hence, we should discard the eigenvector with the least eigenvalue

Similar to the maximisation problem we saw earlier, the solution to the minimisation problem contains the $D - K$ eigenvectors of $\tilde{\bold{\Sigma}}$ that have the least eigenvalues

- Hence, PCA may be understood as selecting $K$ orthonormal basis vectors that minimise the reconstruction errors of the centered data points

## Determining $K$

The wrapper approach

- Dimensionality reduction is usually an intermediate step for some downstream tasks such as classification/regression/clustering
- Use cross-validation based on the performance of the final task to tune the value of $K$

Based on the percentage of variance preserved:

$$
p_{var} = \frac{\sum_{i=1}^{K} \lambda_i}{\sum_{i=1}^{D} \lambda_i} \times 100
$$

- All the $\lambda_i$s are nonnegative, because $X^T X$ is a positive semi-definite matrix
- Predefine a value for the percentage of variance to determine the value of $K$

# Obtaining Eigenvectors via SVD

Consider

$$
\bold{X} = \bold{VDU}^T
$$

Then, $\bold{A}$ can be rewritten as

$$
\begin{aligned}
\bold{A} &= \bold{X}^T \bold{X} \\
&= (\bold{VDU}^T)^T(\bold{VDU}^T) \\
&= \bold{U} \bold{D}^T \bold{V}^T \bold{V} \bold{D} \bold{U}^T \\
&= \bold{U}\bold{D}^T \bold{D} \bold{U}^T
\end{aligned}
$$

Since $\bold{V}$ is an orthonormal matrix, $\bold{V}^T \bold{V} = \bold{I}$

Now, we denote $\tilde{\bold{D}} = \bold{D}^T \bold{D}$, and hence

$$
\bold{A} = \bold{U} \tilde{\bold{D}} \bold{U}^T
$$

$\tilde{\bold{D}}$ is a $d$-by-$d$ diagonal matrix, with diagonal elements $\lambda_1^2 \geq \lambda_2^2 \geq \cdots \geq \lambda_d^2 \geq 0$

Then,

$$
\bold{A} \bold{U} = \bold{U} \tilde{\bold{D}} \bold{U}^T \bold{U} = \bold{U} \tilde{\bold{D}}
$$

where $\bold{AU} = (\bold{A} \times \bold{u}_1, \bold{A} \times \bold{u}_2, ..., \bold{A} \times \bold{u}_d) = (\lambda_1^2 \times \bold{u}_1, \lambda_2^2 \times \bold{u}_2, ..., \lambda_d^2 \times \bold{u}_d)$

$$
\bold{A} \bold{u}_i = \lambda_i^2 \bold{u}_i
$$

Each column $\bold{u}_i$ of $\bold{U}$ is an eigenvector of $\bold{A}$ with the eigenvalue $\lambda_i^2$

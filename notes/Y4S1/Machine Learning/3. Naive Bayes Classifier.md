# Naive Bayes Classifier

Recall the bayesian classifier:

Make predictions on maximum posterior

$$
y^* = c^* \text{ if } c^* = \argmax_c \frac{P(\bold{x} | y = c) P(y = c)}{P(\bold{x})}
$$

However, it is difficult to estimate $P(\bold{x} | y = c)$ because $\bold{x}$ contains many input variables

The Naive Bayes Classifier works as follows:

-   Assume that features are conditionally independent given the class labels

$$
P(\bold{x} | y = c) = \prod_{i = 1}^{d} P(x_i | y = c)
$$

## Independence

-   Let $A$ and $B$ be 2 random variables
-   $A$ is said to be independent of $B$ if the following condition holds: $P(A, B) = P(A) P(B)$

## Conditional Independence

Let $A, B, C$ be three random variables

$A$ is said to be conditionally independent of $B$ given $C$ if

$$
P(A | B, C) = P(A | C)
$$

The conditional independence between $A$ and $B$ given $C$ can also be written as

$$
P(A, B | C) = P(A | C) P(B | C)
$$

## Naive Bayes: Induction

$$
\begin{aligned}
P(\bold{x} | y = c) &= P(x_1, x_2, ..., x_d | y = c) \\
&= P(\bold{X}^{(d - 1)}, x_d | y = c) \\
&= P(\bold{X}^{(d - 1)} | y = c) P(x_d | y = c) & \text{Conditional indepdence} \\
&= P(\bold{X}^{(d - 2)}, x_{d-1} | y = c) P(x_d | y = c) \\
&= P(\bold{X}^{(d - 2)} | y = c) P(x_{d-1} | y = c) P(x_d | y = c) \\
& \cdots \text{Recursively apply conditional independence} \\
&= \prod_{i=1}^{d} P(x_i | y = c)
\end{aligned}
$$

Note: $\bold{X}^{(d - 1)} = [x_1, \cdots, x_{d - 1}]$

# How Naive Bayes Works

Naive Bayes Classifier: $P(\bold{x} | y = c) = \prod_{i = 1}^{d} P(x_i | y = c)$

To classify a test record $\bold{x}^*$, we compute the posteriors for each class by using:

$$
P(y = c | \bold{x}) = \frac{\left(\prod_{i=1}^{d} P(x_i^* | y = c)\right) P(y = c)}{P(\bold{x}^*)}
$$

Since $P(\bold{x}^*)$ is a constant for each class $c$, it is sufficient to choose the class that maximises the numerator

$$
y^* = c^* \iff c^* = \argmax_c \left(\prod_{i=1}^{d} P(x_i^* | y = c)\right) P(y = c)
$$

# Estimating Priors

Estimate class probability:

$$
P(y = c) = \frac{|y = c|}{N}
$$

# Estimate conditional probabilities for discrete features:

$$
P(x_i = k | y = c) = \frac{|x_i = k \land y = c|}{|y = c|}
$$

# Estimate Conditional Probabilities for Continuous Features

For continuous features, we use probability density estimation (covered in the future)

-   Assume values of a feature given a class label follow a Gaussian distribution (Assume $P(x_i | y = c)$ is gaussian)
-   Use training data in the class $c$ to estimate parameters of distribution (mean $\mu$, and variance $\sigma^2$)
-   Once probability density function is known, use it to estimate conditional probability
-   For each class $c$, assume features $x_i$ follow Gaussian distribution

$$
P(x_i | y = c) = \frac{1}{\sqrt{2 \pi \sigma_{ic}^2}} e^{-\frac{(x_i - \mu_{ic})^2}{2\sigma_{ic}^2}}
$$

-   Suppose there are $N_c$ instances in class $c$, then

$$
\mu_{ic} = \frac{1}{N_c} \sum_{j = 1}^{N_c} {x_{i}}_{j} \\
\sigma_{ic}^2 = \frac{1}{N_c - 1} \sum_{j = 1}^{N_c} ({x_i}_j - \mu_{ic})^2
$$

-   Note that ${x_{i}}_j$ is the value of feature $x_i$ of the $jth$ training data in class $c$

## Additional Notes

Since the probability density function is continuous, probability is defined as the area under the curve of the pdf

We should be computing

$$
P(k \leq x_i \leq k + \epsilon) = \int_{k}^{k + \epsilon} \frac{1}{\sqrt{2 \pi \sigma_{ic}^2}} e^{-\frac{(x_i - \mu_{ic}^2)}{2 \sigma_{ic}^2}} \ dx_i
$$

Assuming that $\frac{1}{\sqrt{2 \pi \sigma_{ic}^2}} e^{-\frac{(k - \mu_{ic}^2)}{2 \sigma_{ic}^2}} \approx \frac{1}{\sqrt{2 \pi \sigma_{ic}^2}} e^{-\frac{(k + \epsilon - \mu_{ic}^2)}{2 \sigma_{ic}^2}}$, we can use the area under the rectangle

$$
P(k \leq x_i \leq k + \epsilon) = \frac{\epsilon}{\sqrt{2 \pi \sigma_{ic}^2}} e^{-\frac{(k - \mu_{ic}^2)}{2 \sigma_{ic}^2}}
$$

-   Since $\epsilon$ appears as a constant multiplicative factor for each class, it cancels out when comparing posterior probabiliies $P(y = c | \bold{x})$ for each class

$$
\begin{aligned}
P(y = 0 | x = k) &= P(x = k | y = 0) P(y = 0) \\
&= \frac{\epsilon P(y = 0)}{\sqrt{2 \pi \sigma_0^2}} e^{-\frac{(k - \mu_0^2)}{2 \sigma_0^2}}
\end{aligned}
$$

vs

$$
\begin{aligned}
P(y = 1 | x = k) &= P(x = k | y = 1) P(y = 1) \\
&= \frac{\epsilon P(y = 1)}{\sqrt{2 \pi \sigma_1^2}} e^{-\frac{(k - \mu_1^2)}{2 \sigma_1^2}}
\end{aligned}
$$

Therefore we can still use the following equation to approximate the probability of $x_i = k$ for class $c$

$$
P(x_i = k | y = c) = \frac{1}{\sqrt{2 \pi \sigma_{ic}^2}} e^{-\frac{(k - \mu_{ic})^2}{2 \sigma_{ic}^2}}
$$

# Laplace Estimates

Alternative probability estimation (discrete features)

-   Original

    $$
        P(x_i = k | y = c) = \frac{|x_i = k \land y = c|}{|y = c|}
    $$

-   Laplace (Used to handle unseen data)

    $$
        P(x_i = k | y = c) = \frac{|x_i = k \land y = c| + 1}{|y = c| + n_i}
    $$

    -   $n_i$ is the number of distinct values of $x_i$

# M-Estimate

A more general estimation

-   Original

    $$
        P(x_i = k | y = c) = \frac{|x_i = k \land y = c|}{|y = c|}
    $$

-   M-estimate

    $$
        P(x_i = k | y = c) = \frac{|x_i = k \land y = c| + mp}{|y = c| + m}
    $$

    -   $m$ and $p$ are user-specified parameters
    -   If prior information for $P(x_i = k | y = c)$ is available, then we can set $p$ as the prior

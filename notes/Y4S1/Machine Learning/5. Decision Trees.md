# Decision Trees

A decision tree is a non-parametric supervised learning algorithm, which is utilized for both classification and regression tasks. It has a hierarchical, tree structure, which consists of a root node, branches, internal nodes and leaf nodes.

![](https://images.datacamp.com/image/upload/v1677504957/decision_tree_for_heart_attack_prevention_2140bd762d.png)

From a set of data, we perform induction (to induce a tree), and then on new data, we perform deductino (apply the tree to the new data)

# Decision Tree Induction Algorithms

-   CART, ID3, C4.5, etc
-   High level idea
    -   Given a new application, the goal of asking a series of questions (checking properties of profile) one by one is to find similar profiles in the past until it is confident to make a decision based on the labels of similar applicants
    -   A tree can be learned by splitting training data into subsets based on outcomes of a feature test
    -   This process is recursively applied on each derived subset until the subset at a node has all the same class, or there is no improvement for prediction

Formally,

-   Let $D_t$ be the set of training records that reach a node $t$
-   If $D_t$ contains records that all belong to the same class $y_t$, then $t$ is a leaf node labelled as $y_t$, and we are done with this node
-   Otherwise if $D_t$ is an empty set, then $t$ is a leaf node labelled by the default class $y_d$
-   Otherwise if $D_t$ contains records that belong to more than one class, then a feature is selected to conduct condition test to split the data into smaller subsets
    -   A child node is created for each outcome of the test condition, and the records in $D_t$ are distributed to the children based on the outcomes
    -   Recursively apply the procedure to each subset

# Tree Induction

Greedy strategy

-   Splits records based on a feature test that optimises certain criterion

Issues

-   Determine how to split records
    -   How to specify feature test condition?
        -   Depends on feature types (Discrete vs continuous)
    -   How to determine the best split?
        -   Depends on the number of ways to split (Binary vs multi-way split)
-   Determine when to stop splitting

## How to Specify Attribute Test Condition

For discrete features:

-   Binary features
    -   Generates 2 potential outcomes
-   Discrete features (more than 2 distinct values)
    -   Multi-way split: Use as many partitions as distinct values
    -   Binary split: Divide values into 2 subsets, need to find optimal partitioning

For continuous features

-   Binary split (find a threshold)
-   Multi-way split (find multiple ranges)
-   Have to consider all posible splits and find the best cut
-   Can be very computationally expensive

## How to Determine the Best Split

Intuitively, nodes with a homogeneous class distributions are preferred. We need a measure of "node impurity"

-   A split criterion is defined in terms of the difference in degree of node impurity before and after splitting

We use **entropy** as a measure of impurity. The entropy at a given node $t$ is

$$
\text{Entropy}(t) = - \sum_{c} P(y = c; t) \log_2 P(y = c; t)
$$

![](https://miro.medium.com/v2/resize:fit:565/1*M15RZMSk8nGEyOnD8haF-A.png)

-   For binary class, when $P(y = 0; t) = P(x = 0; t)$ is the highest entropy (1)
-   When all records belong to a single class, lowest entropy (0)

### Information Gain

The information gain from splitting a parent node $p$ into $n$ children nodes $c_1, ..., c_n$ is

$$
\Delta_{\text{info}} = \text{Entropy}(p) - \sum_{i=1}^{n} \frac{n_{c_i}}{n_p} \text{Entropy}(c_i)
$$

Choose a feature whose condition test maximises the gain (minimises the weighted average impurity measures of the children node)

-   $n_{c_i}$ is the number of instances in node $c_i$
-   $n_p$ is the number of instances in $p$

Limitations

-   Disadvantage: Tends to prefer splits that result in large number of partitions, each being small but pure

To overcome this limitation, we can use a gain ratio:

$$
\Delta_{\text{InfoR}} = \frac{\Delta_{\text{info}}}{\text{SplitINFO}}
$$

where $\text{SplitINFO} = -\sum_{i=1}^{P} \frac{n_i}{n} \log_2 \frac{n_i}{n}$

-   $P$ partitions
-   $n_i$ instances in the $i$-th child node
-   $n$ instances in the parent node

## Determine When to Stop Splitting

-   Stop expanding a node when all data instances belong to the same class
    -   Ideal case, but not always possible
-   Stop expanding a node when all the data instances have similar feature values
-   Early termination
    -   Useful to avoid overfitting

# Generalisation

# Underfitting and Overfitting

Overfitting: When test error rate begins to increase, even though training error rate continues to decrease

-   Overfitting results in decision trees that are more complex than necessary
-   Training error no longer provides a good estimate on how well the tree will perform on previously unseen records
-   Need new ways for estimating errors

Underfitting: When the model is too simple, both training and test error rates are large

## Overfitting vs Model Complexity

How do we determine the right model complexity?

-   A model with ideal complexity is the one that produces the lowest generalisation error
-   No knowledge of test data and how well the model will perform on unseen data
-   The best it can do is to estimate the generalisation error of the induced model

## Estimation of Generalisation Errors

Training errors: error on the training set $e(T)$

Generalisation errors: error on previously unseen testing set $e'(T)$

Approaches to estimating generalisation errors

-   Optimistic estimate: $e'(T) = e(T)$
-   Incorporating model complexity
    -   Occam's Razor
        -   Pessimistic Error estimate
-   Use validation set

### Optimistic Estimate

-   Assume that the training set is a good representation of the overall data
-   Training error can be used to provide an optimistic estimate for the generalisation error $e'(T) = e(T)$
-   A decision tree induction algorithm selects the model that produces the lowest training error rate

### Occam's Razor

-   Given 2 models of similar generalisation error, one should prefer the simpler model
-   For complex models, there is a greater chance that it was fitted accidentally by errors in the data
-   Therefore, one should include model complexity when evaluating a model

### Pessimistic Error Estimate

-   Explicitly computes generalisation error as the sum of training error and a penalty term for model complexity

    $$
        e'(T) = e(T) + \Omega(T)
    $$

-   In a decision tree, we can define a penalty term of $k > 0$ on each leaf node:

    $$
        e'(T) = e(T) + \Omega(T) = e(T) + Nk
    $$

    -   $N$ is the number of leaf nodes
    -   $k$ is some value greater than zero. E.g. $k = 0.5$
    -   For example, a tree with 30 leaf nodes, and 10 errors on training out of 1000 instances, and $k = 0.5$
        -   Training error rate = 10/1000 = 1%
        -   Generalisation error = 10 + 30 \* 0.5 = 25
        -   Generalisation error rate = 25 / 1000 = 2.5%

### Validation Set

-   Divide original training data set into 2 smaller subsets
-   One is for training, the other is for validation, used to estimate generalisation error
-   The complexity of the best model is estimated based on the performance of the model on the validation set

## How to Address Overfitting

Pre-pruning (Early stopping)

-   Stop algorithm before it becomes a fully grown tree
-   Typical stopping conditions
    -   Stop if all isntances belong to the same class
    -   Stop if all feature values are the same
-   More restrictive conditions
    -   Stop if number of instances is less than some user-specified threshold
    -   Stop if expanding the current node does not improve generalisation errors

Post-pruning

-   Grow decision tree to its entirety
-   Trim nodes of decision tree in a bottom-up fashion
-   If generalisation error improves after trimming, repalce sub-tree by a new leaf node
-   Class label of leaf node is determined from majority class of instances in the subtree

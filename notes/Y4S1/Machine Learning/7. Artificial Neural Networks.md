# Artificial Neural Networks (ANN)

The study of ANN was inspired by biological neural systems

-   Human brain is a network of densely interconnected neurons, connected to others via dendrites and axons
-   Dendrites and axons transmit electrical signals from one neuron to another
-   Human brain learns by changing the strength of the synaptic connection between neurons
-   ANN is composed of an interconnected assembly of nodes and directed links

# Perceptron

A single node containing some weights $\bold{w}$, a bias $\theta$, and an activation function $a$. It takes in inputs $\bold{x}$ and returns an output $y$

$$
z = f(\bold{x}, \bold{w}) = \sum_{i=1}^{d} w_i x_i - \theta \\
y = a(z)
$$

-   $\bold{x}$ is the input vector
-   $\theta$ is the bias factor
-   $a(z)$ is the activation function, here $a(z) = sign(z)$

## Learning

During training, $\bold{w}$ is adjusted until the outputs of the perceptron become consistent with the true outputs of training data

-   $\bold{w}$ is updated iteratively or in an online learning manner

Algorithm:

1. Let $D = \{(\bold{x}_i, y_i)\}_{i=1}^{N}$ be a set of $n$ training examples
2. Let $t = 0$
3. Initialize $\bold{w}$ with random values $\bold{w}_0$
4. Repeat for each training example:
    1. For a training example $(\bold{x}_i, y_i)$
    2. Compute the predicted output $\hat{y_i}$
    3. Update $\bold{w}_t$: $\bold{w}_{t + 1} = \bold{w}_t + \lambda (y_i - \hat{y_i}) \bold{x}_i$
    4. $t = t + 1$
5. Until stopping condition is met

Update weights based on gradient descent

$$
\bold{w}_{t + 1} = \bold{w}_t - \lambda \frac{\partial \mathcal{L}(\bold{w})}{\partial \bold{w}}
$$

The learning rate $\lambda \in (0, 1]$

Consider the loss function $\mathcal{L}$ for regression: Square error

$$
\mathcal{L} = \frac{1}{2} (y_i - \hat{y_i})^2
$$

We update weights using gradient descent

$$
\frac{\partial \mathcal{L}(\bold{w})}{\partial \bold{w}} = \frac{\partial \mathcal{L}(\hat{y})}{\partial \hat{y}} \frac{\partial \hat{y}(z)}{\partial z} \frac{\partial z(\bold{w})}{\partial \bold{w}}
$$

# Convergence

The decision boundary of a perceptron is a linear hyperplane

-   The perceptron learning algorithm is guaranteed to converge to an optimal solution for linear classification
-   However, if the problem is not linearly separable, the algorithm fails to converge

# Multilayer ANN

Stack multiple layers of neurons before arriving at output

# Activation Functions

-   Sign function (Threshold function)
-   Unipolar sigmoid function: $a(z) = \frac{1}{1 + e^{-\lambda z}}$

# Update weights for Multilayer NN

-   Initialise weights in each layer $(\bold{w}^{(1)}, ..., \bold{w}^{(m)})$
-   Adjust weights such that output of ANN is consistent with class labels of training examples
-   Loss function for each training instance

    $$
        \mathcal{L} = \frac{1}{2} (y_i - \hat{y_i})^2
    $$

-   For each layer $k$, update weights $\bold{w}^{(k)}$ by gradient descent at each iteration $t$

    $$
        \bold{w}_{t + 1}^{(k)} = \bold{w}_{t}^{(k)} - \lambda \frac{\partial \mathcal{L}}{\partial \bold{w}^{(k)}}
    $$

-   Computing gradient wrt weights in each layer is computationally expensive: We can use backpropagation

## Backpropagation

-   Initialize weights $(\bold{w}^{(1)}, ..., \bold{w}^{(k)})$
-   Forward pass: Each training example $(\bold{x}_i, y_i)$ is used to compute outputs of each hidden layer and generate the final output $\hat{y}_i$ based on the ANN
-   Backpropagation: Starting with the output layer, propagate the error back to the previous layer in order to update the weights between 2 layers

# Design Issues for ANN

-   Number of nodes in the input layer
    -   Assign an input node to each numerical/categorical input variable
-   Number of nodes in the output layer
    -   Binary class problem: Single node
    -   $C$-class problem: $C$ output nodes
-   Number of nodes in the hidden layer
    -   Too many parameters results in the network being too complex and easily overfitting the data
    -   If network underfits, increase the number of hidden units
    -   If network overfits, decrease the number of hidden units

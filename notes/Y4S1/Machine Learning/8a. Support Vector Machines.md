# Support Vector Machines

SVMs have shown promising empirical results in many practical applications, such as computer vision, sensor networks and text mining

Objectives of SVM is to learn a maximum margin hyperplane

# Separating Hyperplane

To learn a binary classifier, we want to find a hyperplane (linear decision boundary) so that all the points from one class appear on one side of the hyperplane, all the points from the other class reside on the other

# Maximum Margin

There are multiple hyperplanes that can separate training examples perfectly, their generalisation errors may differ

- How to choose one of the hyperplanes to construct a classifier's decision boundary, with small generalisation errors?

![](https://ars.els-cdn.com/content/image/3-s2.0-B978032385214200001X-f06-02-9780323852142.jpg)

Each decision boundary $B_i$ is associated with a pair of parallel hyperplanes $b_{i1}$ and $b_{i2}$

- $b_{i1}$ is obtained by moving the hyperplane away from the original boundary until it touches the closest circles
- $b_{i2}$ is obtained by moving a hyperplane until it touches the closest squares
- The distances fron $b_{i1}$ and $b_{i2}$ to $B_i$ are the same
- Larger margins imply better generalisation errors. The distance between hyperplanes $b_{i1}$ and $b_{i2}$ is known as the margin of the classifier

# Decision Boundary

Given a binary classification task, denote $y_i = +1$ for the circle class and $y_i = -1$ for the square

The decision boundary is

$$
w_1 x_1 + w_2 x_2 + b = 0 \\
\bold{w} \cdot \bold{x} + b = 0
$$

$\cdot$ is the inner product: $\bold{w} \cdot \bold{x} = \sum_{i=1}^{D} w_i x_i$. Recall that $\bold{a} \cdot \bold{b} = \lVert \bold{a} \rVert \lVert \bold{b} \rVert \cos \theta$, where $\theta$ is the angle between $\bold{a}$ and $\bold{b}$, and $\lVert \bold{a} \rVert$ is the euclidean norm of $\bold{a}$

After we find a decision boundary $\bold{w} \cdot \bold{x} + b = 0$, for any test example $\bold{x}^*$,

$$
\begin{cases}
    f(\bold{x}^*) = +1 & \bold{w} \cdot \bold{x}^* + b \geq 0 \\
    f(\bold{x}^*) = -1 & \bold{w} \cdot \bold{x}^* + b < 0 \\
\end{cases}
$$

# Margin-Induction

Suppose $\bold{x}_a$ and $\bold{x}_b$ are 2 points located on the decision boundary. Then

$$
\begin{aligned}
    \bold{w} \cdot \bold{x}_a + b &= 0 \\
    \bold{w} \cdot \bold{x}_b + b &= 0 \\
    \bold{w} \cdot (\bold{x}_a - \bold{x}_b) &= 0
\end{aligned}
$$

We can see that the direction of $\bold{w}$ is orthogonal to the decision boundary

For any circle $\bold{x}_c$ located above the decision boundary,

$$
\bold{w} \cdot \bold{x}_c + b \geq k, k > 0
$$

For any square $\bold{x}_s$ located below the decision boundary,

$$
\bold{w} \cdot \bold{x}_s + b \leq k', k' < 0
$$

The 2 parallel hyperplanes passing the closest circle(s) and square(s) can be written as

$$
\bold{w} \cdot \bold{x}_c + b = k, k > 0 \\
\bold{w} \cdot \bold{x}_s + b = k', k' < 0
$$

We can further rewrite this as:

$$
\bold{w} \cdot \bold{x}_c + \bar{b} = \bar{k} \\
\bold{w} \cdot \bold{x}_s + \bar{b} =  -\bar{k}
$$

By rescaling $\bold{w} = \bold{w} / \bar{k}$ and $b = \bar{b} / \bar{k}$, we can rewrite the equation

$$
\bold{w} \cdot \bold{x} + b = 1 \\
\bold{w} \cdot \bold{x} + b = -1
$$

The margin can be computed as follows:

We have the 2 parallel hyperplanes passing through the points closest to the decision boundary

$$
\begin{aligned}
    b_{11} &: \bold{w} \cdot \bold{x}_1 + b = 1 \\
    b_{12} &: \bold{w} \cdot \bold{x}_2 + b = -1
\end{aligned}
$$

Then

$$
\begin{aligned}
    \bold{w} \cdot (\bold{x}_1 - \bold{x}_2) &= 2 \\
    \lVert \bold{w} \rVert_2 \lVert \bold{x}_1 - \bold{x}_2 \rVert_2 \cos \theta &= 2 \\
    \lVert \bold{w} \rVert_2 d &= 2 \\
    d &= \frac{2}{\lVert \bold{w} \rVert_2}
\end{aligned}
$$

$d$ is the margin. Note that this holds only for rescaled $\bold{w}$

To maximise margin $d$, we minimise $\lVert \bold{w} \rVert_2^2 / 2$

## Optimisation for SVM

For linear SVMs, we want $\min_{\bold{w}, b} \frac{\lVert \bold{w} \rVert_2^2}{2}$ such that $y_i \times (\bold{w} \cdot \bold{x_i} + b) \geq 1, i = 1, ..., N$

# Multi-Class Classification

Given a 3-class classification problem $C_1, C_2, C_3$. We use the 1 vs rest approach

- Binary classification 1: positive ($C_1$) vs negative ($C_2$ and $C_3$)
- Binary classification 1: positive ($C_2$) vs negative ($C_1$ and $C_3$)
- Binary classification 1: positive ($C_3$) vs negative ($C_1$ and $C_2$)
- For a test instance $\bold{x}^*$, apply binary classifiers $f_1, f_2, f_3$ to make predictions on $\bold{x}^*$
- Combine predicted results to make a final prediction

# Resources

- https://www.youtube.com/watch?v=_PwhiWxHK8o

# Regression with SVM

# 1D Case

A special case, where an instance is represented by one input feature
- To learn a linear function $f(x)$ in terms of $w$ (drop the bias term $b$ for simplicity) from $\{x_i, y_i\}_{i=1}^{N}$

$$
f(x) = w \cdot x
$$

We want to minimise the error (difference between predicted values $f(x_i)$ and target $y_i$)

Suppose we use sum of square error

$$
\mathcal{L}(w) = \frac{1}{2} \sum_{i=1}^{N} (f(x_i) - y_i)^2 = \frac{1}{2} \sum_{i=1}^{N} (wx_i - y_i)^2
$$

We learn the linear model by minimizing error

$$
w^* = \argmin_w \mathcal{L}(w)
$$

To solve the unconstrained minimisation problem, we set the derivative to 0 and solve for $w$

$$
\begin{aligned}
    \frac{\partial \mathcal{L}(w)}{\partial w} &= 0 \\
    \sum_{i=1}^{N} (wx_i - y_i)x_i &= 0 \\
    w \sum_{i=1}^{N} x_i^2 - \sum_{i=1}^{N} y_i x_i &= 0 \\
    w &= \frac{\sum_{i=1}^{N} y_i x_i}{\sum_{i=1}^{N} x_i^2}
\end{aligned}
$$

We can see that the second order derivative $\frac{\partial^2 \mathcal{L}(w)}{\partial w^2} = \sum_{i=1}^{N} x_i^2 > 0$, hence the above gives the minimum point

# General Case

We want to learn a linear function $f(\bold{x})$ in terms of $\bold{w}$ and $b$. To simplify, we define $w_0 = b, X_0 = 1$, and $\bold{w}$ and $\bold{x}$ are of $d + 1$ dimensions

$$
f(\bold{x}) = \bold{w} \cdot \bold{x}
$$

Suppose sum-of-squares error is used:

$$
\mathcal{L}(\bold{w}) = \frac{1}{2} \sum_{i=1}^{N} (f(\bold{x}_i) - y_i)^2 = \frac{1}{2} \sum_{i=1}^{N} (\bold{w} \cdot \bold{x}_i - y_i)^2
$$

We learn the linear model in terms of $\bold{w}$ by minimising the error, and introducing a regularisation term to control the complexity of the model

$$
\bold{w}^* = \argmin_\bold{w} \mathcal{L}(\bold{w}) + \frac{\lambda}{2} \lVert \bold{w} \rVert_2^2
$$

We set the derivative of $\mathcal{L}(\bold{w})$ to 0, and solve for $\bold{w}$. We can obtain a closed-form solution:

Define $\bold{X} = (\bold{x}_1, ..., \bold{x}_N)^T$
- The $i$-th row of $\bold{X}$ is the $i$-th training example

We define $\bold{y} = (y_1, ..., y_N)^T$

We need to find $\frac{\partial \mathcal{L}(\bold{w})}{\partial \bold{w}}$. First, let us rewrite the first part of the error function

$$
\begin{aligned}
    \frac{1}{2} \sum_{i=1}^{N} (\bold{w} \cdot \bold{x}_i - y_i)^2 &= \frac{1}{2} \left \lVert \begin{pmatrix} \bold{w}^T \bold{x}_1 - y_1 \\ \vdots \\ \bold{w}^T \bold{x}_N - y_N \end{pmatrix} \right \rVert^2 \\
    &= \frac{1}{2} \left \lVert \begin{pmatrix} \bold{x}^T_1 \bold{w}  - y_1 \\ \vdots \\ \bold{x}^T_N \bold{w} - y_N \end{pmatrix} \right \rVert^2 \\
    &= \frac{1}{2} \left \lVert \bold{X} \bold{w} - \bold{y} \right \rVert^2 \\
    &= \frac{1}{2} (\bold{Xw} - \bold{y})^T (\bold{Xw} - \bold{y}) \\
    &= \frac{1}{2} (\bold{w}^T \bold{X}^T - \bold{y}^T) (\bold{Xw} - \bold{y}) \\
    &= \frac{1}{2} (\bold{w}^T \bold{X}^T \bold{Xw} - \bold{w}^T \bold{X}^T \bold{y} - \bold{y}^T \bold{Xw} + \bold{y}^T \bold{y}) \\
    &= \frac{1}{2} (\bold{w}^T \bold{X}^T \bold{Xw} - 2 \bold{w}^T \bold{X}^T \bold{y} + \bold{y}^T \bold{y})
\end{aligned}
$$

The regularisation term can be rewritten as

$$
\frac{\lambda}{2} \lVert \bold{w} \rVert_2^2 = \frac{\lambda}{2} \bold{w}^T \bold{w}
$$

Since $\bold{X}^T \bold{X}$ is symmetric, $\frac{\partial \bold{w}^T \bold{X}^T \bold{Xw}}{\partial \bold{w}} = 2 \bold{w}^T \bold{X}^T \bold{X}$

$\frac{\partial \bold{w}^T \bold{w}}{\partial \bold{w}} = 2 \bold{w}^T$

$\frac{\partial \bold{w}^T \bold{X}^T \bold{y}}{\partial \bold{w}} = y^T \bold{X}$ 

$\frac{\partial \bold{y}^T \bold{y}}{\partial \bold{w}} = 0$

Hence,

$$
\frac{\partial J}{\partial \bold{w}} = \bold{w}^T \bold{X}^T \bold{X} - \bold{y}^T \bold{X} + \lambda \bold{w}^T = \bold{X}^T \bold{X} \bold{w} - \bold{X}^T \bold{y} + \lambda \bold{w} = 0
$$

Solving for $\bold{w}$, we get

$$
\bold{w} = \left( \bold{X}^T \bold{X} + \lambda \bold{I} \right)^{-1} \bold{X}^T \bold{y}
$$

# L2 Regularisation

The regularisation penalty term encourages simpler models with small weight norms and is widely used

# Evaluation

We evaluate the performance of a linear regression model using either
- Root Mean Square Error (RMSE)

    $$
        RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (f(\bold{x}_i) - y_i)^2}
    $$
- Mean Absolute Error (MAE)

    $$
        MAE = \frac{1}{N} \sum_{i=1}^{N} |f(\bold{x}_i - y_i)|
    $$
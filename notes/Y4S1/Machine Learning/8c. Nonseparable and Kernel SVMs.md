# Nonseparable and Kernel SVMs

# Linear SVM: Separable Case

For linearly separable data, the optimisation problem of the linear SVM:

$$
\argmin_{\bold{w}, b} \frac{\lVert \bold{w} \rVert_2^2}{2} \text{ st } y_i (\bold{w} \cdot \bold{x}_i + b) \geq 1, i = 1, ..., N
$$

However, what if the problem is nonseparable?

-   We need to introduce slack variables $\xi_i \geq 0$ to absorb the errors

$$
\begin{cases}
    \bold{w} \cdot \bold{x}_i + b \geq 1 - \xi_i & y_i = 1 \\
    \bold{w} \cdot \bold{x}_i + b \geq -1 + \xi_i & y_i = -1 \\
\end{cases}
$$

Or $y_i (\bold{w} \cdot \bold{x}_i + b) \geq 1 - \xi_i$

Note that

-   If $\xi_i = 0$, there is no problem with $\bold{x}_i$
-   If $0 < \xi_i < 1$, $\bold{x}_i$ is still correctly classified, however it is within the margin
-   If $\xi_i = 1$, then $\bold{x}_i$ is on the decision boundary (equal to a random guess)
-   If $\xi_i > 1$, $\bold{x}_i$ is misclassified

# Soft Error

Soft error is the sum of the misclassifications ($\xi_i > 1$) and the number of nonseparable points ($\xi_i > 0$)

$$
E = \sum_i \xi_i
$$

# Linear SVM: Nonseparable Case

For a linear SVM with soft errors,

$$
\argmin_{\bold{w}, b, \bold{\xi}} \frac{\lVert \bold{w} \rVert_2^2}{2} + C \sum_{i=1}^{N} \xi_i \text{ s.t. } \\ y_i (\bold{w} \cdot \bold{x}_i + b) \geq 1 - \xi_i, i = 1, ..., N, \xi_i \geq 0
$$

-   $C \geq 0$ is a parameter to tradeoff the impact of margin maximisation and tolerable errors
-   Nonnegative $\xi_i$ provides an estimate of the error of the decision boundary on the training example $\bold{x}_i$

# Nonlinear SVM

What if the decision boundary is nonlinear?

-   How to generalise linear decision boundary to become nonlinear?
-   Key idea: Transform $\bold{x}_i$ to a higher dimensional space to "make life easier"
    -   Input space: The space the point $\bold{x}_i$ is located
    -   Feature space: The space of $\varphi(\bold{x}_i)$ after transformation
-   Assumption: In a higher dimensional space, it is easier to find a linear hyperplane to classify data

## Feature Mapping

The original input space can always be mapped to some higher-dimensional feature space where the training set is separable

E.g. $\varphi: \mathbb{R}^2 \to \mathbb{R}^3, (X_1, X_2) \to (X_1^2, \sqrt{2} X_1 X_2, X_2^2)$

The optimisation problem of nonlinear SVM becomes:

$$
\argmin_{\bold{w}, b} \frac{\lVert \bold{w} \rVert_2^2}{2} \text{ s.t. } \\ y_i (\bold{w} \cdot \varphi(\bold{x}_i) + b) \geq 1, i = 1, ..., N
$$

The decision boundary is now a hyperplane such that

$$
\bold{w} \cdot \varphi(\bold{x}_i) + b = 0
$$

The computation in the feature space can be costly because it is high dimensional, but the kernel trick comes to the rescue

## Nonlinear SVM: Kernel Trick

Suppose $\varphi(\cdot)$ is given as follows, mapping an instance from 2-dimensional space to 6-dimensional space

$$
\varphi([X_1, X_2]) = [1, \sqrt{2} X_i, \sqrt{2} X_2, X_1^2, X_2^2, \sqrt{2} X_1 X_2]
$$

Given 2 data instances $\bold{a} = [A_1, A_2], \bold{b} = [B_1, B_2]$, we have

$$
\varphi(\bold{a}) = [1, \sqrt{2} A_i, \sqrt{2} A_2, A_1^2, A_2^2, \sqrt{2} A_1 A_2] \\
\varphi(\bold{b}) = [1, \sqrt{2} B_i, \sqrt{2} B_2, B_1^2, B_2^2, \sqrt{2} B_1 B_2] \\
$$

The inner product of the 2 instances after feature mapping:

$$
\varphi(\bold{a}) \cdot \varphi(\bold{b}) = (1 + A_1 B_1 + A_2 B_2)^2 = (1 + \bold{a} \cdot \bold{b})^2
$$

So, if we define the kernel function as follows:

$$
k(\bold{a}, \bold{b}) = (1 + \bold{a} \cdot \bold{b})^2
$$

There is no need to carry out $\varphi(\cdot)$ explicitly. This use of the kernel function to avoid carrying out $\varphi(\cdot)$ explicitly is known as the kernel trick.

In general if $\varphi(\cdot)$ satisfies some conditions, we can find a function $k(\cdot, \cdot)$ such that

$$
k(\bold{x}_i, \bold{x}_j) = \varphi(\bold{x}_i) \cdot \varphi(\bold{x}_j)
$$

## Lagrange Multiplier Method

Given: An objective $f(\bold{w})$ to be minimised, with a set of inequality constraints to be satisfied $h_i(\bold{w}) \leq 0, i = 1, 2, ..., q$. We want to find

$$
\argmin_\bold{w} f(\bold{w}) \text{ s.t. } \\
h_i(\bold{w}) \leq 0, i = 1, ..., q
$$

The Langrangian for the optimisation problem

$$
L(\bold{w}, \bold{\lambda}) = f(\bold{w}) + \sum_{i=1}^q \lambda_i h_i(\bold{w})
$$

$\bold{\lambda} = (\lambda_1, ..., \lambda_q)$ is the langrange multiplier

## The Dual Form (Separable)

By using the lagrangian multiplier method, we convert the objective of a non-linear SVM:

$$
\argmin_{\bold{w}, b} \frac{\lVert \bold{w} \rVert_2^2}{2} \text{ s.t. } \\ y_i (\bold{w} \cdot \varphi(\bold{x}_i) + b) \geq 1, i = 1, ..., N
$$

Into

$$
\argmax_{\bold{\lambda}} L_D(\bold{\lambda}) = - \left( \frac{1}{2} \sum_{i, j} \lambda_i \lambda_j y_i y_j (\varphi(\bold{x}_i) \cdot \varphi(\bold{x}_j)) - \sum_{i=1}^{N} \lambda_i \right)
$$

-   The dual Lagrangian involves only the Lagrange multipliers and the training data
-   The negative sign in the dual Lagrangian transforms a minimisation problem of the primal form to a maximisation problem of the dual form
-   Objective is to maximise $L_D(\bold{\lambda})$
    -   Can be solved using numerical techniques such as quadratic programming

Once the $\lambda_i$'s are found, we can obtain the feasible solutions for $\bold{w}$ and $b$ from:

$$
\bold{w} = \sum_{i=1}^{N} \lambda_i y_i \varphi(\bold{x}_i) \\
\lambda_i (y_i (\bold{w} \cdot \varphi(\bold{x}_i) + b) - 1) = 0
$$

The decision boundary can be expressed as:

$$
\bold{w} \cdot \varphi(\bold{x}) + b = \left( \sum_{i=1}^N \lambda_i y_i \varphi(\bold{x}_i) \cdot \varphi(\bold{x}) \right) + b = 0
$$

-   If $\bold{x}_i$ is a support vector, then the corresponding $\lambda_i > 0$, otherwise $\lambda_i = 0$

For a test instance $\bold{x}^*$, it can be classified using

$$
f(\bold{x}^*) = sign \left( \sum_{i=1}^{N} \lambda_i y_i \varphi(\bold{x}_i) \cdot \varphi(\bold{x}^*) + b\right)
$$

Training:

$$
\argmax_\bold{\lambda} \left( \sum_{i=1}^N \lambda_i - \frac{1}{2} \sum_{i, j} \lambda_i \lambda_j y_i y_j (\varphi(\bold{x}_i) \cdot \varphi(\bold{x}_j)) \right)
$$

Our decision boundary is:

$$
\sum_{i=1}^{N} \lambda_i y_i (\varphi(\bold{x}_i) \cdot \varphi(\bold{x}^*)) + b = 0
$$

-   The datapoints only appear as inner products
-   As long as the inner product in the feature space can be calcualted, no need for explicit mapping

We now replace the inner product in feature space by the kernel function

Training:

$$
\argmax_\bold{\lambda} \left( \sum_{i=1}^N \lambda_i - \frac{1}{2} \sum_{i, j} \lambda_i \lambda_j y_i y_j k(\bold{x}_i, \bold{x}_j) \right)
$$

Our decision boundary is:

$$
\sum_{i=1}^{N} \lambda_i y_i k(\bold{x}_i, \bold{x}^*) + b = 0
$$

## Kernel Functions: Examples

Linear kernel:

$$
k(\bold{x}_i, \bold{x}_j) = \bold{x}_i \cdot \bold{x}_j
$$

Radial basis function kernel with width $\sigma$:

$$
k(\bold{x}_i, \bold{x}_j) = \exp \left( -\frac{\lVert \bold{x}_i - \bold{x}_j \rVert_2^2}{2\sigma^2} \right)
$$

Polynomial kernel with degree $d$:

$$
k(\bold{x}_i, \bold{x}_j) = (\bold{x}_i, \bold{x}_j + 1)^d
$$

# Soft Margin Dual Form

By using the Lagrangian Multiplier method, we transform the optimsation problem:

$$
\argmin_{\bold{w}, b, \bold{\xi}} \frac{\lVert \bold{w} \rVert_2^2}{2} + C \sum_{i=1}^{N} \xi_i \text{ s.t. } \\ y_i (\bold{w} \cdot \bold{x}_i + b) \geq 1 - \xi_i, i = 1, ..., N, \xi_i \geq 0
$$

Into:

$$
\argmax_\bold{\lambda} L_D(\bold{\lambda}) = - \left( \frac{1}{2} \sum_{i, j} \lambda_i \lambda_j y_i y_j (\varphi(\bold{x}_i) \cdot \varphi(\bold{x}_j) ) - \sum_{i=1}^N \lambda_i \right)
$$

such that $0 \leq \lambda_i \leq C$

The decision boundary is

$$
\bold{w} \cdot \varphi(\bold{x}) + b = \left( \sum_{i=1}^N \lambda_i y_i \varphi(\bold{x}_i) \cdot \varphi(\bold{x}) \right) + b = 0
$$

And for a test instance $\bold{x}^*$, it can be classified as:

$$
f(\bold{x}^*) = sign \left( \sum_{i=1}^{N} \lambda_i y_i \varphi(\bold{x}_i) \cdot \varphi(\bold{x}^*) + b\right)
$$

# Kernel Regression

# Regularised Linear Regression with Kernels

Primal objective:

$$
\tau(\bold{w}) = \frac{1}{2} \sum_{i=1}^{N} (\bold{w} \cdot \phi(\bold{x}_i) - y_i)^2 + \frac{\lambda}{2} \lVert \bold{w} \rVert ^2_2
$$

It can be reformulated in terms of a dual form, where the kernel function arises naturally

$$
\begin{aligned}
    \frac{\partial \tau(\bold{w})}{\partial \bold{w}} &= 0 \\
    \sum_{i=1}^{N} (\bold{w} \cdot \phi(\bold{x}_i) - y_i) \phi(\bold{x}_i) + \lambda \bold{w} &= 0
\end{aligned}
$$

# Closed Form Solution

By using the kernel trick $k(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$

$$
\begin{aligned}
    \sum_{i=1}^N (\bold{w} \cdot \phi(\bold{x}_i) - y_i) \phi(\bold{x}_i) + \lambda \bold{w} &= 0 \\
    \bold{w} &= -\frac{1}{\lambda} \sum_{i=1}^N (\bold{w} \cdot \phi(\bold{x}_i) - y_i) \phi(\bold{x}_i) \\
    &= \sum_{i=1}^N a_i \phi(\bold{x}_i)
\end{aligned}
$$

where $a_i = -\frac{1}{\lambda} (\bold{w} \cdot \phi(\bold{x}_i) - y_i)$

Let $\bold{\Phi} = (\phi(\bold{x}_1), ..., \phi(\bold{x}_N))^T$ and $\bold{a} = (a_1, ..., a_N)^T$. Then we can rewrite

$$
\bold{w} = \sum_{i=1}^N a_i \phi(\bold{x}_i) = \bold{\Phi}^T \bold{a} \\
a_i = -\frac{1}{\lambda} (\bold{w} \cdot \phi(\bold{x}_i) - y_i) \implies \bold{a} = \frac{1}{\lambda} (\bold{y} - \bold{\Phi w})
$$

Substituing $\bold{w}$ into the equation for $\bold{a}$, we get

$$
\begin{aligned}
    \bold{a} &= \frac{1}{\lambda} (\bold{y} - \bold{\Phi \Phi}^T \bold{a}) \\
    \bold{a} &= (\lambda \bold{I} + \bold{\Phi \Phi}^T)^{-1} \bold{y} \\
    &= (\lambda \bold{I} + \bold{K})^{-1} \bold{y}
\end{aligned}
$$

Note that

$$
\begin{aligned}
    \bold{K} &= \begin{pmatrix}
        k(\bold{x}_1, \bold{x}_1) & \cdots & k(\bold{x}_1, \bold{x}_N) \\
        \vdots & \ddots & \vdots \\
        k(\bold{x}_N, \bold{x}_1) & \cdots & k(\bold{x}_N, \bold{x}_N) \\
    \end{pmatrix} \\
    &= \begin{pmatrix}
        \phi(\bold{x}_1) \cdot \phi(\bold{x}_1) & \cdots & \phi(\bold{x}_1) \cdot \phi(\bold{x}_N) \\
        \vdots & \ddots & \vdots \\
        \phi(\bold{x}_N) \cdot \phi(\bold{x}_1) & \cdots & \phi(\bold{x}_N) \cdot \phi(\bold{x}_N) \\
    \end{pmatrix} \\
    &= \bold{\Phi \Phi}^T
\end{aligned}
$$

And now we solve for $\bold{w}$

$$
\bold{w} = \bold{\Phi}^T (\lambda \bold{I} + \bold{K})^{-1} \bold{y}
$$

Now we can rewrite $f(\bold{x})$

$$
\begin{aligned}
    f(\bold{x}) &= \bold{w} \cdot \bold{\phi}(\bold{x}) \\
    &= \phi(\bold{x})^T \bold{w} \\
    &= \phi(\bold{x})^T \bold{\Phi}^T (\bold{K} + \lambda \bold{I})^{-1} \bold{y} \\
    &= \bold{k}(\bold{x})^T (\bold{K} + \lambda \bold{I})^{-1} \bold{y}
\end{aligned}
$$

where $\bold{k}(\bold{x}) \in \mathbb{R}^N$, where $\bold{k}_i(\bold{x}) = k(x_i, \bold{x})$

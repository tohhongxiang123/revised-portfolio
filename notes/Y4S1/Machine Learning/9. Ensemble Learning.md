# Ensemble Learning

Objective: To improve model performance in terms of accuracy by aggregating the predictions of multiple models
- Construct a set of base models from the training data
- Make predictions by combining predicted results made by each base model

Suppose we have 3 base binary classifiers
- Each classifier has an error rate of 0.35, or accuracy of 0.65
- Given a test instance, if we choose any of the classifiers to make a prediction, the probability that the classifier makes a wrong prediction is 35%
- To ensemble, we combine the 3 base classifiers using the majority vote
- The ensemble makes a wrong prediction only if more than 1 classifier predicts incorrectly
- Assuming that the 3 base classifiers are independent, the overall error rate of the ensemble is:

$$
\sum_{i=2}^{3} {3 \choose i} \epsilon^i (1 - \epsilon)^{3 - i} = 0.2817
$$

- The error rate of the ensemble is lower than the error rates of the individual models
- However, we cannot always guarantee that the base classifiers are not completely independent
  - An extreme case is when they are perfectly correlated (Always has the same predictions)

Also note: The ensemble classifier performs worse than the base classifiers if the base classifier error rate is larger than 0.5
- When error rate > 0.5, base models are more likely to predict wrongly. Ensemble would more likely choose the wrong prediction as well

# Necessary Conditions

There are 2 necessary conditions for an ensemble classifier to perform better than a single classifier
1. Base classifiers are not perfectly correlated with each other
   - In practice, base classifiers are usually somewhat correlated
2. The base classifiers should do better than a classifier that performs random guessing (e.g. for binary classification, accuracy should be better than 0.5)

# Ensemble Methods

How to generate a set of base classifiers?
- By manipulating the training set: Multiple training sets are created by resampling the original data according to some sampling distribution. A classifier is then trained on each training set. E.g. Bagging, Boosting
- By manipulating input features: A subset of input features is chosen to form each training set. A classifier is then built from each training set, such as Random Forest
- By manipulating the learning algorithms: Applying the algorithm several times on the same training data with different parameters, or applying different algorithms

How to combine the base classifiers for predictions?
1. Let $D$ denote the original training data, $k$ denote the number of base classifiers, and $T$ be the test dataset
2. For i = 1 to k
   1. Train a base classifier $f_i$ on $D$
3. For each test instance
   1. Generate $f_1(\bold{x}), f_k(\bold{x})$
   2. Calculate $f_M(\bold{x}) = Merge(f_1(\bold{x}), ..., f_k(\bold{x}))$
   3. We can merge in multiple ways, e.g. majority voting

# Bagging

Known as bootstrap aggregating (bootstrapping), to repeatedly sample with replacement according to a uniform probability distribution
- Build classifier on each bootstrap sample, which is the same size as the original data
- Use majority voting to determine the class label of the ensemble classifier

Suppose a training set $D$ contains $N$ examples
- A training instance has a probability of $1 - \frac{1}{N}$ of not being selected
- The probability of a training instance not ending up in a training set is $(1 - \frac{1}{N})^N \approx \frac{1}{e}$
- A bootstrap sample $D_i$ contains approximately $\frac{1}{e} = 63.2$% of the original training data

# Boosting

Boost a set of weak learners into a strong learner, and make instances currently misclassified more important

In general, we adaptively change the distribution of training data, so that the base classifiers will focus more on previously misclassified records

Specifically
- Initially all $N$ instances are assigned equal weights
- Unlike bagging, weights may change at the end of each boosting round
- In each boosting round, after the weights are assigned to the training instances, we draw a bootstrap sample from the original data, by using the weights as a sampling distribution to build the model

Procedure
1. Initially, all instances are assigned equal weights $\frac{1}{N}$, so that they are all equally likely to be chosen for training. A sample is uniformly drawn to obtain a new training set
2. Classifier is induced from the training set, and used to classify all the examples in the original training data
3. The weights of each training instance is updated at the end of each boosting round
    - Instances that are wrongly classified have their weights increased
    - Instances that are correctly classified have their weights decreased
4. Repeat steps 2 and 3 until the stopping condition is met
5. Finally, ensemble is obtained by aggregating the base classifiers obtained from each boosting round

To aggregate the predictions in the ensemble, we use weighted voting
- Each classifier contributes a different amount to the final prediction

# Random Foresets

A class of ensemble methods specifically designed for decision tree classifiers
- Random forests grow many trees
- Each tree is generated based on a random subset of features
- Final result on classifying a new instance: Voting
  - Forest chooses the classification result having the most votes

![](https://av-eks-blogoptimized.s3.amazonaws.com/33019random-forest-algorithm287548.png)

Algorithm
- Choose $T$: Number of trees to grow
- Choose $m' < m$, where $m$ is the number of total features. Typically $m' = 0.2m$
- For each tree:
  - Choose a training set via bootstrapping
  - For each node, randomly choose $m'$ features and calculate the best split
  - Fully grown and not pruned
- Use majority vote among all trees

Random forests
- Bagging + random features
- Improves accuracy
  - Incorporate more diversity
- Improve efficiency
  - Searching among subsets of features is much faster than searching among the complete set

# Voting

- Majority voting
  - Takes the class label that receives the largest number of votes as the final prediction
- Weighted voting
  - A generalised version of majority voting by introducing weights for each classifier

# Combining By Learning

Stacking
- A general procedure where a learner is trained to combine individual learners
- Individual learners: First-level learners
- Combiner: Second-level learner, or meta-learner

We have $k$ base classifiers as our first-level learners. On some training instance $\bold{x}_i$, we use all our base classifiers to predict

$$
\bold{x}_i' = (f_1(\bold{x}_i), ..., f_k(\bold{x}_i))
$$

Now, we learn a meta classifier $f_M$ from $\{\bold{x}_i', y_i \}, i = 1, ..., N$

For a test instance $\bold{x}^*$,
- We calculate $f_M((f_1(\bold{x}^*), ..., f_k(\bold{x}^*)))$
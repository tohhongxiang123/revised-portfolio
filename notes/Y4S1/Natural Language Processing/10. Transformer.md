# Transformer

In RNNs, we had the bottleneck problem

-   Encoding of the source sentence needs to capture all information about the source sentence

Solution: Attention

-   On each step of the decoder, use a direct connection to the encoder to focus on the relevant part of the source sequence
-   Then, use the attention distribution to take a weighted sum of the encoder hidden states
-   Attention output mostly contains information from the hidden states that receive high attention
-   Then, concatenate attention output with decoder hidden state to compute $\hat{y}_1$ as before

Formally, attention:

-   We have encoder hidden states $\bold{h}_1, ..., \bold{h}_N \in \mathbb{R}^h$
-   On timestep $t$, we have decoder hidden state $\bold{s}_t \in \mathbb{R}^h$

    $$
        \bold{s}_1 = f(W_s \cdot \bold{h}_{N} + b_s) \\
        \bold{s}_{t+1} = RNN(\bold{s}_t, y_t)
    $$

    -   The first hidden state of the decoder is calculated from the last hidden state of the encoder
    -   The $t$-th hidden state of our encoder is based off the previous timestep's prediction $y_t$ and the previous hidden state $h_t$

-   We get the attention scores $e^t$ for this timestep

    $$
        \bold{e}^t = [\bold{s}_t^T \bold{h}_1, \cdots, \bold{s}_t^T \bold{h}_N] \in \mathbb{R}^N
    $$

-   We take a softmax to get the attention distribution $\alpha^t$ for this step

    $$
        \bold{\alpha}^t = softmax(\bold{e}^t) \in \mathbb{R}^N
    $$

-   We use $\bold{\alpha}^t$ to take a weighted sum of the encoder hidden states, to get the attention output $\bold{a}^t$

    $$
        \bold{a}_t = \sum_{i=1}^N \bold{\alpha}_i^t \bold{h}_i \in \mathbb{R}^h
    $$

-   We concatenate attention output with decoder hidden state to copute $\hat{y}_t$
    $$
        \hat{y}_t = softmax(W_y \cdot [a_t;s_t] + b_y)
    $$

Different types of attention:

-   Basic dot-product attention: $\bold{e}_i = \bold{s}^T \bold{h}_i \in \mathbb{R}$
    -   Assume that $\bold{s}$ and $\bold{h}$ have the same dimensions
-   Multiplicative attention: $\bold{e}_i = \bold{s}^T \bold{W} \bold{h}_i \in \mathbb{R}$
    -   Where $\bold{W} \in \mathbb{R}^{d_2 \times d_1}$ is a weight matrix
-   Additive attention: $\bold{e}_i = \bold{v}^T \tanh (\bold{W}_1 h_i + \bold{W}_2 \bold{s}) \in \mathbb{R}$
    -   Where $\bold{W}_1 \in \mathbb{R}^{d_3 \times d_1}, \bold{W}_2 \in \mathbb{R}^{d_3 \times d_2}$ are weight matrices, and $\bold{v} \in \mathbb{R}^{d_3}$ is a weight vector
    -   $d_3$ (Attention dimensionality) is a hyperparameter

Advantages of attention

-   Attention improves neural machine translation performance significantly (Necessary to allow decoder to focus on relevant parts of the source)
-   Attention solve the bottleneck problem (Allows decoder to look directly at the source and bypass the bottleneck)
-   Attention helps with vanishing gradient problem (Provides shortcut to faraway states)
-   Attention provides interpretability (We can inspect attention distribution to see what the decoder was focusing on)
-   Attention is a general deep learning technique
    -   We have encoder hidden states $h_1, ..., h_N \in \mathbb{R}^h$ (Keys/Values)
    -   We have decoder hidden state $s_t \in \mathbb{R}^h$ at timestep $t$ (Query)
    -   Given a set of value vectors and a query vector, attention is a technique to compute a weighted sum of the values dependent on the query
        -   The weighted sum is a selective summary of the information contained in the values, where the query determines which values to focus on
        -   Attention is a way to obtain a fixed-size representation of an arbitrary set of representations (the values) dependent on some other reprsentation (query)

We can think of attention as performing fuzzy lookup in a key-value store

-   In a lookup table, we have a table of keys that maps to values. The query matches one of the keys and returns the values
-   In attention, the query matches all keys softly to a weight between 0 and 1. The keys' values are multiplied by the weights and summed

# Self-Attention

Attentions can be generally applied between encoder and decoder for seq2seq learning

-   How about attentions within a single sequence? We use self-attention

Why self-attention?

-   RNNs are unrolled left-to-right (or right-to-left)
-   Only encodes linear locaity (nearby words affect more)
-   Takes several steps for distance word pairs to interact
-   Forward and backward passes have O(sequence length) unparallelizable operations
-   Future RNN hidden states cannot be computed in full before past RNN hidden states have been computed
-   Inhibits training on very large datasets
-   With self attention, we treat each word's representation as a query to access and incorporate information from a set of values
-   Easy to parallelize (per layer)
-   Max interaction distance is O(1), since all words interact at every layer

## Self-Attention Mechanism

Let $\bold{w}_{1:n}$ be a sequence of words in vocabulary $V$, like "Zuko made his uncle tea"

For each $\bold{w}_i$, let $\bold{x}_i = E \bold{w}_i$, where $E \in \mathbb{R}^{d \times |V|}$ is an embedding matrix

1. Transform each word embedding with weight matrices $Q, K, V \in \mathbb{R}^{d \times d}$
    - $\bold{q}_i = Q \bold{x}_i$ (Query)
    - $\bold{k}_i = K \bold{x}_i$ (Key)
    - $\bold{v}_i = V \bold{x}_i$ (Values)
2. Compute pairwise similarities between keys and queries, and normalize with softmax
    $$
    \begin{aligned}
        \bold{e}_{ij} &= \bold{q}_i^T \bold{k}_j \\
        \bold{\alpha}_{ij} &= \frac{\exp(\bold{e}_{ij})}{\sum_{j'} \exp(e_{ij'})}
    \end{aligned}
    $$
3. Compute the output for each word as a weighted sum of values

    $$
        \bold{o}_i = \sum_j \alpha_{ij} v_j
    $$

## Barriers and Solutions for Self-Attention

| Barriers                                                        | Solutions           |
| --------------------------------------------------------------- | ------------------- |
| Does not have an inherent notion of order                       | Positional Encoding |
| No nonlinearities for deep learning, all just weighted averages | Add nonlinearity    |

### Positional Encoding

Since self-attention does not build in order information, we encode the order of the sentence into our keys, queries and values

-   Consider representing each sequence index as a vector $\bold{p}_i \in \mathbb{R}^d, i \in \{1, ..., n\}$
-   Easy to incorporate this information into our self-attention block: Add $\bold{p}_i$ to our inputs
-   Recall that $\bold{x}_i$ is the embedding of the word at index $i$. Now, our positioned embedding is: $\bold{x}_i = \bold{x}_i + \bold{p}_i$
-   Learned absolute position representations: Let all $p_i$ be learnable parameters. Learn a matrix $\bold{p} \in \mathbb{R}^{d \times n}$, and let each $\bold{p}_i$ be a column of that matrix

Pros:

-   Flexibility: Each position gets to be learned to fit the data
-   Cons: Cannot extrapolate to indices outside of $1, ..., n$

### Adding Nonlinearity

Note that there are no elementwise nonlinearity in self-attention, stacking more self-attention layers just re-averages out the value vectors

Fix: Add a feed-forward network to post-process each output vector

$$
m_i = MLP(output_i) = W_2 \times ReLU(W_1 \ output_i + b_1) + b_2
$$

# From Self-Attention to Transformers

Transformers are basic modules commonly adopted in various language models

-   A lot like our minimal self attention architecture but with a few more components
-   Embeddings and positional embeddings are identical
-   We just need to replace our self attention with multi-head self attention
-   Each head attends to different parts of our sentence

How query-key-value attention is computed in matrix form:

-   Let $X = [x_1, ..., x_n] \in \mathbb{R}^{n \times d}$ be the concatenation of input vectors
-   Compute query, key, value matrices $XQ, XK, XV \in \mathbb{R}^{n \times d}$
-   Compute output using self-attention:
    -   Generate attention score matrix
        $$
            A = softmax(XQ \cdot (XK)^T) \in \mathbb{R}^{n \times n}
        $$
    -   Aggregate with value matrix:
        $$
            output = A \cdot (XV) \in \mathbb{R}^{n \times d}
        $$

What if we want to look in multiple places in the sentence at once?

-   For word $i$, self-attention looks where $x_i^T Q^T K x_j$ is high, but maybe we want to focus on different $j$ for different reasons

We define multiople attention heads, through multiple $Q, K, V$ matrices

Let $Q_l, K_l, V_l \in \mathbb{R}^{d \times d/h}$, where $h$ is the number of attention heads, and $l$ ranges from 1 to $h$

Each attention head performs attention independently

$$
o_l = softmax(X Q_l K_l^T X^T) \cdot XV_l \in \mathbb{R}^{d/h}
$$

Then the outputs of all the heads are combined

$$
o = [o_1, ..., o_h] Y, Y \in \mathbb{R}^{d \times d}
$$

## Scaled Dot Product

Scaled Dot Product attention aids in training

-   When dimensionality $d$ becomes large, dot products between vectors tend to become large
-   Because of this, inputs to softmax function can be large, making gradients small
-   Instead of using the self-attention function we have seen:
    $$
    o_l = softmax(X Q_l K_l^T X^T) \cdot XV_l
    $$
-   We divide the attention scores by $\sqrt{d/h}$, to stop the scores from becoming large, just as a function of $d/h$ (The dimensionality divided by the number of heads)

    $$
        o_l = softmax\left(\frac{X Q_l K_l^T X^T}{\sqrt{d/h}}\right) \cdot XV_l
    $$

Now that we have replaced self-attention with multi-head self attention, we will go through 2 more optimisation tricks

1. Residual connections
2. Layer normalisation

In most transformer diagrames, these are often written together as "Add and Norm"

![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)

## Residual Connections

Residual connections are a trick to help models train better

-   Instead of $X^{(i)} = Layer(X^{(i - 1)})$ where $i$ represents the layer
-   We let $X^{(i)} = X^{(i - 1)} + Layer(X^{(i - 1)})$ (So we only have to learn the residual from the previous layer)
-   Gradient is great through the residual connection; It is 1
-   Bias towards the identity function

## Layer Normalisation

A trick to help models train faster

Idea: Cut down on uninformative variation in the hidden vector values by normalising them to unit mean and standard deviation within each layer

-   LayerNorm's sucecss may be due to its normalising gradients

Let $x \in \mathbb{R}^d$ be an individual word vector in the model

-   Let $\mu = \sum_{j=1}^{d} x_j \in \mathbb{R}$. This is the mean
-   Let $\sigma = \sqrt{\frac{1}{d} \sum_{j=1}^{d} (x_j - \mu)^2} \in \mathbb{R}$ be the standard deviation
-   Let $\gamma, \beta \in \mathbb{R}^d$ be learned gain and bias parameters (can be omitted)

Layer normalisation computes

$$
o = \frac{x - \mu}{\sqrt{\sigma} + \epsilon} \gamma + \beta
$$

# Encoder-Decoder Attention

We saw that self-attention is when keys, queries and values comes from the same source

-   Let $h_1, ..., h_n$ be output vectors from the transformer encoder; $x_i \in \mathbb{R}^d$
-   Let $z_1, ..., z_n$ be input vectors from the decoder, $z_i \in \mathbb{R}^d$
-   Keys and values are drawn from the encoder (like a memory)
    -   $k_i = Kh_i, v_i = Vh_i$
-   The queries are drawn from the decoder $q_i = Q z_i$

# Resources

-   https://jalammar.github.io/illustrated-transformer/

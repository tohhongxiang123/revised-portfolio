# Pretraining

Recall Word2Vec

- Requires a word dictionary for lookup
- Same word vector with different contexts (Embedding matrix + Context matrix)

Given some input $\bold{x}$ (one-hot vector of a word)

$$
\begin{aligned}
    \bold{h} &= f(\bold{Wx} + \bold{b}) \\
    s &= \bold{u}^T \bold{h} \\
    J_t (\theta) &= \sigma (s) = \frac{1}{1 + e^{-s}}
\end{aligned}
$$

Problem with Word Dictionary

- Assume a fixed vocab of tens of thousands of words, built from the training set
- All novel words seen at test time mapped to a single `UNK`

# Byte-Pair Encoding

SUbword modelling in NLP encompasses a wide range of methods for reasoning about structure below the word level (Parts of words, characters, bytes)

- Learn a vocabulary of parts of words (subword tokens)
- At training/testing time, each word is split into a sequence of known subwords

Byte-pair encoding is a simple, effective strategy for defining a subword vocabulary

1. Start wtih a vocabulary containing only characters and a end-of-word symbol
2. Using a corpus of text, find the most common adjacent characters (e.g. "ab" as a subword of "absolute")
3. Replace instances of the character pair with the new subword; repeat until the desired vocab size

Originally used in NLP for machine translation, now a similar method (WordPiece) is used in pretraining models

Byte-pair encoding is an unsupervised word segmentation algorithm

- Start with a unigram vocabulary of all (Unicode) characters in a dataset
- Most frequent n-gram pairs -> A new n-gram

Common words end up being part of the subword vocabulary, while rarer words are split into (sometimes intuitive, sometimes not) components

- In the worst case, words are split into single characters (the rarest words)
- E.g. common words such as "hat" and "learn" stay as full words as they are very common
- Variations of common words such as "taaaaasty" split into "taa", "aaa", "sty"
- Misspellings split "laern" into "la" and "ern"
- Novel items such as "Transformerify" split into "Transformer" and "ify"

Problem with Identical Word Vectors

- The same word can have different meaning because of its context: "I record the record"

# BERT

Bidirectional Encoder Representations from Transformers

- BERT uses Word Piece tokenisation
- BERT gives contextualised word representations via multi-layer self-attention (transformer)

# Language Model Pretraining

Initial pipeline

- Start with pretrained word embeddings (no context)
- Learn how to incorporate context in an LSTM or transformer while training on the task

Issues with initial pipeline

- Training data we have for our downstream task must be suffifient to teach all contextual aspects of the language
- Most of the parameters in our network are randomly intialised

Updated pipeline: In modern NLP,

- All (or almost all) parameters in NLP networks are intialised via pretraining
- Pretraining methods hide parts of the input from the model, and train the model to reconstruct those parts

This has been exceptionally effective at building strong

- Representations of the language
- Parameter initialisations for strong NLP models
- Probaility distributions over language that we can sample from

## Pretraining Objectives

1. Masked Language Model (MLM)
2. (Causal) Language Modelling (LM)

### Masked Language Model (MLM)

Idea: Replace some fraction of words in the input with a special `MASK` token, and predict these words

```
I [M] to the [M]
```

$$
h_1, ..., h_T = Encoder(w_1, ..., w_T) \\
y_i \sim A w_i + b
$$

We only add loss terms from words that are masked out. If $\tilde{x}$ is the masked version of the sentence $x$, we are learning $p_\theta(x | \tilde{x})$. This is called Masked LM

### (Causal) Language Modelling

Formally, given a sequence of words $x^{(1)}, ..., x^{(t)}$, we compute the probability distribution of the next word $x^{(t + 1)}$

$$
P(x^{(t + 1)} | x^{(t)}, ..., x^{(1)})
$$

where $x^{(t + 1)}$ can be any word in the vocabulary $V$

- The student opened their ...

Pretraining via Language Modelling

- Train a neural network to perform language modelling on a large amount of text
- Predict the next word given past context

# Training Paradigms

3 types of training paradigms

1. Encoders
   - Gets bi-directional context: Can condition on future
   - How to train them to build strong representations?
2. Encoder-Decoders
   - Good parts of decoders and encoders
   - Whats the best way to pretrain them?
3. Decoders
   - Language models
   - Nice to generate from, but cannot condition on future words

## Encoders

So far, we looked at language model pretraining. However, encoders get bidirectional context, so we cannot do language modelling

Idea: Replace some fraction of words in the input with a special `MASK` token, and predict these masked words

```
I [M] to the [M]
```

$$
h_1, ..., h_T = Encoder(w_1, ..., w_T) \\
y_i \sim A w_i + b
$$

We only add loss terms from words that are masked out. If $\tilde{x}$ is the masked version of the sentence $x$, we are learning $p_\theta(x | \tilde{x})$. This is called Masked LM

### BERT

- In 2018, proposed "Masked LM" objective
- Released the weights of a pretrained transformer, a model labelled BERT

Some more details about Masked LM for BERT:

- Predict a random 15% of (sub)word tokens
  - Replace input word with `MASK` 80% of the time
  - Replace input word with a random token 10% of the time
  - Leave input word unchanged 10% of the time (but still predict it)
- Why? Does not let the model get complacent and not build strong representations of non-masked words

Pretraining input to BERT was 2 separate contiguous chunks of text

- Input is split into 2 chunks
- Each chunk includes the token embeddings, segment embeddings and position embeddings

BERT was trained to predict whether one chunk follows the other, or is randomly sampled

- Later work argued that the next sentence prediction was not necessary

Details of BERT

- 2 models released
  - BERT-base: 12 layers, 768-dim hidden states, 12 attention heads, 110 million parameters
  - BERT-large: 24 layers, 1024-dim hidden states, 16 attention heads, 340 million prameters
- Trained
  - BooksCorpus (800 million words)
  - English wikipedia (2500 million words)
- Pretraining is expensive and impractical on a single GPU
  - BERT was pretrained on 64 TPU chips for a total of 4 days
- Finetuning is practical and common on a single GPU

Limitations of Pretrained Encoders

- If task involves generating sequences, consider using a pretrained decoder; BERT and other pretrained encoders do not naturally lead to nice autoregressive (1 word at a time) generation methods

## Decoders

When using language model pretrained decoders, we can ignore that they were trained to model $p(w_t | w_{1: t-1})$

We can finetune them by training a classifier on the last word's hidden state

$$
h_1, ..., h_T = Decoder(w_1, ..., w_T) \\
y \sim A h_T + b
$$

Generative Pretrained Transformer (GPT)

- Transformer decoder with 12 layers and 117M parameters
- 768-dimensional hidden states, 3072-dimensional feed-forward hidden layers
- Byte-pair encoding with 40000 merges
- Trained on BooksCorpus
  - Contains long spans of contiguous text, for learning long-distance dependencies

## Encoder-Decoders

For encoder-decoders, we could do something like language modelling, where a prefix of every input is provided to the encoder and is not predicted

$$
h_1, ..., h_T = Encoder(w_1, ..., w_T) \\
h_{T+1}, ..., h_{2T} = Decoder(w_1, ..., w_T) \\
y_i \sim A h_i + b, i > T
$$

Encoder portion benefits from bidirectional context; decoder portion is used to train the whole model through language modelling

What Raffel et al. 2018 found to work best was span corruption

- Replace different-length spans from the input with unique placeholders; decode out the spans that were removed
- Thank you for **inviting me** to your party **last** week
- Input: Thank you `X` me to your party `Y` week

# Pretraining to Finetuning

Pretraining can improve NLP applications by serving as parameter initialisation

1. Pretrain on language modelling: Lots of text, learn general things
2. Finetune on task: Not many labels, adapt to the task

## The Pretraining/Finetuning Paradigm

Consider, provides parameter $\hat{\theta}$ by approximating $\min_\theta \mathcal{L}_{pretrained}(\theta)$ (Pretraining loss)

Then, finetuning approximates $\min_\theta \mathcal{L}_{finetune}(\theta)$, starting from $\hat{\theta}$ (Finetuning loss)

The pretraining may matter, because stochastic gradient descent sticks relatively close to $\hat{\theta}$ during finetuning

- So maybe the finetuning local minima near $\hat{\theta}$ tends to generalise well
- And/or, the gradients of finetuning loss near $\hat{\theta}$ propagates nicely

## Parameter-Efficient Finetuning

Finetuning every parameter in a pretrained model works well, but is memory intensive

- But lightweight finetuning methods adapt pretrained models in a constrained way
- Leads to less overfitting and/or more efficient finetuning and inference

Full finetuning: Adapt all parameters

Lightweight finetuning: Train only a few existing or new parameters

## Prefix-Tuning, Prompt-Tuning

- Prefix tuning adds a prefix of parameters, and freezes all pretrained parameters
- Prefix is processed by model just like real words would be
- Advantage: Each element of a batch at inference could run a different tuned model

# Prompting

Natural Language Generation:

- Focuses on systems that produce fluent, coherent and useful language outputs for human consumption
- Natural Language Generation is one side of NLP

NLG Model

- Directly models the conditional probability $P(y | x)$ of translating a source sequence to a target sequence
- E.g. `les pauvres sont de'munis` -> `the poor dont have any money`

$$
p(y | x) = \sum_{j=1}^{m} \log p(y_j | y_{< j}, s) \\
p(y_j | y_{< j}, s) = softmax(g(h_j))
$$

- Predict next word based on all of the previous words

Training objective

$$
J_t = \sum_{(x, y) \in \mathcal{D}} -\log p(y | x)
$$

- Maximise negative log likelihood across all pairs in training data

NLG uses an Encoder-Decoder model

- Generate target sentence by taking the argmax on each step of the decoder
- Greedy decoding (Take the most probable word at each step)

Decoding:

- At each time step $t$, our model computes a vector of scores for each token in our vocabulary
  $$
      S = f(\{y_{<t}\}) \in \mathbb{R}^V
  $$
  - Given words before $t$, score every word if it occurred at $t$
- We then compute a probability distribution $P$ over these scores with a softmax function:

  $$
      P(y_t = w | \{ y_{< t}\}) = \frac{\exp(S_w)}{\sum_{w' \in V} exp(S_{w'})}
  $$

- Our decoding algorithm defines a function to select a token form this distribution (e.g. taking the most probable word)

  $$
      \hat{y}_t = g(P(y_t = w | \{ y_{< t}\}))
  $$

## Decoding Algorithm - Greedy Decoding

Core idea: Always select the next token with the highest probability

$$
\hat{y}_t = \argmax_{w \in V} P(y_t = w | y_{< t})
$$

Problem: Greedy decoding cannot correct previous mistakes

- Input: il a m'entarte
- he \_\_\_
- he hit \_\_\_
- he hit a \_\_\_ (cannot go back now)

## Decoding Algorithm - Beam Search

Core idea: On each step of the decoder, keep track of the $k$ most probable partial translations (hypotheses). $k$ (Beam size) usually ranges from 5 to 10

$$
score(y_1, ..., y_t) = \log P_{LM} (y_1, ..., y_t | x) \\
= \sum_{i=1}^{t} \log P_{LM} (y_i | y_1, ..., y_{i-1}, x)
$$

- From each of the $k$ hypothesis at timestep $t$, we find the $k$ most likely next sequences (Total $k^2$ candidates)
- We only keep the $k$ highest-scoring candidates
- Beam search is not guaranteed to find the optimal solution, but is more efficient than exhaustive search

In greddy decoding, usually we decode until the model produces an `END` token

- E.g. `<START> He hit me with a pie <END>`

In beam-search decoding, different hypotheses may produce `END` tokens on different timesteps

- When a hypothesis produces `END`, that hypothesis is complete
- Place it aside, and continue exploring other hypotheses via beam search

We continue beam-search until

- We reach timestep $T$ (where $T$ is some predefined cutoff), or
- We have at least $n$ completed hypotheses (where $n$ is some predefined cutoff)

## Decoding with Sampling

Sample a token from the distribution of tokens

$$
\hat{y} \sim P(y_t = w | \{ y \}_{< t})
$$

- It is random, so you can sample any tokens
- Higher probability tokens are more likely

Problem: Vanilla sampling makes every token in the vocabulary an option

- Even if most of the probability mass in the distribution is over a limited set of options, the tail of the distribution could be very long, and in aggregate have considerable mass (heavy-tailed distribution)
- Many tokens are probably really wrong in the current context
- For these wrong tokens, we give them individually a tiny chance to be selected
- However, since there are many of them, we still give them as a group a high chance to be selected

Solution: Top-k sampling

- Only sample from the top $k$ tokens from the probability distribution
- Common values are $k = 50$, but it is up to the user
- Increasing $k$: More diverse but riskier outputs
- Decreasing $k$: Safe but generic outputs

Issues with Top-k sampling

- Top-k sampling can cut off too quickly
  - When the top few words all have similar probabilities, we cutoff words that are still probable
- Top-k sampling can also cut off too slowly
  - Only a few very probable words, the last few words in the top $k$ set are all very unlikely

Solution: Top-p (Nucleus) Sampling

- Problem: Probability distributions we sample from are dynamic
  - When the distribution $P_t$ is flatter, a limited $k$ removes many viable options
  - When the distribution $P_t$ is peakier, a high $k$ allows for too many options to have a chance of being selected
- Solution: Top-p sampling
  - Sample from all tokens in the top $p$ cumulative probability mass (i.e. where mass is concentrated)
  - Varies $k$ depending on the uniformity of $P_t$

## Decoding with Temperature

Recall: On timestep $t$, the model computes a probability distribution $P_t$ by applying the softmax function to a vector of scores $s \in \mathbb{R}^{|V|}$

$$
P_t(y_t = w) = \frac{\exp (S_w)}{\sum_{w' \in V} \exp(S_{w'})}
$$

You can apply a temperature hyperparameter $\tau$ to the softmax to rebalance $P_t$:

$$
P_t(y_t = w) = \frac{\exp (S_w / \tau)}{\sum_{w' \in V} \exp(S_{w'} / \tau)}
$$

- Raising $\tau > 1$: $P_t$ becomes more uniform
  - More diverse outputs (probability is spread around vocab)
- Lowering $\tau < 1$: $P_t$ becomes more spiky
  - Less diverse outputs (probability is concentrated around the top words)

Temperature is a hyperparameter for decoding, it can be tuned for both beam-search and sampling

## Decoding Takeaways

- Decoding is still a challenging problem in NLG: A lot more work to be done
- Different decoding algorithms can allow us to inject biases that encourage different properties of coherent NLG
- Some of the most impactful advances in NLG of the past few yearse have come from simple but effective modifications to decoding algorithms

# Evaluations for NLG

- Content Overlap Metrics
- Model-based metrics
- Human evaluations

## Content Overlap Metrics

```
Ref: They walked to the grocery store
Gen: The woman went to the hardware store
```

- Compute a score that indicates the lexical similarity between the generated and gold-standard (human written) text
- Fast and efficient, and widely used
- N-gram overlap metrics (BLEU, ROUGE, METEOR, CIDEr, etc.)

### N-Gram Overlap Metrics

Word overlap-based metrics; BLEU, ROUGE, METEOR, CIDEr etc

- Not ideal for machine translation
- Get progressively worse for tasks that are more open-ended than machine translation
  - Worse for summarisation, as longer output texts are harder to measure
  - Much worse for dialogue, which is more open-ended than summarisation
  - Much much worse for story generation, which are open-ended, but whose sequence length can make it seem you're getting decent scores

### Model-based Metrics

- Use learned representations of words and sentences to compute semantic similarity between generated and reference texts
- No more n-gram bottleneck, because text units are represented as embeddings
- Embeddings are pretrained, distance metrics used to measure similarity can be fixed

Common model-based metrics

- Vector similarity: Embedding based similarity for semantic distance between texts
- Word Mover's Distance: Measure distance between 2 sequences using word embedding similarity matching
- BERTSCORE: Uses pretrained contextual embeddings from BERT, and matches words in candidate and reference sentences by cosine similarity

### Human Evaluations

- Automatic metrics fall short of matching human decisions
- Human evaluation is the most important form of evaluation for text generation systems
- Gold standard in developing new automatic metrics
  - New automated metrics must correlate well with human evaluations

Procedure

- Ask humans to evaluate the quality of generated text
- Overall or along some specific dimension
  - Fluency
  - Coherence/consistency
  - Factuality and correctness
  - Common sense
  - Style/formality
  - Grammaticality

Human judgements are the gold standard, but is slow and expensive

- Results can be inconsistent
- Can be illogical
- Misinterpret your question
- Precision not recall

## Evaluation Takeaways

- Content overlap metrics provide a good starting point for evaluating the quality of generated texts, but they are not good enough on their own
- Model based metrics can be more correlated with human judgement, but behavior is not interpretable
- Human judgements are critical, but humans are inconsistent

In many cases, you are the best judge of output quality

- Look at your model generations, don't just rely on numbers
- Publicly release large samples of the output of systems that you create

# Emergent Zero-Shot Capabilities

Consider GPT-2 (1.5B parameters)

- One key emergent ability is zero-shot learning: The ability to do many tasks without examples, and no gradient updates, by simply
  - Specifying the right sequence prediction problem (e.g. question answering)
    - E.g. Give a passage about Tom Brady, then ask "Where was Tom Brady born?"
  - Comparing probabilities of sequences
    - E.g. The cat could not fit into the hat because it was too big. Does "it" refer to the cat or the hat?
    - Is P(because the cat was too big) >= P(because the hat was too big)

# Emergent Few-Shot Capability

Specify a task by simply prepending examples of the task before your example

- Also called in-context learning, to stress that no gradient updates are performed when learning a new task

Zero-shot:

```
Translate English to French:
cheese ->
```

One-shot:

```
Translate English to French:
sea otter -> loutre de mer
cheese ->
```

Few-shot:

```
Translate English to French:
sea otter -> loutre de mer
peppermint -> menthe poivree
plush giraffe -> girafe peluche
cheese ->
```

## Chain of Thought Prompting

- Provide input with example + reasoning. Then ask question.

![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)

Zero COT Prompting: Prepend "Let's think step by step" to the answer, and let the model continue generating

![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fzero-cot.79793bee.png&w=1080&q=75)

# Instruction Finetuning

Make the model sensitive to instructions

- Simply saying "Explain the moon landing to a 6 year old in a few sentences" may not give the expected results
- This is because language models are not aligned with user intent
- We can do finetuning to help

Finetuning:

1. Collect examples of (instruction, output) pairs across many tasks, and finetune an LM
2. Evaluate on unseen tasks

The Super-NaturalInstructions dataset contains over 1.6K tasks, 3M+ examples: Classification, sequence tagging, rewriting, translation etc.

## Common Evaluation Set

New benchmarks for measuring LM performance on 57 diverse knowledge intensive tasks

- [Massive Multitask Language Understanding (MMLU)](https://arxiv.org/abs/2009.03300)
- [BIG-Bench](https://github.com/google/BIG-bench)

## Limitations

- One limitation of instruction finetuning: Expensive to collect ground truth data for tasks
- But there are other, subtler limitations too
  - Open-ended creative geenration have no right answer
  - Language modelling penalises all token-level mistakes equally, but some errors are worse than others
- Even with instruction finetuning, there is a mismatch between LM objective, and the objective of "satisfying the human preferences"
- Can we explicitly attempt to satisfy human preferences?

# Reinforcement Learning with Human Feedback

Consider training a language model on some task (e.g. summarisation)

- For each LM sample s, imagine we had a way to obtain a human reward of that summary: $R(s) \in \mathbb{R}$, higher reward = better
- Now we want to maximise the expected reward of samples from our LM

  $$
      \mathbb{E}_{\hat{S} \sim p_\theta(s)}[R(\hat{s})]
  $$

Problem 1: Human-in-the-loop is expensive

- Solution: Instead of directly asking humans for preferences, model their preferences as a separate NLP problem
- Train an LM $RM_\phi(s)$ to predict human preferences from an annotated dataset, then optimise for $RM_\phi$ instead

Problem 2: Human judgements are noisy and miscalibrated

- Solution: Instead of asking for direct ratings, ask for pairwise comparisons, which can be more reliable

$$
J_{RM}(\phi) = -\mathbb{E}_{(s^w, s^l) \sim D} [\log \sigma(RM_\phi(s^w) - RM_\phi(s^l))]
$$

- $s^w$: Winning sample (More preferable sample)
- $s^l$: Losing sample (Less preferable sample)
- $s^w$ should score higher than $s^l$

Limitations

- Human preferences are unreliable
- Reward hacking is a common problem in RL
- Chatbots are rewarded to produce responses that seem authoritative and helpful, regardless of truth
- This can result in making up facts + hallucinations

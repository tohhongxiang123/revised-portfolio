# Text Normalization

# Words and Corpora

Words don't just appear out of nowhere
- In NLP, we work on a particular corpus (or a few corpora) to develop a system before deployment

A corpus is a computer-readable collection of text or speech
- A corpus is sometimes called a "text dataset"
- Example corpus can be a collection of online reviews

# Corpora

Code switching: Speakers or writers using multiple languages in a single sentence/conversation
- Singlish

Texts could be in different genres: news, medical reports, online reviews etc

Texts can reflect demographic characteristics such as age, gender, socio-economic class

Languages change over time

## Corpora Datasheet

When developing computational models for language processing from a corpus, we must consider
- Who produced the language
- In what context
- For what purpose

A corpora datasheet specifies properties of the corpus
- Motivation: Why the corpus was collected, by whom, and who funded it?
- Situation: When and in which situation was the text written/spoken? Such as spoken conversations, social media comments, monologues etc.
- Language variety: What language (including region and dialect) was the corpus in?
- Speaker demographics: What was the age/gender/etc of the authors of the text?
- Collection process: How big is the data? If the data is a subsample, how was it sampled? Was data collected with consent? How was the data pre-processed? What metadata is available?
- Annotation process: What are the annotations? What are the demographics of the annotators, how were they trained? How was the data annotated?
- Distribution: Are there copyright or other intellectual property restrictions?

# Lemma, Wordform, Word Type, Word Token

A lemma is the base form of a set of words in general which has the same stem, the same major part-of-speech, and the same word sense
- **cat**: cat, cats
- **break**: breaks, broke, broken, breaking

The wordform is the full inflected or derived form of the word
- Inflectional: Has the same word class as the original: cat -> cats
- Derivational: Change of word class: care -> careless, computer -> computerize

Word type vs word token
- Types are the number of **distinct words** in the corpus
- Tokens refer to the **number of occurrences** of words
- "Singapore is most beautiful city in Asia, 11th most beautiful
in the world!" has 13 tokens (13 words) and 11 types (11 distinct words)

# Text Normalization

Tokenizing (segmenting) words
- The task of segmenting running text into separate words (and/or symbols)
- We use a **tokenizer** to tokenize the text

The process of text normalization:
1. Tokenizing
2. Normalizing word forms (Lemmatisation)
3. Segmenting sentences

## Tokenization

Words in english are separated by whitespace. Using a naive white-space splitter gives us words such as
- cents.
- said,
- positive."
- Crazy?'

Word-internal punctuation
- e.g.
- Ph.D.
- AT&T
- Yahoo!

Implementation of tokenisation
- Deterministic algorithms based on RE
- E.g. Python-based Natural Language Toolkit (NLTK)

Sometimes, whitespace splitters do not work
- E.g. URL segmentation: "https://www.google.com" or "www.youtube.com"
- Hashtag segmentation: "#imdying", "#pleasehelpme"

Tokenisation is also **language dependent**
- Chinese words composed of characters
- For other languages, we usually use probabilistic algorithms or deep-learning
- German noun compounds are not segmented: "Lebensversicherungsgesellschaftsangestellter" (life insurance company employee)

## Normalizing Word Forms

The task of putting words/tokens in a standard format, choosing a single normal form for words with multiple forms

## Case Folding and Lemmatization

Case folding
- Mapping everything to lower case
- Case folding is useful for the generalisation of many tasks, such as information retrieval or speech recognition
- Case folding is generally not done for information extraction and machine translation

Lemmatisation
- The task of determining that 2 words have the same root despite their surface differences
- For e.g., "am", "are", "is" have the same shared lemma "be"
- "dinner" and "dinners" have the same shared lemma "dinner"
- The most sophisticated methods for lemmatisation involve complete morphological parsing of the word

Morphology is the study of the way words are built up from smaller meaning-bearing units called "morphemes"

There are 2 classes of morphemes
- Stems: The central morpheme of the word which supplies the main meaning
- Affixes: Adding additional meaning of various kinds
- For e.g. "fox" consists of 1 morpheme (fox)
- "cats" consists of 2 morphemes (the stem "cat" and the affix "-s")

Note that morphological parsing is complex (and slow for some tasks)
- A morphological parser takes a word like "cats" and parses it into "cat" and "s"

Stemming
- A simpler but cruder form of lemmatisation, which mainly consists of chopping off word-final stemming affixes
- Can be considered a naive form of morphological parsing
- Resultant stem may not be a valid english word
- There are many stemmers that exist, and the most popular/common one is [Porter's stemmer](https://tartarus.org/martin/PorterStemmer/def.txt)
    - Porter's stemmer does not need a lexicon (or dictionary)
    - It uses a set of rewrite rules that strips suffixes and is run in series as a cascade
      - ING -> "" (Monitoring -> Monitor)
      - SSES -> SS (grasses -> grass)
    - E.g. the word "computerisation"
      - ization -> ize (computerisation -> computerize)
      - ize -> "" (computerize -> computer)

An example of stemming using Porter's Stemmer:

Original text:
```
This was not the map we found in Billy Bones's chest, but 
an accurate copy, complete in all things-names and heights
and soundings-with the single exception of the red crosses
and the written notes.
```

After:
```
Thi wa not the map we found in Billi Bone s chest but an
accur copi complet in all thing name and height and sound
with the singl except of the red cross and the written note
```

Link to try [here](https://textanalysisonline.com/nltk-porter-stemmer)

## Segmenting Sentences

We split a document/paragraph into sentences

Punctuation (like periods, question marks, exclamation points) are the most useful cues for segmenting a text into sentences
- Question marks and exclamation marks are generally unambiguous markers of sentence boundaries, but not always true (E.g. Yahoo!)
- Periods are more ambiguous (Ph.D., Mr., U.S.A)

Sentence segmentation/split/tokenisation can be done by machine learning models as well

# Edit Distance

## Measuring String Similarity

Much of language processing involves measuring **how similar 2 strings are**
- Used in applications such as text correction (should "graffe" be replaced with "giraffe" or "gaffe")

## Minimum Edit Distance (Levenshtein Distance)

The minimum edit distance between 2 strings is the minimum number of editing operations required to transform a string into another. The operations are:
- Insertion
- Deletion
- Substitution

Editing operations can be on letters or words, depending on the application

To calculate the minimum edit distance, we can use the [levenshtein distance algorithm](https://en.wikipedia.org/wiki/Levenshtein_distance) which uses dynamic programming

$$
D[i, j] = \min \begin{cases}
    D[i - 1, j] + 1, \\
    D[i, j - 1] + 1, \\
    D[i - 1, j - 1] + \begin{cases}
        1; \text{if source[i] $\neq$ target[j]} \\
        0; \text{otherwise}
    \end{cases}
\end{cases}
$$

```python
def minDistance(self, word1: str, word2: str) -> int:
        dp = []
        for i in range(len(word1) + 1):
            dp.append([0] * (len(word2) + 1))

        for i in range(len(word2) + 1):
            dp[0][i] = i
        for i in range(len(word1) + 1):
            dp[i][0] = i

        for i in range(1, len(word1) + 1):
            for j in range(1, len(word2) + 1):
                if word1[i - 1] == word2[j - 1]:
                    dp[i][j] = dp[i - 1][j - 1]
                else:
                    dp[i][j] = min(
                        dp[i][j - 1], 
                        dp[i - 1][j], 
                        dp[i - 1][j - 1]
                    ) + 1 

        return dp[-1][-1] 
```

Note: In some algorithms, we do not allow substitution, so a "substitution" would be a delete, then an add, which has a cost of 2 instead of 1

## Computing Alignments

- We often need to align each character of the 2 strings to each other
- We do this by keeping backtrace pointers
  - Everytime we enter a cell, we remember where we came from
  - Some cells have multiple extensions because the minimum edit distance can be achieved from more than 1 of the previous operations
- When we reach the end, we trace back the path
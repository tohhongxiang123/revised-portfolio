# N-Gram Language Models

The simplest language model

-   A n-gram is a sequence of $n$ words

## Our Task: Computing $P(w | h)$

$P(w | h)$: The probability of a word $w$ given some history $h$

-   For e.g. $h$ is "I will make" and the word $w$ is "it"

We want to estimate the probability of $P(w | h)$ from a large text collection

-   For e.g. `P("it" | "I will make")`
-   Count the number of times $h$ occurs
-   Count the number of times $w + h$ occurs
-   $P(w | h) = \frac{C(w + h)}{C(h)}$
-   What if $h$ is very long?

## Chain Rule of Probability: A better way to compute $P(w | h)$

We denote the probability that the random variable $X_i$ will take on the value $x$ as $P(X_i = x)$

We also denote the probability $P(w_1, ..., w_n) = P(w_{1:n})$

The chain rule:

$$
\begin{aligned}
P(X_1, ..., X_n) &= P(X_1) P(X_2 | X_1) P(X_3 | X_1 X_2) ... P(X_n | X_{1:n-1}) \\
&= \prod_{k=1}^{n} P(X_k | X_{1:k-1})
\end{aligned}
$$

Applying the chain rule to words, we can estimate the joint probability of an entire sequence of words

$$
P(w_{1:n}) = \prod_{k=1}^{n} P(w_k | w_{1:k-1})
$$

## Bigram Model

Approximates the probability of a word occurring given all the previous words $P(w_n | w_{1:n-1})$ using only the conditional probability of the previous word $P(w_n | w_{n - 1})$

-   $P(w_n | w_{1:n-1}) \approx P(w_n | w_{n - 1})$

Generalizing to the N-gram model:

$$
P(w_n | w_{1:n-1}) \approx P(w_n | w_{n - N + 1: n - 1})
$$

For e.g., for the sentence "I will make it":

```
P("I will make it") = P("I") P("will" | "I") P("make" | "I will") P("it" | "I will make")
```

Using bigram:

```
P("I will make it") = P("I") P("will" | "I") P("make" | "will") P("it" | "make")
```

To compute $P(w_n | w_{n - 1})$, we estimate bigram probabilities by maximum likelihood estimation (MLE)

$$
P(w_n | w_{n - 1}) = \frac{C(w_{n-1} w_n)}{C(w_{n-1})}
$$

-   $C(x)$ is the frequency of $x$
-   Note: $w_n$ happens after $w$, so we divide by the count of the **previous word**

## Practical Problems

Probability of a sentence is typically very small. To avoid numerical underflow, we use log probabilities

$$
p_1 p_2 p_3 p_4 = e^{\log p_1 + \log p_2 + \log p_3 + \log p_4}
$$

## Evaluating Language Model

Intrinsic Evaluation

-   If a test set is very frequently used, we may learn some of its characteristics. There is a risk that we tune some parameters to make the model perform better on the test set
-   A development set is often used to learn parameters
-   We split the data into 3 sets: Training, development and test
    -   Training set is used to train the model
    -   Development set is used for parameter search
    -   Test set is truly unseen, and used to evaluate the model after training

### Perplexity

Perplexity (PP) is the probability of the test set (assigned by the language model), normalized by the number of words. $N$ denotes number of words in a test data, $w_1, ..., w_N$ is the test data.

$$
PP(W) = P(w_1 ... w_N)^{-\frac{1}{N}}
$$

Chain rule:

$$
PP(W) = \sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_i | w_1 ... w_{i - 1})}}
$$

For bigrams:

$$
PP(W) = \sqrt[N]{\prod_{i=1}^{N} \frac{1}{P(w_i |  w_{i - 1})}}
$$

A good model gives high probability on the test data, hence a **low** perplexity score

-   An improvement in perplexity does not guarantee an improvement in the performance on a real task

### Out of Vocabulary Words

-   The model has no knowledge of these words $P(x_{unknown} | x) = 0$
-   Hence perplexity is undefined
-   Happens to all unknown words

### Handling OOV Words

In training, we add a pseudo word `<UNK>`

-   Any potential unknown words in the test set is considered an instance of `<UNK>`
-   But where are the instances of `<UNK>` in the training data?

Modelling UNK:

-   Choose a vocabulary (word list) that is fixed in advance, before training
-   In the training set, convert any words that are not in this vocabulary into UNK, in a text normalization step
-   Estimate probabilities of UNK from its counts, just like any other regular word in the training set

How to create a vocabulary:

-   Approach 1: Based on some existing knowledge of a dataset, and fix a vocabulary in advance
-   Approach 2: Implicitly define a vocabulary set based on word distribution

Example for approach 2:

-   Count frequency of every word in the training dataset
-   All words that appear less than $k$ times are considered unknown

### Rare Word Combinations

-   Words such as "offer" or "loan" are common words
-   However in the training set, "denied offer" or "denied loan" never occurs. Hence `P(offer | denied) = 0`, but that should not be the case

The training data is never large enough to cover all possible word combinations

We use **smoothing** to avoid assigning 0 probabilities to unseen events. There are many smoothing techniques available

-   Laplace (add-one) smoothing
-   Add-$k$ smoothing
-   Stupid backoff
-   Kneser-Ney smoothing

The simplest way to perform smoothing is Laplace Smoothing

-   Assuming every possible n-gram appears once which we do not explicitly observe
-   Add one to all the n-gram counts, before we normalize them into probabilities
-   However, laplace smoothing does not perform well enough to be used for smoothing in modern n-gram models
    -   But it usefully introduces many concepts used in other smoothing algorithms
    -   Is a practical smoothing algorithm for other tasks such as text classification

Using Laplace Smoothing:

-   For each word, we added $|V|$ number of additional appearances ($|V|$ is the size of the vocabulary)
-   Before smoothing, $P(w_n | w_{n-1}) = \frac{C(w_{n-1} w_n)}{C(w_n)}$
-   After smoothing, $P(w_n | w_{n - 1}) = \frac{C(w_{n-1} w_n) + 1}{C(w_n) + |V|}$
-   We have the problem of reducing already existing probailities, as too much probability mass is moved to all the zeroes

### Add-k Smoothing

Instead of adding 1, we add a fractional value $k$, where $0 < k < 1$. This moves less probability mass from the seen to the unseen events

$$
P(w_n | w_{n - 1}) = \frac{C(w_{n - 1} w_n) + k}{C(w_{n-1}) + k|V|}
$$

The value of $k$ is usually set to 0.1, 0.05, 0.5 or other values, determined by the development set

Add-k is useful for some tasks such as text classification, but does not do so well for language modelling

### Backoff and Interpolation

Backoff: Use less context

-   We use the trigram if evidence is sufficient, otherwise we use the bigram, otherwise the unigram
-   We only back off to a lower-order n-gram if we have zero evidence for a higher order n-gram

Interpolation: We mix probability estimates from all the n-gram estimators, weighting and combining the trigram, bigram and unigram counts

$$
\hat{P}(w_n | w_{n - 2} w_{n - 1}) = \lambda_1 P(w_n) + \lambda_2 P(w_n | w_{n-1}) + \lambda_3 P(w_n | w_{n - 2} w_{n - 1}) \\
\lambda_1 + \lambda_2 + \lambda_3 = 1
$$

# Language Generation

Sampling sentences from a language model

-   Sampling from a distribution: Choose random points according to their likelihood
-   Sampling from a language model, which represents a distribution over sentences, means to generate some sentences: choose each sentence according to its likelihood

Visualize how sentence generation works for the unigram case

-   A unigram language model is a collection of unigram probabilities
-   E.g. $P(the), P(a), P(of)$
-   We place these unigram probabilities in a line, ordered by probabilities
-   We generate a random value $0 < x < 1$, find the point on the probability line, and print the word whose interval includes this random value

## Language Generation (Bigram Case)

-   Generate the first word by sampling $P(w_1 | <S>)$
-   Generate the second word by sampling $P(w_2 | w_1)$
-   Generate the rest of the words
-   Generation stops when $P(</S> | w_n)$ is generated

# Parts of Speech and Named Entities

Parts of Speech (POS) refers to word classes such as Noun, Verb, Adjective etc

-   Also known as lexical categories, word classes, morphological classes, lexical tags
-   Knowing word classes tells us more about neightboring words and syntactic structure
    -   E.g. nouns are preceded by determiners/adjectives
    -   Verbs have a dependency link to nouns
-   POS tagging is a key aspect of parsing sentence structures

Named Entity is a proper name for person, location, organisation etc.

-   NEs are useful clues to sentence structure and meaning understanding
-   Knowing if a named entity such as "Washington" is a name of a person/place/university is important to tasks like question answering and information extraction

Sequence labelling:

-   POS tagging: Takes a sequence of words, and assigns each word a POS like NOUN or VERB
-   Named Entity Recognition (NER): Assigns words or phrases tags like PERSON, LOCATION, ORGANISATION

# POS Tagging

![](https://www.researchgate.net/publication/2873803/figure/tbl1/AS:669991049392137@1536749722377/1-The-Penn-Treebank-POS-tagset.png)

-   There are multiple POS tagsets defined (what POS tags can be assigned)

# More About Word Classes

-   Closed classes are those with relatively fixed memberships, such as prepositions; new prepositions are rarely coined
    -   Closed class words are generally funcction words (and, of, it) which tend to be short, occur frequently, and often have structuring uses in grammar
-   Nouns and verbs are open classes: new nouns and verbs like iPhone or fax are continually being created/borrowed
    -   Nouns are words for people, places, things etc. Common nouns include concrete terms like cat and mango, abstractions like algorithm and beauty, and verb-like terms like pacing
        -   Proper nouns are names of specific persons/entities
    -   Verbs refer to actions and processes, including main verbs like draw and provide
        -   English verbs have inflections: eat/eats/eating/eaten
    -   Adjectives often describe properties or qualities of nouns
    -   Adverbs generally modify something (often verbs, hence the name adverb)
-   Prepositions indicate spatial/temporal relations
    -   on, before, by, beside
-   Determiners like "this" and "that" can mark the start of an article English noun phrase
    -   A, an, the
-   Pronouns act as a shorthand for referring to an entity/event
    -   Personal pronouns refer to persons/entities (you, she, I, it, me)
    -   Possessive pronouns are forms of personal pronouns that indicate either actual possession, or more often an abstract relation between the person and some object (my, yours, his etc)
    -   Wh-pronouns (what, who, whom) are used in certain question forms, or act as complementisers (Frida, who married Diego...)

# POS Tagging

POS tagging is the process of assigning a part-of-speech to each word in a text

-   Janet will back the bill
-   NOUN AUX VERB DET NOUN

POS tagging is challenging

-   Words are ambiguous - have more than one possible POS
    -   Tagging is a disambiguation task: the goal is to find the correct tag for the situation
    -   Words can have multiple POSes, e.g. Book the flight, hand me the book
-   Accuracy of most POS tagging algorithms are extremely high (97%)
    -   Most-frequent-tag baseline has an accuracy of about 92%

## POS Tagging with Hidden Markov Model (HMM)

-   A sequence labeller assigns a label to each unit (word) in a sequence (sentence), thus mapping a sequence of observations to a sequence of labels of the same length
-   HMM is a classic probabilistic sequence model
    -   Given a sequence of words, it computes the probability distribution over possible sequences of labels, and chooses the best label sequence
-   POS Tagging in probabilistic view
    -   Consider **all possible sequences of tags** (each tag for one word)
    -   Out of this universe of sequences, choose the tag sequence which is the **most probable** given the observation sequence of $n$ words

## Markov Chains

-   HMM is based on augmenting the markov chain
-   A markov chain is a model on the probabilities of each sequence of random variables (states), each of which can take on values from some set
-   Markov assumption: The probability of next state only depends on the current state

## HMM

-   A markov chain is useful to compute a probability for a sequence of observable events
    -   Based on today's weather, predict tomorrow's weather
    -   Based on the current word, predict the next word (as in bigram models)
-   In many cases, the events we are interested in are hidden
    -   E.g. in POS tagging, we can only observe words, but not their tags
    -   We cannot use the current tag to predict the next tag for a word sequence
    -   We call the tags hidden because they are not observed
-   A hidden Markov Model (HMM) allows us to talk about both observed events and hidden events. For POS tagging
    -   Observed events are words in the input sentences
    -   Hidden events are the POS tags for these words
    -   The observed events are considered as causal factors in the probabilistic model

We have

-   $Q = q_1 ... q_N$: A set of $N$ states
-   $A = a_{11} a_{12} ... a_{N1} a_{N2} ... a_{NN}$: A transition probability matrix $A$, each $a_{ij}$ represents the probability of moving from state $i$ to $j$, such that $\forall i, \sum_{j=1}^{n} a_{ij} = 1$
-   $O = o_1 ... o_T$: A sequence of $T$ observations, each one drawn from a vocabulary $V = v_1 ... v_V$
-   $B = b_i(o_t)$: A sequence of observation likelihoods (also called emission probabilities), each expressing the probability of an observation $o_t$ being generated form state $q_i$
-   $\pi = \pi_1 ... \pi_N$: An initial probability distribution over states. $\pi_i$ is the probability that the markov chain will start in state $i$. Some states $j$ may have $\pi_j = 0$, meaning that they cannot be initial states. Also, $\sum_{i=1}^{n} \pi_i = 1$

For POS tagging

-   States are teh set of possible tags in the tagset
-   Transition probabilities are the probabilities of moving from one tag to another (e.g. $P(noun | adj)$)
-   Observation is a word: observations are the given sentence for POS tagging
-   Observation likelihood is the likelihood of seeing a word for a tag (e.g. $P(table | noun)$)

A first-order hidden markov model instantiates 2 simplifying assumptions

1. Markov assumption: The probability of a particular state depends only on the previous state: $P(q_i = a | q_1 ... q_{i-1}) = P(q_i = a | q_{i - 1})$
2. Output independence assumption: The probability of an output observation $o_i$ depends only on state $q_i$ that produced the observation and not on any other states/observations: $P(o_i | q_1, ..., q_T ; o_1 ..., o_T) = P(o_i | q_i)$

Decoding: For any model such as a HMM, that contains hidden variables, the task of determining the hidden variables sequence corresponding to thte sequence of observations is called decoding

## POS Tagging with HMM

Out of possible sequences of $n$ tags $t_1, ..., t_n$, the single tag sequence such that $P(t_1 ... t_n | w_1 ... w_n)$ is the highest

$$
\hat{t}_{1:n} = \argmax_{t_{1:n}} P(t_{1:n} | w_{1:n})
$$

-   The hat means "our estimate of the best one"

We can use Bayes rule to transform this equation into a set of other probabilities that are easier to compute

$$
\begin{aligned}
\hat{t}_{1:n} &= \argmax_{t_{1:n}} P(t_{1:n} | w_{1:n}) \\
&= \argmax_{t_{1:n}} \frac{P(w_{1:n} | t_{1:n}) P(t_{1:n})}{P(w_{1:n})} \\
&= \argmax_{t_{1:n}} P(w_{1:n} | t_{1:n}) P(t_{1:n})
\end{aligned}
$$

-   $P(w_{1:n} | t_{1:n}) \approx \prod_{i=1}^{n} P(w_i | t_i)$: The probability of a word appearing depends only on its own POS tag (e.g. $P(table|noun)$)
-   $P(t_{1:n}) \approx \prod_{i=1}^{n} P(t_i | t_{i-1})$: Probability of a tag appearing depends only on the previous tag (e.g. $P(Noun | Adj)$)

Therefore

$$
P(w_{1:n} | t_{1:n}) P(t_{1:n}) = \prod_{i=1}^{n} P(w_i | t_i) P(t_i | t_{i-1})
$$

## Computing the Two Kinds of Probabilities

Given a fully annotated dataset, where every token in a sentence is annotated with its POS tag

-   E.g. The/DT grand/JJ jury/NN commented/VBD on/IN a/DT number/NN of/IN other/JJ topics/NNS ./.
-   We will have tag transitions DT -> JJ, JJ -> NN, NN -> VBD etc
-   We also have words and their frequencies for each tag

The tag transition probability $P(t_i | t_{i-1})$

$$
P(t_i | t_{i - 1}) = \frac{\text{Count}(t_{i-1} t_i)}{\text{Count}(t_{i-1})}
$$

-   Relative frequency of JJ following DT

The word likelihood probability $P(w_i | t_i)$

$$
P(w_i | t_i) = \frac{\text{Count}(t_i, w_i)}{\text{Count}(t_i)}
$$

-   Among all words tagged to DT, how many are "The"

## HMM Decoding

-   There are many possible sequences
-   We need an algorithm to minimise our computations to choose the most probable sequence
    -   Many computations are reparative
    -   We can save computed results (dynamic programming)

## Viterbi Algorithm

The Viterbi Algorithm first sets up a probabilty matrix

-   One column for each observation $o_t$ (A word)
-   One row for each state $q_t$ (tag) in the state graph
-   Each cell of the lattice $v_t(j)$ represents the probability that the HMM is in state $j$ after seeing the first $t$ observations and passing through the **most probable state sequences** $q_1, ..., q_{t-1}$, given the HMM model parameters
    -   $v_t(j)$ is computed recursively taking themost probable path to this cell

1. From left to right, column by column, compute Viterbi path probability for each cell $v_t(j)$
2. Maintain a backtrace pointer for each cell, to indicate from which cell the $v_t(j)$ is obtained
3. When $v_t(j)$'s for the last observation are computed, select the max value, and traceback the sequence

## Conditional Random Fields (CRFs)

HMM is useful and powerful, but needs some augmentations

-   Not straightforward to handle unknown words such as proper names and acronyms
-   Hard for generative models to add arbitrary features directly
    -   E.g. capitalisation can be indicative for proper nouns
    -   Words ending with "-ed" tend to be past tense

CRF is a discriminative sequence model based on log-linear models

-   We briefly describe the linear chain CRF
-   Given a sequence of input words $X = x_1 ... x_n$, and want to compute a sequence of output tags $Y = y_1 ... y_n$
-   In a CRF, we compute $P(Y | X)$ directly, training the CRF to discriminate among the possible tag sequences
    -   $\hat{Y} = \argmax_{Y \in \mathcal{Y}} P(Y | X)$

A CRF is a log-linear model that assigns a probability to an entire output (tag) sequence $Y$ out of all possible sequences $\mathcal{Y}$, given the entire input (word) sequence $X$

-   CRF does not compute probability for each tag at each time step
-   At each time step, CRF computes log-linear functions over a set of relevant features
-   These local features are aggregated and normalised to produce a global probability for the whole sequence

In a CRF, the feature function $F$ maps an entire input sequence $X$ and an entire output sequence $Y$ to a feature vector

-   Assume we have $K$ features, with weight $w_k$ for each feature $F_k$

$$
P(Y | X) = \frac{\exp \left( \sum_{k=1}^{K} w_k F_k(X, Y) \right)}{\sum_{Y' \in \mathcal{Y}} \left( \sum_{k=1}^{K} w_k F_k (X, Y') \right )}
$$

# Named Entity Recognition (NER)

A named entity is roughly anything that can be referred to with a proper name: A person/location/organisation/concept etc

-   Depends on the domain

The task of NER is to find spans of text that constitute proper names and tag the type of the entity

-   4 entity tags are the most common: PER (person), LOC (location), ORG (organisation) or GPE (geo-political entity)
-   E.g. United Airlines (ORG)
-   Friday (TIME)
-   $6 (MONEY)
-   Singapore (LOC)

## POS Tagging vs NER

Differences

-   In POS tagging, each word gets one tag
-   In NER, we do not know the boundary of names before we can label them (Named entities can span multiple words)

Similarities

-   Same word may have different POS tags (Back can be ADJ or ADV)
-   Same text span may have different NE types (E.g. Victoria, Washington)
-   Both POS tagging and NER require surrounding words as context to make the tagging
-   Both POS tagging and NER work at sentence level (sequence labelling)

## Sequence Labelling for NER

-   A standard approach to sequence labelling for a span-recognition problem like NER is BIO tagging
    -   B: Begin
    -   I: Inside
    -   O: Outside
-   Variants include: IO, BIOES (E for ending, S for single), BIOEU (U for unit)

E.g.: Jane Villanueva of United, a unit of United Airlines Holding, said the fare applies to the Chicago route

| Word       | IO Label | BIO Label | BIOES Label |
| ---------- | -------- | --------- | ----------- |
| Jane       | I-PER    | B-PER     | B-PER       |
| Villanueva | I-PER    | I-PER     | E-PER       |
| of         | O        | O         | O           |
| United     | I-ORG    | B-ORG     | B-ORG       |
| Airlines   | I-ORG    | I-ORG     | I-ORG       |
| Holding    | I-ORG    | I-ORG     | E-ORG       |
| discussed  | O        | O         | O           |
| the        | O        | O         | O           |
| Chicago    | I-LOC    | B-LOC     | S-LOC       |
| route      | O        | O         | O           |
| .          | O        | O         | O           |

Models for POS Tagging can be applied to NER and other sequence labelling tasks

-   HMM
-   CRF

## Features for NER for CRF models

Example features

-   Identity of $w_i$ (or the word itself), identity of neighboring words
-   Embeddings for $w_i$, embeddings for neighboring words
-   Part of speech of $w_i$, part of speech for neighboring words
-   Presence of $w_i$ in a gazetteer
-   $w_i$ contains a particular prefix (from all prefixes of length <= 4)
-   $w_i$ contains a particular suffix (from all suffixes of length <= 4)
-   Word shape of $w_i$ (upper/lower case letters/digits), word shape of neighboring words
-   Short word shape of $w_i$, short word shape of neighboring words
    -   For short word shape, consecutive character types are removed, XXxxdd becomes Xxd

Gazetteer: A list of place names as an external resource. This can be replaced with a list of names in domain-specific settings

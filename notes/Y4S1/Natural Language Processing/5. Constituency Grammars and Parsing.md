# Constituency Grammars and Parsing

# Syntax vs Semantic

-   The word syntax comes from "syntaxis", meaning "setting out together or arrangement"
-   In our context: syntax refers to the way words are arranged together

    -   There are certain probabilities between words, e.g. N-gram model
    -   Words can often be replaced by words under the same POS tags, like "I have a green apple" vs "I have a red apple"
    -   A formal way to describe syntax: Grammar

-   Context free grammars are the backbone of many formal models of the syntax of natural language
    -   Applications: Grammar checking, semantic interpretation, dialogue understanding and machine translation
-   There are other forms of grammars: Combinatory categorial grammar (CCG) and syntactic dependency

# Constituency

-   Syntactic constituency is the idea that groups of words can behave as a single unit, or constituents
    -   E.g. Noun Phrase, a seqeunce of words surrounding at least one noun
        -   Harry the Horse
        -   A high-class spot
        -   Three parties from Brooklyn
-   Prepositional phrases
    -   The whole prepositional phrase behaves as a single unit and can be placed at different places in the sentence
    -   E.g. **On september 17th**, I'd like to fly from Atlanta to Denver
    -   I'd like to fly **on september 17th** from Atlanta to Denver
    -   I'd like to fly from Atlanta to Denver **on september 17th**

# Context-Free Grammar (CFG)

-   CFG is also known as Phrase-Structure Grammars, and the formalism is equivalent to Backus-Naur Form (BNF)
-   A CFG consists of
    -   A set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together
    -   A lexicon of words and symbols
-   The symbols in a CFG are divided into 2 classes
    -   Terminal: Symbols that correspond to the words in the language
    -   Nonterminals: Symbols that express abstractions over these terminals

# Treebanks

A treebank is a corpus in which each sentence is annotated with a parse tree

-   Sufficiently robust grammars consisting of CFG rules can be used to assign a parse tree to any sentence
    -   Build a corpus where every sentence in the collection is paired with a corresponding parse tree
    -   Such a syntactically annotated corpus is called a treebank
    -   Typically build using Parsers to autmatically parse each sentence, followed hand-corrections by humans
-   E.g. treebanks
    -   The [Penn Treebank Project](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) for constituency parsing
    -   The [Universal Dependencies Project](https://universaldependencies.org/) for dependency parsing
-   From treebanks, we can derive the grammars of a language

# Constituency Parsing

Syntactic parsing is the task of assigning a syntactic structure to a sentence

-   We need a grammar and a parser

Parse trees can be used in many applications

-   Grammar checking: Sentences that cannot be parsed may have grammatical errors
-   Semantic analysis: Parse trees serve as an intermediate stage of representation for understanding the meaning of a sentence
-   Other applications such as question answering and information abstraction

Key challenge: Structural ambiguity

-   Occurs when a grammar can assign more than one parse to a sentence
-   I shot an elephant in my pajamas
    -   Either I am in my pajamas and shot an elephant
    -   Or an elephant is wearing my pajamas, and I shot that elephant

# Structural Ambiguity: Attachment ambiguity

-   A policeman shot the criminal with a revolver
-   The criminal had a revolver? The policeman used a revolver?

Attachment ambiguity: If a particular constituent can be attached to the parse tree at more than one place

# Structural Ambiguity: Coordination Ambiguity

Coordination ambiguity occurs in some phrases with a conjunction word like "and"

-   old men and women

    -   [old men] and [women]
    -   [old][men and women]

-   There are many grammatically correct but semantically unreasonable parses for naturally occurring sentences
-   Such ambiguity affects all parsers

# Parsing: Dynamic Programming Approach

-   In syntactic parsing, the subproblems represents parse trees for all constituents detected in the input
    -   Ince a constituent has been discovered in a segment of input, we can record it and make it available for usein any subsequent derivation
-   We use CKY (Cocke-Kasami-Younger) algorithm

## CKY Algorithm

[Description of the CYK Algorithm](https://www.youtube.com/watch?v=f6ZrBBs7aJE)

CKY requires grammars to be in Chomsky-Normal form (CNF)

-   CNF rules can only be in 2 forms
    -   $A \to BC$
    -   $A \to w$
    -   The right hand side can only be either 2 non-terminals, or a single terminal
-   Any CFG can be converted into a corresponding CNF grammar
    -   Rules that mix terminals with non-terminals on the RHS
        -   $INF-VP \to to \ VP$. Create a dummy non-terminal $TO$
        -   $INF-VP \to TO \ VP, TO \to to$
    -   Rules with only a single non-terminal on the RHS
        -   $S \to VP$. Rewrite RHS and expand $VP$ with all its corresponding rules
        -   $S \to Verb \ NP, S \to Verb \ NP \ PP, ...$
    -   Rules with more than 2 non-terminals
        -   $S \to Verb \ NP \ PP$
        -   $S \to X_1 \ PP, X_1 \to Verb \ NP$

The algorithm is as follows

-   For a sentence of $n$ words, we work with an upper-triangular portion of $(n + 1) \times (n + 1)$ matrix
-   The indexes starting with 0 points at the gaps between input words
-   Each cell $[i, j]$ contains the set of nonterminals that represent all constituents that span positions $i$ to $j$ of the input
    -   E.g. `0 Book 1 the 2 flight 3 through 4 Houston 5`
-   We loop from left to right, bottom to top
    -   [0, 1], then [1, 2], [0, 2], then [2, 3], [1, 3], [0, 2] etc

# Evaluating Parsers

The standard tool for evaluating parsers that assign a single parse tree to a sentence is the PARSEVAL metrics

-   PARSEVAL metric measures how much the constituents in the hypothesis parse tree look like the constituents in a reference parse
-   PARSEVAL thus requires a human-labeled reference parse tree for each sentence in a test set

A constituent in a hypothesis parse $C_h$ of a sentence $s$ is labelled correct if there is a constituent in the reference parse $C_r$ with the same starting, ending and non-terminal symbols

-   $recall = \frac{\text{No. of correct constituents in hypothesis parse of s}}{\text{No. of total constituents in reference parse of s}}$
-   $precision = \frac{\text{No. of correct constituents in hypothesis parse of s}}{\text{No. of total constituents in hypothesis parse of s}}$
-   $F_1 = \frac{2PR}{P + R}$

# Partial/Shallow Parsing

Many language processing tasks do not require complex or complete parse trees for all inputs

-   A partial parse or shallow parse of input sentences may be sufficient. E.g. Information extraction systems only need to identify and classify segments in a text that are likely to contain valuable information
-   Example partial parsing is chunking
-   Chunking is the process of identifying and classifying the flat, non-overlapping segments of a sentence that constitue the basic, non-recursive phrases corresponding to: Noun phrases, verb phrases, adjective phrases and prepositional phrases
    -   Segmenting: Finding the non-overlapping extents of the chunks
    -   Labelling: Assigning the correct tag to the discovered chunks
    -   Chunking can be formulated as a sequence labelling task, with BIO tagging scheme

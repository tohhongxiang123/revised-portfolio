# Dependency Parsing

We now study another important family of formalisms called dependency grammars

- In dependency formalisms, phrasal constituents and phrase-structure rules do not play a direct role.
- The syntactic structure of a sentence is described solely in terms of directed binary grammar relations between the words

Relations among words are illustrated with directed, labelled type arcs from heads to dependents

- This is a typed dependency structure, because teh labels are drawn from a predefined list of gramatical relations
- In untyped dependency structure, the relations are not labelled
- A root node explicitly marks the root of the tree, the head of the entire structure

![](<https://editor.analyticsvidhya.com/uploads/29920Screenshot%20(127).png>)

- The internal structure of dependency parse consists solely of directed relations between lexical items in the sentence
- These head-dependent relationships directly encode important information that is often buried in the more complex phrase-structure parses
  - A dependency grammar approach abstracts away from word order information, representing only the information that is necessary for the parse
  - The head-dependent relations provide an approximation to the semantic relationship between predicates and their arguments
  - Useful for many applications, such as coreference resolution, question answering and information extraction

# Dependency Grammar

The traditional linguistic notion of grammatical relation provides the basis for the binary relations in dependency structure

- The arguments to these relations consist of a head and a dependent
- The head is the word in a phrase that is the most grammatically important
  - Typically, N is the head of NP, V is the head of VP
  - In a noun phrase, various modifiers can occur before or after the head noun
- In dependency grammar, the head-dependent relationship is made explicity by directly linking heads to the words that are immediately dependent on them
- The grammatical relation tells the grammatical function the dependent plays with respect to its head
  - E.g. subject, direct object, indirect object

# The Universal Dependencies Project

- Linguists have developed taxonomies of relations that includes the familiar notions of subject and object
- The UD project provides dependency relations that are linguistically motivated, computationally useful and cross-linguistically applicable
  - The core set of frequently used relations can be broken into 2 sets
  - Clausal relations describe syntactic roles with respect to a predicate (e.g. a verb)
  - Modifier relations categorise the ways that words can modify their heads

# Dependency Formalisms

A dependency structure is a directed graph $G = (V, A)$

- A set of vertices $V$. For simplicity, we consider $V$ corresponds exactly to the set of words in a given sentence
- A set of arcs $A$. Arcs are ordered pairs of vertices, which captures the head-dependent and grammatical function relationships between the elements in $V$

A dependency tree is a directed graph that satisfies the following constraints

- There is a single designated root node that has no incoming arcs
- Other than the root node, each vertex has exactly one incoming arc
- There is a unique path from the root to each vertex in $V$

A dependency tree:

- Each word has a single head
- The dependency structure is connected
- There is a single root node from which one can follow a unique directed path to each of the words in the sentence

# Dependency Treebanks

Dependency treebanks have been created manually or with the help of parsers

- Having human annotators label dependency structures for a given corpus
- Using automatic parsers to provide an initial parse, then hand correct the parses

Dependency treebanks can also be translated from consitutent based treebanks

- Identifying all the head-dependent relations in the consituent based structure
- Identifying the correct dependency relations for these relations

The head is the word in the phrase that is grammatically the most important

- N is the head of NP, V is the head of VP

In a phrase parse tree, heads are passed up; thus each nonterminal in a parse tree is annotated with its lexical head

## From Constituent-based to Dependency Treebanks

Translate constituent-based treebanks to dependency treebanks

- Mark the head child of each node in a phrase structure, e.g. NP -> DT NN, the head child is NN in this rule
- In the dependency structure, make the head of each non-head child depend on the head
- Grammatical relations and function tags in the constituent based parse trees can be used to label edges in the resulting dependency tree
- This method works reasonably well in english. For other languages most dependency treebanks are developed directly using human annotators

## Transition-Based Dependency Parsing

This architecture is based on shift-reduce parsing, a paradigm originally developed for analysing programming languages

- Oracle is a classifier to make one of the tree decisions: LeftArc, RightArc, Shift

## Shift-Reduce Parsing

- The parser walks through the sentence from left to right, successively shifting items from the buffer onto the stack
- At each point, we examine the top 2 elements on the stack, and decide
  - LeftArc: Assert a head (top word)-dependent (second word) relation; remove the second word from the stack
  - RightArc: Assert a head (second word)-dependent (top word) relation; remove the top word from the stack
  - Shift: Remove the word from the front of the input buffer and push it onto the stack

## More on Dependency Parsing

Multiple paths

- There could be multiple shift-reduce sequences that may lead to a reasonable parse
- There may be other transition sequences that lead to different equally valid parses

Oracle (classifier) decisions

- In this example, we assume that the oracle always provides the correct operator at each point in the parse, which is unrealistic
- Incorrect decisions will lead to incorrect parses because the parser has no opportunity to go back and pursue alternative choices
- In this example, we do not consider labels. To produce labelled trees, LeftArc and RightArc operators need to come with dependency labels (e.g. LeftArc(NSUBJ) or RightArc(DOBJ)). This is equivalent to expanding the set of transition operators

There are enhancements for transition-based dependency parsing, and alternative parsing algorithms like Graph-Based Dependency Parsing and neural based models

# Resources

- https://web.stanford.edu/~jurafsky/slp3/8.pdf

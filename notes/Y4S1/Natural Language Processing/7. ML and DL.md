# Machine Learning and Deep Learning

Useful tasks

-   Text classification
-   Information extraction
-   Structured prediction
-   Text generation
-   Question answering
-   Multimodal (Image to text, text to speech)
-   Machine translation
-   Knowledge graph

NLP is the study of formalisms, models and algorithms to allow computers to perform useful tasks involving knowledge about human languages

Machine learning is a set of methods that can automatically detect patterns in data, and then use the uncovered patterns to predict future data, or to perform other kinds of decision making under uncertainty

# Supervised Learning

Classification: To learn a mapping from inputs $\bold{x}$ to outputs $y \in \{1, 2, ..., C \}$ with $C$ being the number of classes ($y$ is categorical)

-   $C = 2$: Binary
-   $C > 2$: Multiclass

Regression: To learn a mapping from inputs $\bold{x}$ to outputs $y \in \mathbb{R}$

# Machine Learning Pipeline

1. Model assumption
2. Model training: Learn the model approximating input-output mapping
3. Model testing: Evaluate model on test data

# Linear Regression

Task: Model the relationship between $\bold{x} \in \mathbb{R}^d$ to $y \in \mathbb{R}$

## Problem Setup

We have training data $\mathcal{D} = \{ (\bold{x}_i, y_i) \}_{i=1}^{N}$

-   $\mathcal{D}$ is the training set
-   $N$ is the number of training examples
-   $x_i \in \mathbb{R}^d$ is a $d$-dimensional vector
-   $y_i \in \mathbb{R}$ is the output

Loss function: Least squares

$$
\argmin_\bold{w} \sum_{i=1}^{N} (y_i - \hat{y_i})^2
$$

where $\hat{y_i} = w_0 + w_1 x_i$ is the predicted outcome for the $i$-th observation

We can write the actual value of the $i$-th observation as:

$$
y_i = w_0 + w_1 x_i + \epsilon_i
$$

where $\epsilon_i$ is the residual (error) we get for the $i$-th observation (the difference between our linear prediction and the true response)

Our objective function is

$$
\begin{aligned}
    J(\theta) &= \argmin_\theta \sum_{i=1}^{N} (y_i - \hat{y_i})^2 \\
    &= (y - \bold{Xw})^T (y - \bold{Xw})
\end{aligned}
$$

We can solve for the minimum point of $J$:

$$
\begin{aligned}
    J(\theta) &= (y - \bold{Xw})^T (y - \bold{Xw}) \\
    &= y^T y - y^T \bold{Xw} - \bold{w}^T \bold{X}^T y + \bold{w}^Y \bold{X}^T \bold{Xw} \\
    \frac{\partial J}{\partial \bold{w}} &= -2\bold{X}^T y + 2 \bold{X}^T \bold{Xw} \\
    &= 0 \\
    \bold{w} &= (\bold{X}^T \bold{X})^{-1} \bold{X}^T y
\end{aligned}
$$

## Probabilistic Formulation

We have seen previously that

$$
y_i = \bold{w}^T \bold{x}_i + \epsilon_i
$$

We often assume that $\epsilon_i$ is normally distributed with mean 0 and variance $\sigma^2$

$$
\epsilon_i \sim \mathcal{N}(0, \sigma^2)
$$

Hence, we can say that

$$
y_i \sim \mathcal{N}(\bold{w}^T \bold{x}_i, \sigma^2)
$$

It is common to assume the training examples are independent and identically distributed (IID). So, we assume all our training examples are normally distributed with mean $\bold{w}^T \bold{x}_i$ and variance $\sigma^2$

## Maximum Likelihood Estimation

We can write linear regression as a model of the form

$$
P(y | \bold{x}, \bold{\theta}) = \mathcal{N}(y | \bold{w}^T \bold{x}, \sigma^2)
$$

We want to find the best weights $\hat{\theta}$ such that

$$
\hat{\theta} \triangleq \argmax_\theta \log p(\mathcal{D} | \theta)
$$

This is the maximum likelihood estimation (MLE)

$$
\begin{aligned}
    \mathcal{l}(\bold{\theta}) &= \log p(\mathcal{D} | \bold{\theta}) \\
    &= \sum_{i=1}^N \log p(y_i | \bold{x}_i, \bold{\theta}) \\
    &= \sum_{i=1}^N \log \left[ \left( \frac{1}{2 \pi \sigma^2} \right)^{\frac{1}{2}} \exp \left( -\frac{1}{2\sigma^2} (y_i - \bold{w}^T \bold{x}_i)^2 \right) \right] \\
    &= \frac{-1}{2\sigma^2} RSS(\bold{w}) - \frac{N}{2} \log (2\pi\sigma^2)
\end{aligned}
$$

where $RSS(\bold{w}) \triangleq \sum_{i=1}^{N} (y_i - \bold{w}^T \bold{x}_i)^2$ is the residual sum of squares

# Logistic Regression

Find a mapping between $\bold{x} \in \mathbb{R}^d$ to $y \in \{1, ... C\}$

## Problem Setup

We have training data $\mathcal{D} = \{ (\bold{x}_i, y_i) \}_{i=1}^{N}$

-   $\mathcal{D}$ is the training set
-   $N$ is the number of training examples
-   $x_i \in \mathbb{R}^d$ is a $d$-dimensional vector
-   $y_i \in \{1, ... C\}$ is the output (Categorical)

Let us consider a binary response variable $y \in \{0, 1\}$. Cases where response variable has more than 2 outcome categories may be analysed in **multinomial logistic regression**

We cannot use linear regression for binary outcomes

-   Relationship between $\bold{x}$ and $y$ is not linear
-   $y$ is not normally distributed
-   Linear models cannot constrain $y$ to be 0 or 1

We are interested in $p(y_i = 1 | \bold{x}_i, \bold{w}) \in [0, 1]$

-   We replace normal distribution with bernoulli distribution: $y \sim Bernoulli(p) \in \{0, 1\}$

We can use a sigmoid function

$$
sigm(\eta) \triangleq \frac{1}{1 + \exp(-\eta)}
$$

Now, we can define

$$
\begin{aligned}
    P(y | \bold{x}, \bold{w}) &= Bernoulli(y | sigm(\bold{w}^T \bold{x})) \\
    p(y_i = 1 | \bold{x}_i, \bold{w}) &= sigm(\bold{w}^T \bold{x}_i) \\
    p(y_i = 0 | \bold{x}_i, \bold{w}) &= 1 - sigm(\bold{w}^T \bold{x}_i) \\
    \therefore p(y_i | \bold{x_i}, \bold{w}) &= sigm(\bold{w}^T \bold{x}_i)^{y_i} \cdot (1 - sigm(\bold{w}^T \bold{x}_i))^{1 - y_i}
\end{aligned}
$$

MLE:

$$
\begin{aligned}
    \bold{w}^* &= \argmax_\bold{w} \log p(\mathcal{D} | \bold{w}) \\
    &= \argmax_\bold{w} \sum_{i=1}^{N} \log p(y_i | \bold{x}_i, \bold{w}) \\
    &= \argmax_\bold{w} \sum_{i=1}^{N} \log (\mu_i^{y_i} (1 - \mu_i)^{1 - y_i}) \\
    &= \argmin_\bold{w} - \sum_{i=1}^{N} (y_i \log \mu_i + (1 - y_i)\log(1 - \mu_i))
\end{aligned}
$$

Note that:

-   $\mu_i = sigm(\bold{w}^T \bold{x}_i)$
-   The last step converts into negative log likelihood.
-   The final equation is called the cross entropy error function

Unlike linear regression, we can no longer write the MLE in closed form. We use an optimisation algorithm to compute it instead

For this, we use the gradient and the hessian

$$
\begin{aligned}
    \bold{g} &= \frac{d f(\bold{w})}{d\bold{w}} = -\sum_{i} (\mu_i - y_i) \bold{x}_i = \bold{X}^T (\mu - y) \\
    \bold{H} &= \frac{d \bold{g}(\bold{w})^T}{d \bold{w}} = -\sum_i \nabla_\bold{w} \mu_i \bold{x}_i^T = - \sum_i \mu_i (1 - \mu_i) \bold{x}_i \bold{x}_i^T = \bold{X}^T \bold{S} \bold{X}
\end{aligned}
$$

-   $\bold{S} \triangleq diag(\mu_i (1 - \mu_i))$
-   From neural networks course, recall gradient descent for logistic regression ($\nabla_\bold{W} J = \bold{X}^T (\bold{d} - f(\bold{u}))$)
-   One can show that $\bold{H}$ is positive definite, hence negative log likelihood is convex, and has a unique global minimum

## Multiclass Logistic Regression

We extend logistic regression to multi-class logistic regression, to handle more than 2 classes

For multinomial logistic regression, we model the probability using softmax:

$$
\begin{aligned}
P(y_i = c | \bold{x}_i, \bold{W}) &= \mu_{ic} = \frac{\exp(\bold{w}_c^T \bold{x}_i)}{\sum_{c'=1}^{C} \exp (\bold{w}_{c'}^{T} \bold{x}_i)} \\
P(y_i | \bold{x}_i, \bold{W}) &= \prod_{c=1}^{C} \mu_{ic}^{y_{ic}}
\end{aligned}
$$

-   Note that we can multiply the probabilities because of the IID assumption
-   $\bold{W} = [\bold{w}_1, ..., \bold{w}_k] \in \mathbb{R}^{d \times k}$

Then we solve for the best weights:

$$
\begin{aligned}
    \bold{W}^* &= \argmax_\bold{W} \log p(\mathcal{D} | \bold{W}) \\
    &= \argmax_\bold{W} \sum_{i=1}^{N} \log p(y_i | \bold{x}_i, \bold{W}) \\
    &= \argmax_\bold{W} \sum_{i=1}^{N} \log \prod_{c=1}^{C} \mu_{ic}^{y_{ic}} \\
    &= \argmax_\bold{W} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{ic} \log \mu_{ic} \\
    &= \argmax_\bold{W} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{ic} \log \left[ \frac{\exp(\bold{w}_c^T \bold{x}_i)}{\sum_{c'=1}^{C} \exp(\bold{w}_{c'}^T \bold{x}_i)} \right] \\
    &= \argmax_\bold{W} \sum_{i=1}^{N} \left[ \left(\sum_{c=1}^{C} y_{ic} \bold{w}_c^T \bold{x}_i \right) - \log \left( \exp(\bold{w}_{c'}^T \bold{x}_i) \right) \right]
\end{aligned}
$$

Note:

-   $y_{ic} = \mathbb{I}(y_i = c)$
-   $\sum_{c=1}^{C} y_{ic} \bold{w}_c^T \bold{x}_i$ has only one term (when $y_i = c$, and 0 elsewhere). This can be thought of as "hope" (Log-probability of $i$-th example belonging to class $c$)
-   $\sum_{c=1}^{C} y_{ic} \log (\sum_{c'=1}^C \exp(\bold{w}_{c'}^T \bold{x}_i)) = \log (\sum_{c'=1}^C \exp(\bold{w}_{c'}^T \bold{x}_i))$ because only 1 of the $y_{ic}$ is 1, the rest is 0. This can be thought of as "fear" (Log-probability of the $i$-th example not belonging to class $c$)

# Gradient Descent

The most commonly used method for unconstraine3d optimisation. Goal is to minimise $\sum_{i=1}^{N} J_i (\bold{\theta})$ with respect to $\bold{\theta}$

**Stochastic gradient descent** (SGD) updates parameters after every single data point

-   For timestep $t \in \{1, ..., T\}$
    -   Choose a random permutation $\pi$ of $\{1, ..., N\}$ (Shuffle training data)
    -   For $i \in \{1, ..., N\}$:
        -   $\bold{\theta} \leftarrow \bold{\theta} - \eta \cdot \nabla_\bold{\theta} J_{\pi(i)}$ (Iterate through shuffled training data, and update weights for each example)

**Batch Gradient Descent** uses the entire dataset to compute a single gradient update

-   For timestep $t \in \{1, ..., T \}$:
    -   $\bold{\theta} \leftarrow \bold{\theta} - \eta \cdot \nabla_\bold{\theta} \frac{1}{N} \sum_{i=1}^{N} J_i$ (Take the whole training set, calculate the average loss, and update weights based on the average loss)

**Mini-batch Gradient Descent** updates parameters after every batch of data points

-   For timestep $t \in \{1, ..., T\}$:
    -   Choose a batch of size $S$: $\{x_{b_1}, ..., x_{b_S}\}$
    -   Do batch gradient descent on this particular batch
    -   $\bold{\theta} \leftarrow \bold{\theta} - \eta \cdot \nabla_\bold{\theta} \frac{1}{S} \sum_{i = b_1}^{b_S} J_i$

Comparison between different methods of gradient descent

| Method     | Accuracy              | Update speed | Memory Usage | Online Learning |
| ---------- | --------------------- | ------------ | ------------ | --------------- |
| Batch      | Good                  | Slow         | High         | No              |
| Stochastic | Good (with annealing) | High         | Low          | Yes             |
| Mini-batch | Good                  | Medium       | Medium       | Yes             |

-   [Explanation of annealing](https://macromoltek.medium.com/machine-learning-and-simulated-annealing-588b2e70d0cc)
-   Tl;dr: Start by accepting bad solutions, but as solution space is explored, lower the probability of accepting worse solutions. This balances exploration vs exploitation
-   In the beginning, we want to explore as much as possible
-   As we explore more of the solution space, we want to exploit the good solutions instead

Main issue with GD: What is the step size $\eta$?

-   Too small: Slow convergence
-   Too large: Fail to converge

There are many variants of GD:

-   Momentum
-   Nesterov accelerated gradient
-   Adagrad
-   Adadelta
-   RMSprop
-   Adam
-   Adam extensions

# Deep Learning

Deep learning is a subfield of machine learning.

-   Most machine learning methods work well because of human-designed representations/input features
-   Machine learning becomes just optimising feature weights to make a good prediction

Deep learning algorithms attempt to learn multipole levels of representations and an output from raw inputs $\bold{x}$

-   Manually designed features are often over-specified, incomplete, and take a long time to design/validate
-   Learned features are easy to adapt and fast to learn
-   Deep learning provides a flexible learnable framework for representing world, visual and linguistic information
-   Deep learning can learn in both unsupervised and supervised settings

## From Logistic Regression to Neural Network

A neural network is just running several logistic regressions at the same time

-   If we feed a vector of inputs through a bunch of logistic regression functions, we get a vector of outputs which we can feed into another logistic regression function
-   Output layer can be a classification/regression layer

$$
\begin{aligned}
z &= Wx + b \\
a &= f(z)
\end{aligned}
$$

-   Note that $f$ is applied element-wise

Neural networks require non-linearity

-   Without non-linearity, NN can only do a linear transform
-   Extra layers can be compiled into a single linear transform

Activation functions

-   Sigmoid
-   Hyperbolic tangent (tanh)
-   Rectified Linear Unit (ReLU)
-   Leaky ReLU

To train neural networks

-   We forward propagate inputs to produce an output
-   We calculate the error ($J(d, y)$, $d$ is the target, $y$ is the prediction)
-   We backpropagate error derivatives through the model

Backpropagation is discussed in Neural Networks course

## Dropout

Simple but powerful regulariser

-   Randomly turnoff some nodes during training
-   Use all activations during inference
-   Dropout has the effect of making training data noisy, forcing nodes within a layer to probabilistically take on more or less responsibility for the inputs
-   Dropout encourages the network to learn a sparse representation
-   Breaks-up situations where network layers co-adapt to correct mistakes from prior layers, in turn making the model more robust

## L1/2 Regularisation

If a particular feature $f_j$ is usually positive, then it always improves the loss to increase $\theta_j$

Regularisation: Discourage every $\theta_j$ from getting too large in magnitude

$$
\argmin_\bold{\theta} J(\bold{\theta}) + \lVert \bold{\theta} \rVert_p^p
$$

where $\lambda > 0$ is a hyperparameter, and $p = 1$ or $p = 2$

$$
\begin{aligned}
    \lVert \bold{\theta} \rVert_1 &= \sum_{j = 1}^{d} | \theta_j | \\
    \lVert \bold{\theta} \rVert_2 &= \sqrt{\sum_{j = 1}^{d} \theta_j^2}
\end{aligned}
$$

Note that $L_1$ regularisation encourages sparsity (Explanation [here](https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models))

-   Tl;dr: $L_2$ regularisation causes larger weights to be decreased more, while for $L_1$, decreasing any weight has equal effect. Hence, $L_2$ regularisation makes it less likely that the weights reach 0 compared to $L_1$

## Gradient Clipping

If the gradient becomes too big, then the SGD update step becomes too big

-   This can cause bad updates: We take too large a step and reach a bad parameter configuration (with large loss)
-   In the worst case, this results in Inf or NaN in your network (then you will have to restart training from an earlier checkpoint)

Gradient clipping: If the norm of the gradient is greater than some threshold, we scale the gradient down before applying SGD

```
g = dE/dW
if |g| >= threshold:
    g = threshold / |g| * g
```

Intuition: Take a step in the same direction, but smaller

![](https://miro.medium.com/v2/resize:fit:1400/1*vLFINWklJ0BtYtgzwK223g.png)

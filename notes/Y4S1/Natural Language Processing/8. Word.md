# Word

Word meaning: The idea that is represented by a word, phrase etc.

In a computer, we can use WordNet: A thesaurus containing lists of synonym sets and hypernyms (is-a relationship)

Problems with WordNet

-   Great as a lexical resource, but missing nuance: E.g. proficient is a synonym for good, but this is correct only in some contexts
-   Missing new meanings of words: Wicked, genius, etc
-   Impossible to keep up-to-date, requires human labor to create and adapt
-   Subjective

Problems with discrete representation

-   Hard to compute accurate word similarity
-   In traditional NLP, we regard words as discrete symbols
    -   E.g. motel = [0, 0, 1], hotel: [1, 0, 0]
    -   These 2 vectors are orthogonal

Representing words by context

-   Learn to encode similarity in the vectors themselves
-   Distributional semantics: A word's meaning is given by the words that frequently appear close-by
-   When a word $w$ appears in a text, its context is the set of words that appear nearby (within a fixed-size window)
-   Idea: Use the many contexts of $w$ to build up a representation of $w$

Word Vectors

-   Build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts
-   Word vectors are sometimes called word embeddings or word representations. They are a distributed representation

# Word2Vec

W2V is a framework for learning word vectors. The ideas includes

-   We have a large corpus of text: A long list of words
-   Every word in a fixed vocabulary is represented by a vector
-   Go through each position $t$ in the text, which has a center word $c$ and context (outside) words $o$
-   Use similarity of word vectors for $c$ and $o$ to calculate the probability of $o$ given $c$ or vice versa
-   Keep adjusting vectors to maximise probability

Skipgram

-   For a window size of 2:
    -   We have a center word at position $t$
    -   We can find $P(w_{t-2} | w_t), P(w_{t-1} | w_t), P(w_{t+1} | w_t), P(w_{t+2} | w_t)$

For each position $t = 1, ..., T$, predict context words within a window of fixed size $m$, given center word $w_t$. Data likelihood:

$$
L(\theta) = \prod_{t=1}^{T} \prod_{\substack{-m \leq j \leq m \\ j \neq 0}} P(w_{t + j} | w_t; \theta)
$$

-   $\theta$ is all variables to be optimised

The objective function $J(\theta)$ is the average negative log likelihood

$$
J(\theta) = -\frac{1}{T} \log L(\theta) = -\frac{1}{T} \sum_{t=1}^{T} \sum_{\substack{-m \leq j \leq m \\ j \neq 0}} P(w_{t + j} | w_t; \theta)
$$

Minimising objective function = Maximising predictive accuracy

To calculate $P(w_{t + j} | w_t; \theta)$, we use 2 vectors per word $w$:

1. $v_w$ when $w$ is a center word
2. $u_w$ when $w$ is a context word

For a center word $c$ and a context word $o$:

$$
P(o | c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V} \exp(u_w^T v_c)}
$$

-   Dot product compares similarity between $o$ and $c$
-   Sum to normalise over entire vocab to give probability distribution
-   This is the softmax function

To learn, we update parameters $\theta$:

$$
\theta \leftarrow \theta - \alpha \nabla_\theta J(\theta)
$$

We changed the problem from: "Predict neighboring word" to "Are teh 2 words neighbors?"

# SkipGram with Negative Sampling

Maximise probability that real outside word appears (co-occur), minimise probability that random words appear around center word

$$
J_t(\theta) = \log \sigma(u_o^T v_c) + \sum_{i=1}^{k} \log \sigma(-u_{o_i}^T v_c)
$$

-   $\sigma(u_o^T v_c)$ is the probability that the real outside word appears
-   $\sum_{i=1}^{k} \log \sigma(-u_{o_i}^T v_c) = \sum_{i=1}^{k} \log (1 - \sigma(u_{o_i}^T v_c))$ is the probability that random words do not appear

## Skipgram Training

-   Create 2 matrices: An embedding matrix and a context matrix. Initialise these matrices with random values
-   In each training step, take one positive example, and its corresponding negative example
-   Look up embeddings: For the input word, we look in the embedding matrix. For context words, we look in the context matrix
-   Compute predictions, loss, gradients and model updates

There are 2 hyperparameters to find:

-   Window size
-   Number of negative samples

# Intrinsic Evaluation of Word Vectors

-   Word vectors should capture "relational similarities"

Consider the sentence:

-   Man is to woman as king is to queen
-   $a$ is to $a^*$ as $b$ is to $b^*$
-   $a - a^* = b - b^*$
-   $b - a + a^* = b^*$
-   We wish to find $\argmax_{b^*} \cos(b^*, b - a + a^*)$

We have word analogy datasets

-   Word vector distances and their correlation with human judgments
-   Example: [WordSim353](http://alfonseca.org/eng/research/wordsim353.html)

# Extrinsic Evaluation of Word Vectors

Word vectors are crucial in many intermediate and end tasks

-   Intermediate tasks
    -   Named Entity Recognition
    -   Parsing
    -   Cross-lingual embeddings
-   End applicaitons
    -   Machine translation
    -   Sentiment analysis
    -   Summarisation

# Language Modelling

A language model takes in a list of words (history/context) and attempts to predict the words that follow them

Given a sequence of words $x^{(1)}, ..., x^{(t)}$, compute the probability distribution of the next word $x^{(t + 1)}$

$$
P(x^{(t + 1)} | x^{(t)}, ..., x^{(1)})
$$

where $x^{(t + 1)}$ can be any word in the vocabulary $V$

## Importance of Language Modelling

-   A benchmark task to track our progress on understanding language
-   Important component of many NLP tasks, especially those involving generating text or esetimating the probabilitt of a text

## Evaluating Language Models

The standard evaluation metric for Language Models is perplexity

$$
PP = \prod_{t=1}^{T} \left( \frac{1}{P_{LM}(x^{(t+1)} | x^{(t)}, ..., x^{(1)})} \right)^{\frac{1}{T}}
$$

This is equal to the exponential of the cross entropy loss $J(\theta)$:

$$
= \prod_{t=1}^{T} \left(\frac{1}{\hat{y}_{x_{t + 1}}^{(t)}}\right)^{\frac{1}{T}} = \exp \left( \frac{1}{T} \sum_{t=1}^{T} -\log \hat{y}_{x_{t + 1}}^{(t)} \right)
$$

Lower perplexity is better

# N-Gram Language Model

An N-gram is a chunk of $n$ consecutive words

-   Idea: Collect statistics about how frequent different n-grams are, and use these to predict the next word

Make a markov assumption: $x^{(t + 1)}$ depends on only the preceding $n - 1$ words

-   $P(x_{t + 1} | x_t, ..., x_1) = P(x_{t + 1} | x_t, ..., x_{t - n + 2})$
-   How to get these n-gram probabilities? Count them in some large corpus of text
-   $= C(x_{t+1}, ..., x_{t-n+2}) / C(x_t, ..., x_{t-n+2})$

## Sparsity Problem

What if the n-gram never occurred in the data?

-   Smoothing

Increasing $n$ worsens sparsity problem, and increases model size

## Generating Text with N-Gram LM

Pick the word with the highest probability

-   Results in grammatical but incoherent sentences
-   We could increase $n$, but that worsens the sparsity problem and increases model size

# Neural Language Model

Recall a fixed-window NER model, which predicts the class for a specific word

-   Now, instead of predicting the class for the center word, predict the next word
-   No sparsity problem
-   Dont need to store all n-grams
-   However, fixed window is too small
-   Enlarging window enlarges weights of NN
-   Window can never be large enough
-   No symmetry in how inputs are processed

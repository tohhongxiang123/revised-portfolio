# Sequence Modelling

Recall that a language model takes in a list of words (history/context) and predicts the word that follows them:

$$
P(\bold{x}^{(t + 1)} \ | \ \bold{x}^{(t)}, ..., \bold{x}^{(1)})
$$

- $\bold{x}^{(t)}, ..., \bold{x}^{(1)}$ is a sequence of words
- $\bold{x}^{(t + 1)}$ can be any word in the vocabulary $V$

Improvements over N-gram LM:

- No sparsity problem
- Don't need to store all observed n-grams

Remaining problems:

- Fixed window too small
- Enlarging window enlarges $W$, but window can never be large enough
- Words are multiplied by completely different weights in $W$, no symmetry in how inputs are processed

# Recurrent Neural Networks

Main idea: Apply the same weights repeatedly (recurrently)

![](https://media.geeksforgeeks.org/wp-content/uploads/20230518134831/What-is-Recurrent-Neural-Network.webp)

Given some sentence $\bold{x}^{(0)}, ..., \bold{x}^{(t)}$, we feed in words starting from $\bold{x}^{(0)}$

Word embeddings:

$$
\bold{e}^{(t)} = \bold{E} \bold{x}^{(t)}
$$

- $\bold{E} \in \mathbb{R}^{d \times |V|}$ is the entire embedding matrix
- $\bold{e}^{(t)} \in \mathbb{R}^{d}$ is the embedding corresponding to the $t$-th word
- $\bold{x}^{(t)} \in \mathbb{R}^{|V|}$ is a one-hot vector of the $t$-th word

Hidden state:

$$
\bold{h}^{(t)} = \sigma(\bold{W}_h \bold{h}^{(t - 1)} + \bold{W}_e \bold{e}^{(t)} + b_1)
$$

- $\bold{h}^{(t)}$ is the hidden state at timestep $t$

Output distribution

$$
\hat{\bold{y}}^{(t)} = \text{softmax}(\bold{Uh}^{(t)} + \bold{b}_2) \in \mathbb{R}^{|V|}
$$

- The probability of every word in the vocabulary appearing after the history

## Training RNN

1. Get a big corpus of text, which is a sequence of words $\bold{x}^{(1)}, ..., \bold{x}^{(T)}$
2. Feed into RNN, compute output distribution $\hat{\bold{y}}^{(t)}$ for every step $t$ (Predict probability distribution of every word, given words so far)
3. Loss function on step $t$ is cross-entropy between predicted probability distribution $\hat{\bold{y}}^{(t)}$, and the true next word $\bold{y}^{(t)}$ (which is a one-hot for $\bold{x}^{(t + 1)}$)

   $$
       J^{(t)}(\theta) = CE(\bold{y}^{(t)}, \hat{\bold{y}}^{(t)}) = - \sum_{w \in V} \bold{y}_w^{(t)} \log \hat{\bold{y}}_w^{(t)} = - \log \hat{\bold{y}}_{x_{t+1}}^{(t)}
   $$

4. Average this to get overall loss for entire training set

   $$
       J(\theta) = \frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)
   $$

However, computing loss and gradients across entire corpus is too expensive memory-wise

- In practice, we consider $\bold{x}^{(1)}, ..., \bold{x}^{(t)}$ as a sentence/document
- In mini-batch, we compute loss/gradients for batches of sentences/documents, and update the losses
- The gradient wrt a repeated weight, is the sum of the gradient wrt each time it appears

  $$
      \frac{\partial J^{(t)}}{\partial \bold{W}_h} = \sum_{i = 1}^{t} \frac{\partial J^{(t)}}{\partial \bold{W}_h} \Bigr|_{(i)}
  $$

- Then, backpropagate over timesteps $i = t, ..., 0$, summing gradients as you go. This algorithm is called "backpropagation through time"

## Problems with RNN: Vanishing Gradients

$$
\frac{\partial J^{(4)}}{\partial \bold{h}^{(1)}} = \frac{\partial \bold{h}^{(2)}}{\partial \bold{h}^{(1)}} \frac{\partial \bold{h}^{(3)}}{\partial \bold{h}^{(2)}} \frac{\partial \bold{h}^{(4)}}{\partial \bold{h}^{(3)}} \frac{\partial J^{(4)}}{\partial \bold{h}^{(4)}}
$$

- When each of the partial derivatives are small, the gradient signal gets smaller and smaller as it backpropagates further
- Model weights are only updated with respect to near effects, not long-term effects
- Due to vanishing gradients, RNNs are better at learning from sequential recency than syntactic recency

Advantages of RNN

- Can process any length input. Computation for step $t$ can (in theory) use information from many steps back
- Model size does not increase for longer input context
- Same weights applied every timestep, so there is symmetry in how inputs are processed

Disadvantages of RNN

- Recurrent computation is slow
- In practice, difficult to access information from many steps back

# Tackling Vanishing Gradients

Main problem: Too difficult for vanilla RNNs to learn to preserve information over many timesteps. Same hidden state is constantly updated

## Long Short-Term Memory (LSTM)

![](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)

At timestep $t$, there is a hidden state $\bold{h}^{(t)}$ and cell state $\bold{c}^{(t)}$

- Both are vectors of length $n$
- Cell stores long-term info
- LSTM can read/erase/write information from the cell

Selection of which information is erased/written/read is controlled by 3 corresponding gates

- Gates are also vectors of length $n$
- At each timestep, each element of the gates can be open (1), closed (0), or somewhere in between
- Gates are dynamic: Value is computed based on current context

Given current input and previous hidden states, compute the following gates:

- Forget gate: Controls what is kept vs forgotten, from previous cell states: $\bold{f}^{(t)} = \sigma (\bold{W}_f \bold{h}^{(t - 1)} + \bold{U}_f \bold{x}^{(t)} + \bold{b}_f)$
- Input gate: Controls what parts of the new cell content are written to the cell: $\bold{i}^{(t)} = \sigma (\bold{W}_i \bold{h}^{(t - 1)} + \bold{U}_i \bold{x}^{(t)} + \bold{b}_i)$
- Output gate: Controls what parts of the cell are outputted to the hidden state: $\bold{o}^{(t)} = \sigma (\bold{W}_o \bold{h}^{(t - 1)} + \bold{U}_o \bold{x}^{(t)} + \bold{b}_o)$

Now, compute how much to forget, update and output

- New cell content: New content written to the cell: $\tilde{\bold{c}}^{(t)} = \tanh (\bold{W}_c \bold{h}^{(t-1)} + \bold{U}_c \bold{x}^{(t)} + \bold{b}_c)$
- Cell state: Erase (forget) some content from last cell state, and write (input) some new cell content: $\bold{c}^{(t)} = \bold{f}^{(t)} \circ \bold{c}^{(t - 1)} + \bold{i}^{(t)} \circ \tilde{\bold{c}}^{(t)}$
  - Note that $a \circ b$ is an elementwise product of $a$ and $b$
- Hidden state: Read (output) some content from the cell: $\bold{h}^{(t)} = \bold{o}^{(t)} \circ \tanh \bold{c}^{(t)}$
  - Note that all these output vectors have the same length

### Why LSTMs Work

LSTM architecture makes it easier for RNN to preserve information over many time steps

- If forget gate is set to remember everythinhg every timestep, the info in the cell is preserved indefinitely
- In practice, LSTMs use adaptive gates, which are also learned (using their respective parameters)
- LSTMs however do not guarantee that there is no vanishing/exploding gradient, but provides an easier way for model to learn long-distance dependencies

## Residual Connections

![](https://production-media.paperswithcode.com/methods/resnet-e1548261477164.png)

Vanishing/exploding gradients are not specific to RNNs

- Can happen in all deep neural architectures due to chain rule and/or choice of nonlinearity function
- Thus lower layers are learnt very slowly
- Solution: Lots of new deep feedforward/convolutional architectures add more direct connections (thus allowing gradient to flow)
  - Residual connections aka ResNet
  - Also konwn as skip-connections
  - Identity connection preserves information by default
  - Makes deep networks easier to train

# Sequence Modelling

- POS/NER tagging
- Sentiment classification

## Bidirectional RNN

![](https://media.geeksforgeeks.org/wp-content/uploads/20230302163012/Bidirectional-Recurrent-Neural-Network-2.png)

$$
\begin{aligned}
    \overrightarrow{\bold{h}}^{(t)} &= RNN_{FW}(\overrightarrow{\bold{h}}^{(t - 1)}, \bold{x}^{(t)}) \\
    \overleftarrow{\bold{h}}^{(t)} &= RNN_{FW}(\overleftarrow{\bold{h}}^{(t - 1)}, \bold{x}^{(t)}) \\
    \bold{h}^{(t)} &= [\overrightarrow{\bold{h}}^{(t)}; \overleftarrow{\bold{h}}^{(t)}]
\end{aligned}
$$

- $\overrightarrow{\bold{h}}^{(t)}$ is the hidden state for the forward RNN, which requires the previous forward hidden state $\overrightarrow{\bold{h}}^{(t - 1)}$ and the current timestep input $\bold{x}^{(t)}$
- Similarly for the backward RNN
- Then, the hidden states are concatenated to form $\bold{h}^{(t)}$
- Note that the forward RNN and the backward RNN have separate weights

Bidirectional RNNs are only applicable when you have access to the entire input sequence. They are not applicable to language modelling, as future tokens are not accessible

- If you do have the entire input sequence, bidirectionality is powerful for encoding (should be used by default)
- E.g. BERT (Bidirectional Encoder Representations from Transformers) is a powerful pretrained contextual representation system, built on bidirectionality

## Stacked RNNs

![](https://www.mdpi.com/entropy/entropy-25-00520/article_deploy/html/images/entropy-25-00520-g002.png)

RNNs are already "deep" in one dimension (time)

- We can also make them "deep" in another dimension, by adding another RNN on top of the existing one: This is a multilayer RNN
- This allows network to compute more complex representations
  - Lower RNNs should compute lower-level features, higher RNNs should compute higher-level features
- Multi-layer RNNs are known as stacked RNNs

Higher-performing RNNs are often multi-layered (but not as deep as CNNs or feed-forward networks)

- However, skip-connections/dense-connections are needed to train deeper RNNs (e.g. 8 layers)
- Transformer-based networks such as BERT can be up to 24 layers

# Seq2seq Applications

- Machine translation
- Summarisation
- Dialogue generation

## Machine Translation

Machines that can translate text from one language to another, classic test of language understanding

Challenges of MT:

- Different structures for different languages
  - Morphology
  - Syntax
  - Semantics
  - Pragmatics and discourse

Morphological differences

- Morpheme: Minimal meaningful unit of a language
- A word is made up of multiple morphemes. E.g. read + ing = reading
- Number of morphemes per word varies with language

Lexical differences

- Word boundaries
  - Boundaries between words are not marked in some languages (Chinese, Japanese, Thai)
- Lexical gaps
  - One-to-one mapping between lexical items are rate
  - Chinese have no term for "brother", "sister", "aunt", etc
- Mapping of words to phrase
  - Computer science -> Informatique (French)

Syntactic differences

- Word order differences
  - English/german/french/mandarin: Subject-verb-object
  - Hindi/Japanese: Subject-object-verb
  - Irish, Classical Arabic: Verb-subject-object

Discourse and Pragmatic differences

- Use of pronouns, discourse markers, grammatical politeness
  - Cold language: References to actors are not explicit (e.g. chinese, japanese)
  - Hot language: References to actors are explicit (e.g. english)

### Early Machine Translation

Early translation used rule-based systems, using a bilingual dictionary to map words (Russian to english, motivated by the cold war)

Problems with rule-based systems

- Writing rules requires a lot of human effort and knowledge
- We should not undermine the importance of rules
- Statistical/Neural MT relies on huge bilingual corpora, which may not be available for restricted domains (dialects, instructions)

### Statistical Machine Translation

Parallel corpora (aka bilingual, bitext) are available in several language pairs

- Contains the same text in 2 or more languages
- Basic idea: Use parallel corpus as training set of translation examples, and learn a translation model

E.g., given a foreign (French) sentence $F$, find an English sentence $E$

$$
\begin{aligned}
    \hat{E} &= \argmax_{E \in English} P(E | F) \\
    &= \argmax_{E \in English} \frac{P(F | E) P(E)}{P(F)} \\
    &= \argmax_{E \in English} P(F | E) P(E)
\end{aligned}
$$

- Decoder (Beam search) used to find max
- $P(F | E)$ represents our translation model (faithful)
  - Models how words and phrases should be translated; Learnt from parallel data
- $P(E)$ represents our language model (fluency)
  - Models how to write good english, learn from monolingual data

#### Fluency Modelling $P(E)$

We need a metric that ranks the sentence "That car almost crash to me" as less fluent as "That car almost hit me"

- Answer: Language models: $P(me | almost, hit) > P(to | almost, crash)$
- Advantage: This is monolingual knowledge

#### Faithfulness Modelling $P(F|E)$

More faithful translations will be composed of phrases that are high probability translations

- How often was "slapped" translated as "dio una bofetada" in a large bitext (parallel English-Spanish corpus)

Goal to compute $P(F|E)$ from a bitext $(E, F)$ corpus. We have 3 options

1. Consider sentence pairs $(E, F)$ to compute $P(F|E)$ (Sparse)
2. Consider word pairs $(e_i, f_j)$ of the sentences, and then take conditional independence assumption to compute $P(F|E)$ (Word alignment, word-based machine translation)
3. Consider phrase-pairs $(\bold{e}_i, \bold{f}_j)$ to compute $P(F|E)$ (Phrasal alignment, phrase-based machine translation)

### Neural Machine Translation

Machine translation with a single neural network. 2 models are put together

1. Encoder
2. Decoder

This neural architecture is called sequence-to-sequence (seq2seq)

- Distributed representation of words and phrases
- End-to-end training
- Allows larger context (even extra sentential)

![](https://www.guru99.com/images/1/111318_0848_seq2seqSequ1.png)

- Encoder produces an encoding of the source sentence
- Provides initial hidden state for decoder
- Decoder is a conditional language model, that generates the target sentence conditioned on the encoding
- Seq2seq directly models the conditional probability $P(y | x)$ of translating a source sequence $x_1, ..., x_n$ to $y_1, ..., y_m$

Consider the translation from an english sentence $x$ to a german sentence $y$

$$
\begin{aligned}
    x &= \text{He loved to eat} \\
    y &= \text{Er liebte zu essen} \\
    p(y | x)  &= \sum_{j=1}^{m} \log p(y_j | y_{< j}, \bold{s}) \\
    h_j &= RNN(h_{j-1}, y_{j - 1}) \\
    p(y_j | y_{< j}, \bold{s}) &= softmax(g(h_j))
\end{aligned}
$$

Training objective:

$$
J_t = \sum_{(x, y) \in \mathcal{D}} -\log p(y | x)
$$

For decoding,

- Generate (or decode) the target sentence by taking the argmax on each step of the decoder
- This is greedy decoding (take the most probable word on each step)

#### Evaluate NMT

BLEU (Bilingual Evaluation Understudy)

- BLEU compares machine-written translation to one or several human-written translations, and computes a similarity score based on
  - n-gram precision (usually for 1, 2, 3, 4-grams)
  - Plus penalty for too-short system translations
- BLEU is useful but imperfect
  - Many valid ways to translate a sentence
  - A good translation can get a poor BLEU score because it has low n-gram overlap with human translation

N-gram precision: number of correct n-grams / number of total predicted n-grams

$$
GAP = \prod_{i=1}^{N} p_i^{w_i}
$$

- $p_i$: Probability of $i$-th n-gram appearing
- $w_i$: 1 / Length of $i$-th n-gram
- Over all possible n-grams

#### Brevity Penalty

Brevity penalty penalises sentences that are too short

$$
P = \begin{cases}
    1 & c > r \\
    e^{(1 - r / c)} & c \leq r
\end{cases}
$$

- $c$ is the predicted length: Number of words in the predicted sentence
- $r$ is the target length: Number of words in the sentence

BLEU = Brevity penalty \* GAP

#### Advantages of NMT

Better performance

- More fluent
- Better use of context
- Better use of phrase similarities

A single neural network to be optimised end-to-end

- No subcomponents to be individually optimised

Requires much less human engineering effort

- No feature engineering
- Same method for all language pairs

#### Disadvantages of NMT

Compared to SMT:

- NMT is less interpretable (hard to debug)
- NMT is difficult to control (Cannot specify rules or guidelines for translation, safety concerns)

### Seq2Seq Usage

One model for almost all NLP tasks

- Summarisation (long text -> short text)
- Dialogue (previous utterances -> next utterance)
- Parsing (input text -> output parse tree)
- Code generation (natural language -> python code)
- Segmentation/tagging (input text -> output tag sequence)

# Resources

- https://colah.github.io/posts/2015-08-Understanding-LSTMs/
- https://desh2608.github.io/2017-09-16-metrics-for-nlg-evaluation/

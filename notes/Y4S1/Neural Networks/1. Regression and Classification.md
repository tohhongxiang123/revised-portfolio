# Regression

Primarily, neural networks are used to predict outputs labels from input features. Prediction tasks can be labelled into 2 categories:

1. Regression: Labels are continuous (Predicting age, weight, height etc)
2. Classification: Labels are discrete (Predicting gender, digits, flower types etc)

Training finds network weights and biases that are optimal for the prediction of labels from features

# Linear Neuron

The synaptic input $u$ to a neuron is given by

$$
u = w^T x + b
$$

- $w = [w_1, ..., w_n]^T$ is the weights vector
- $x = [x_1, ..., x_n]^T$ is the inputs from the previous neurons
- $b$ is the bias

A linear neuron has a linear activation function

$$
y = f(u) = u
$$

## Linear Neurons Perform Linear Regression

Representing a dependent (output) variable as a linear combination of independent (input) variables is known as **linear regression**

The output of a linear neuron can be written as

$$
y = w_1 x_1 + ... + w_n x_n + b = \left( \sum_{i = 1}^{n} w_i x_i \right) + b
$$

- The weights and biases act as regression coefficients

Given training examples $\{\bold{x}_p, d_p\}_{p = 1}^{P}$ where $\bold{x}_p \in \mathbb{R}^n$ and target $d_p \in \mathbb{R}$, training a linear neuron finds a linear mapping $\phi: \mathbb{R}^n \to \mathbb{R}$ given by

$$
y = \bold{w}^T \bold{x} + b
$$

## Stochastic Gradient Descent (SGD) for a Linear Neuron

Stochastic: Random

The cost function $J$ for regression is usually given as the square error between the neuron output and the target

Given a training pattern $(\bold{x}, d)$ we define $J$ as:

$$
J = \frac{1}{2} (d - y)^2
$$

- Note: We add a $\frac{1}{2}$ in front to remove coefficients when differentiating
- $y$ is the output from the neuron, while $d$ is the actual label

Now, using these equations:

$$
\begin{aligned}
J &= \frac{1}{2} (d - y)^2 \\
y &= u = \bold{w}^T \bold{x} + b \\
\end{aligned}
$$

We want to find $\nabla_\bold{w} J$ to minimise the error from the weights,

$$
\begin{aligned}
\frac{\partial J}{\partial y} &= -(d - y) \\
\nabla_\bold{w} J &= \frac{\partial J}{\partial \bold{w}} = \frac{\partial J}{\partial u} \frac{\partial u}{\partial \bold{w}}  \\
u &= \bold{w}^T x + b = \left( \sum_{i = 1}^{n} w_i x_i \right) + b \\
\frac{\partial u}{\partial \bold{w}} &= \begin{pmatrix} \frac{\partial u}{\partial \bold{w_1}} \\ \vdots \\ \frac{\partial u}{\partial \bold{w_n}} \end{pmatrix} = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} = \bold{x} \\
\end{aligned}
$$

Hence, we get:

$$
\begin{aligned}
\nabla_\bold{w} J = -(d - y) \bold{x}
\end{aligned}
$$

Similarly, since $\frac{\partial u}{\partial b} = 1$,

$$
\nabla_b J = \frac{\partial J}{\partial u} \frac{\partial u}{\partial b} = -(d - y)
$$

For SGD, we update the weights based on the gradient we receive

$$
\begin{aligned}
w &\leftarrow w - \alpha \nabla_\bold{w} J = \bold{w} + \alpha (d - y) \bold{x} \\
b &\leftarrow b - \alpha \nabla_b J = b + \alpha (d - y)
\end{aligned}
$$

- $\alpha$ is the learning rate

## SGD Algorithm for Linear Neuron

Given a training set $\{ \bold{x}_p, d_p \}_{p = 1}^{P}$, we set a learning rate parameter $\alpha$. We initialize $\bold{w}$ and $b$ randomly. Then, we repeat until convergence:

For every training pattern $(\bold{x}_p, d_p)$:

$$
\begin{aligned}
y_p &= \bold{w}^T \bold{x}_p + b \\
w &\leftarrow w + \alpha (d_p - y_p) x_p \\
b &\leftarrow b + \alpha (d_p - y_p)
\end{aligned}
$$

# Gradient Descent (GD) for a Linear Neuron

Unlike stochastic gradient descent, we update the weights/biases after processing the whole batch of data

Given a training dataset $\{(\bold{x}_p, y_p)\}_{p=1}^{P}$, the cost function $J$ is given by the sum of square errors

$$
J = \frac{1}{2} \sum_{p = 1}^{P} (d_p - y_p)^2
$$

where $y_p$ is the neuron output for the input pattern $\bold{x}_p$

Then,

$$
\begin{aligned}
\nabla_{\bold{w}} J &= \sum_{p = 1}^{P} \nabla_{\bold{w}} J_p \\
&= - \sum_{p = 1}^{P} (d_p - y_p) \bold{x}_p \\
&= - ((d_1 - y_1) \bold{x}_1 + ... + (d_P - y_P) \bold{x}_P) \\
&= -(\bold{x}_1 \cdots \bold{x}_P) \begin{pmatrix} (d_1 - y_1) \\ \vdots \\ (d_P - y_P) \end{pmatrix} \\
&= - \bold{X}^T (\bold{d} - \bold{y})
\end{aligned}
$$

- $\bold{X} = \begin{pmatrix} \bold{x}_1^T \\ \vdots \\ \bold{x}_P^T \end{pmatrix}$ is the data matrix
- $\bold{d} = \begin{pmatrix} d_1 \\ \vdots \\ d_P \end{pmatrix}$ is the target vector
- $\bold{y} = \begin{pmatrix} y_1 \\ \vdots \\ y_P \end{pmatrix}$ is the output vector

Similarly, $\nabla_b J$ can be obtained by considering inputs of +1, and substituting a vector of +1 instead of $\bold{X}$

$$
\nabla_b J = -\bold{1}_P^T (\bold{d} - \bold{y})
$$

where $\bold{1} = \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix} \in \mathbb{R}^P$

The output vector $y$ for the batch of $P$ patterns is given by:

$$
\bold{y} = \begin{pmatrix} y_1 \\ \vdots \\ y_P \end{pmatrix}
= \begin{pmatrix} \bold{x}_1^T \bold{w} + b \\ \vdots \\ \bold{x}_P^T \bold{w} + b \end{pmatrix} = \begin{pmatrix} \bold{x}_1^T \\ \vdots \\ \bold{x}_P^T \end{pmatrix} \bold{w} + b \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix} = \bold{X} \bold{w} + b \bold{1}_P
$$

Now, we can use the equations to find how to update $\bold{w}$ and $b$

$$
\begin{aligned}
w &\leftarrow w - \alpha \nabla_\bold{w} J \\
b &\leftarrow b - \alpha \nabla_\bold{b} J
\end{aligned}
$$

$$
\begin{aligned}
w &\leftarrow w + \alpha \bold{X}^T (\bold{d} - \bold{y}) \\
b &\leftarrow b + \alpha \bold{1}_P^T (\bold{d} - \bold{y})
\end{aligned}
$$

where $\bold{y} = \bold{X} \bold{w} + b \bold{1}_P$

## GD Algorithm for Linear Neuron

Given a training set $(\bold{X}, \bold{d})$, we set a learning parameter $\alpha$. We initialize $\bold{w}$ and $b$ randomly, then repeat until convergence:

$$
\begin{aligned}
\bold{y} &= \bold{X} \bold{w} + b \bold{1}_P \\
\bold{w} &= \bold{w} + \alpha \bold{X}^T (\bold{d} - \bold{y}) \\
b &= b + \alpha \bold{1}_P^T (\bold{d} - \bold{y})
\end{aligned}
$$

# GD vs SGD for Linear Neurons

| \                           | GD                                                                       | SGD                                                            |
| --------------------------- | ------------------------------------------------------------------------ | -------------------------------------------------------------- |
| Data points used per update | $(\bold{X}, \bold{d})$                                                   | $(\bold{x}_P, d_p)$                                            |
| Cost Function               | $J = \frac{1}{2} \sum_{p = 1}^{P} (d_p - y_p)^2$                         | $J = \frac{1}{2} (d_p - y_p)^2$                                |
| Output                      | $\bold{y} = \bold{Xw} + b \bold{1}_P$                                    | $y_p = \bold{x}_P^T \bold{w} + b$                              |
| Update weights              | $\bold{w} \leftarrow \bold{w} + \alpha \bold{X}^T (\bold{d} - \bold{y})$ | $\bold{w} \leftarrow \bold{w} + \alpha (d_p - y_p) \bold{x}_p$ |
| Update biases               | $b \leftarrow b + \alpha \bold{1}_P^T (\bold{d} - \bold{y})$             | $b \leftarrow b + \alpha (d_p - y_p)$                          |

# Perceptron

Perceptron is a neuron with a **sigmoid** activation function and has an output

$$
y = f(u) = \frac{1}{1 + e^{-u}}
$$

And $u = \bold{w}^T \bold{x} + b$

The square error is used as a cost function for learning. Perceptrons perform **non-linear regression** of inputs

## Minimising Cost Function for Perceptron

The cost function $J$ is given by

$$
J = \frac{1}{2} (d - y)^2
$$

where $y = f(u)$ and $u = \bold{w}^T \bold{x} + b$

The gradient with respect to the synaptic input

$$
\frac{\partial J}{\partial u} = \frac{\partial J}{\partial y} \frac{\partial y}{\partial u} = -(d - y) f'(u)
$$

We know that $\frac{\partial u}{\partial \bold{w}} = \bold{x}$ and $\frac{\partial u}{\partial b} = 1$

$$
\begin{aligned}
\nabla_\bold{w} J &= \frac{\partial J}{\partial \bold{w}} = \frac{\partial J}{\partial u} \frac{\partial u}{\partial \bold{w}} = -(d - y) f'(u) \bold{x} \\
\nabla_b J &= \frac{\partial J}{\partial b} = \frac{\partial J}{\partial u} \frac{\partial u}{\partial b} = -(d - y)f'(u)
\end{aligned}
$$

Gradient learning equations:

$$
\begin{aligned}
\bold{w} &\leftarrow \bold{w} - \alpha \nabla_{\bold{w}} J \\
b &\leftarrow b - \alpha \nabla_b J
\end{aligned}
$$

Substituting gradients:

$$
\begin{aligned}
\bold{w} &\leftarrow \bold{w} + \alpha (d - y) f'(u) \bold{x}  \\
b &\leftarrow b + \alpha (d - y)f'(u)
\end{aligned}
$$

## SGD Algorithm for Perceptron

Given a training set $\{(\bold{x}_p, d_p)\}_{p=1}^{P}$, set a learning parameter $\alpha$. Initialize $\bold{w}$ and $b$ randomly, then repeat until convergence:

For each training pattern $(\bold{x}_p, d_p)$:

$$
\begin{aligned}
u_p &= \bold{w}^T \bold{x}_p + b \\
y_p &= f(u_p) = \frac{1}{1 + e^{-u_p}} \\
\bold{w} &\leftarrow \bold{w} + \alpha (d_p - y_p) f'(u_p) \bold{x}_p \\
b &\leftarrow b + \alpha (d_p - y_p) f'(u_p)
\end{aligned}
$$

## GD Algorithm for Perceptron

Similarly for GD algorithm for linear neuron, we just need to include a $f'(u)$

$$
\begin{aligned}
\nabla_{\bold{w}} J &= -\bold{X}^T (\bold{d} - \bold{y}) f'(\bold{u}) \\
\nabla_{b} J &= -\bold{1}_P^T (\bold{d} - \bold{y}) f'(\bold{u})
\end{aligned}
$$

- $\bold{X} = \begin{pmatrix} \bold{x}_1^T \\ \vdots \\ \bold{x}_P^T \end{pmatrix}$
- $\bold{d} = \begin{pmatrix} d_1 \\ \vdots \\ d_P \end{pmatrix}$
- $\bold{y} = \begin{pmatrix} y_1 \\ \vdots \\ y_P \end{pmatrix}$
- $f'(\bold{u}) = \begin{pmatrix} f'(u_1) \\ \vdots \\ f'(u_p) \end{pmatrix}$

The gradient descent learning is given by

$$
\begin{aligned}
\bold{w} &\leftarrow \bold{w} - \alpha \nabla_{\bold{w}} J \\
b &\leftarrow b - \alpha \nabla_{b} J
\end{aligned}
$$

Substituting the above, we get

$$
\begin{aligned}
\bold{w} &\leftarrow \bold{w} + \alpha \bold{X}^T (\bold{d} - \bold{y}) f'(\bold{u}) \\
b &\leftarrow b + \alpha \bold{1}_P^T (\bold{d} - \bold{y}) f'(\bold{u})
\end{aligned}
$$

Given a training set $(\bold{X}, \bold{d})$, we set a learning parameter $\alpha$, and initialize $\bold{w}$ and $b$ randomly. Repeat until convergence:

$$
\begin{aligned}
\bold{u} &= \bold{Xw} + b\bold{1}_P \\
\bold{y} &= f(\bold{u}) = \frac{1}{1 + e^{-u}} \\
\bold{w} &\leftarrow \bold{w} + \alpha \bold{X}^T (\bold{d} - \bold{y}) f'(\bold{u}) \\
b &\leftarrow b + \alpha \bold{1}_P^T (\bold{d} - \bold{y}) f'(\bold{u})
\end{aligned}
$$

The derivative of the sigmoid function:

$$
\begin{aligned}
f'(u) &= \frac{-1}{(1 + e^{-u})^2} \frac{d (e^{-u})}{d u} \\
&= \frac{e^{-u}}{(1 + e^{-u})^2} \\
&= \frac{1}{1 + e^{-u}} - \frac{1}{(1 + e^{-u})^2} \\
&= y(1 - y)
\end{aligned}
$$

For the $\tanh$ activation function, we can easily find the derivative as well

$$
\begin{aligned}
f(u) &= \frac{e^u - e^{-u}}{e^u + e^{-u}} \\
f'(u) &= \left( 1 - \frac{e^u - e^{-u}}{e^u + e^{-u}} \right) \\
&= 1 - y^2
\end{aligned}
$$

# Classification

Classification is used to identify/distinguish classes or groups of objects

- E.g. identify males from females, rugby players from ballet dancers

Let us consider the problem of identifying rugby players from ballet dancers. We will use 2 distinctive features:

- Height
- Weight

Let $x_1$ denote height, and $x_2$ denote weight. Every individual is represented by a point $\bold{x} = (x_1, x_2)$ in the feature space

## Decision Boundary

The decision boundary of a classifier, $g(x) = 0$, where $g$ is the discriminant function

A classifier finds a decision boundary that separates the 2 classes in the feature space. On one side of the boundary, discriminant function is positive, and negative on the other side

For e.g., if $g(\bold{x}) > 0$, $\bold{x}$ is a ballet dancer. If $g(\bold{x}) \leq 0$, $\bold{x}$ is a rugby player

## Linear Classifier

If 2 classes can be separated by a straight line, the classification is **linearly separable**. For linearly separable classes, one can design a linear classifier

A linear classifier implements discriminant functions or a decision boundary represented by a straight line (hyperplane) in the multi-dimensional feature space. Generally the feature space is multidimensional, and a hyperplane is indicated by a linear sum of coordinates

Given input features $\bold{x} = (x_1 \cdots x_n)^T$, a linear description function is given by

$$
g(\bold{x})= w_0 + w_1 x_1 + ... + w_n x_n
$$

where $\bold{w} = (w_1 \cdots w_n)^T$ are the coefficients/weights, and $w_0$ is the constant term

## Discrete Perceptron as a Linear Two-Class Classifier

The linear discriminant function can be implemented by the synaptic input to the neuron

$$
g(\bold{x}) = u = \bold{w}^T \bold{x} + b
$$

And with the threshold activation function $1(u)$:

$$
u = g(\bold{x}) > 0 \iff y = 1 \\
u = g(\bold{x}) \leq 0 \iff y = 0
$$

That is, a two-class linear classifier (aka dichotomizer) can be implemented with an artificial neuron with a threshold (unit step) activation function (discrete perceptron)

The output (0 or 1) of the binary neuron represents the **label** of the class

## Classification Error

In classification, the error is expressed as the total number of mismatches between the target and output labels

$$
E = \sum_{p=1}^{P} 1(d_p \neq y_p)
$$

## Logistic Regression Neuron

A logistic regression neuron performs binary classification of inputs, classifying inputs into 2 labels (0 or 1)

A logistic regression neuron receives an input $\bold{x} \in \mathbb{R}^n$, and produces a class label $y \in \{0, 1\}$ as the output

The activation function of the logistic regression neuron is the sigmoid

$$
f(u) = \frac{1}{1 + e^{-u}}
$$

where $u = \bold{w}^T \bold{x} + b$ is the synaptic input to the neuron

The activation function of a logistic regression neuron gives the probability that the neuron output belongs to class "1"

$$
P(y = 1 | \bold{x}) = f(u)
$$

Then, we can easily find the probability of the neuron output belonging to class "0":

$$
P(y = 0 | \bold{x}) = 1 - P(y = 1 | \bold{x}) = 1 - f(u)
$$

When $u = 0, f(u) = P(y = 1 | \bold{x}) = P(y = 0 | \bold{x}) = 0.5$. If $f(u) > 0.5, y = 1$, else $y = 0$

## SGD for Logistic Regression

Given a training patter $(\bold{x}, d)$ where $\bold{x} \in \mathbb{R}^n$ and $d \in \{0, 1\}$, the cost function for classification is given by the cross-entropy:

$$
J = -d \log(f(u)) - (1 - d) \log(1 - f(u))
$$

where $u = \bold{w}^T \bold{x} + b$ and $f(u) = \frac{1}{1 + e^{-u}}$

The cost function $J$ is minimized using the gradient descent procedure

$$
J = \begin{cases}
-\log(f(u)) & \text{ if } d = 1 \\
-\log(1 - f(u)) & \text{ if } d = 0
\end{cases}
$$

The gradient with respect to $u$:

$$
\frac{\partial J}{\partial u} = -\left( \frac{d}{f(u)} - \frac{1 - d}{1 - f(u)}\right) f'(u)
$$

Substituting $f'(u) = f(u) (1 - f(u))$ for the sigmoid activation function, we get

$$
\frac{\partial J}{\partial u} = -\frac{d - f(u)}{f(u)(1 - f(u))}f(u)(1 - f(u)) = -(d - f(u))
$$

Then, substituting $\frac{\partial J}{\partial u}$, $\frac{\partial u}{\partial \bold{w}} = \bold{x}$ and $\frac{\partial u}{\partial b} = 1$:

$$
\begin{aligned}
\nabla_{\bold{w}} J &= \frac{\partial J}{\partial u} \frac{\partial u}{\partial \bold{w}} = -(d - f(u))\bold{x} \\
\nabla_b J &= \frac{\partial J}{\partial u} \frac{\partial u}{\partial b} = -(d - f(u))
\end{aligned}
$$

The gradient learning equations are:

$$
\begin{aligned}
\bold{w} &\leftarrow \bold{w} - \alpha \nabla_\bold{w} J \\
b &\leftarrow b - \alpha \nabla_b J
\end{aligned}
$$

Substituting $\nabla_{\bold{w}} J$ and $\nabla_b J$ for the logistic regression neuron, we get:

$$
\begin{aligned}
\bold{w} &\leftarrow \bold{w} + \alpha (d - f(u))\bold{x} \\
b &\leftarrow b + \alpha (d - f(u))
\end{aligned}
$$

## SGD for Logistic Regression Neuron

Given a training dataset $\{(\bold{x}_p, d_p)\}_{p=1}^{P}$, we set a learning rate $\alpha$, and initialize $\bold{w}$ and $b$ randomly. Iterate until convergence:

For every pattern $(\bold{x}_p, d_p)$:

$$
\begin{aligned}
u_p &= \bold{w}^T \bold{x}_p + b \\
f(u_p) &= \frac{1}{1 + e^{-u_p}} \\
\bold{w} &\leftarrow \bold{w} + \alpha (d_p - f(u_p))\bold{x_p} \\
b &\leftarrow b + \alpha (d_p - f(u_p))
\end{aligned}
$$

## GD for Logistic Regression Neuron

Given a training set $\{(\bold{x}_p, d_p)\}_{p=1}^{P}$ where $\bold{x}_p \in \mathbb{R}^n$ and $d_p \in \{0, 1\}$, the cost function for logistic regression is the cross-entropy (negative log-likelihood) over all training patterns

$$
J = - \sum_{p = 1}^{P} d_p \log(f(u_p)) + (1 - d_p) \log (1 - f(u_p))
$$

where $u_p = \bold{w}^T \bold{x}_p + b$ and $f(u_p) = \frac{1}{1 + e^{-u_p}}$

$$
\begin{aligned}
\nabla_{\bold{w}} J &= \sum_{p = 1}^{P} \nabla_\bold{w} J_p \\
&= - \sum_{p=1}^{P} (d_p - f(u_p)) \bold{x}_p \\
&= -((d_1 - f(u_1))\bold{x}_1 + \cdots + (d_p - f(u_p))\bold{x}_p) \\
&= -(\bold{x}_1 \cdots \bold{x}_p) \begin{pmatrix} d_1 - f(u_1) \\ \vdots \\ d_p - f(u_p) \end{pmatrix} \\
&= -\bold{X}^T (\bold{d} - f(\bold{u}))
\end{aligned}
$$

where $\bold{X} = \begin{pmatrix} \bold{x}_1^T \\ \vdots \\ \bold{x}_P^T \end{pmatrix}$, $\bold{d} = \begin{pmatrix}d_1 \\ \vdots \\ d_P \end{pmatrix}$ and $f(\bold{u}) = \begin{pmatrix} f(u_1) \\ \vdots \\ f(u_p) \end{pmatrix}$

For $\nabla_b J$, we can substitute $\bold{X}$ for $\bold{1}_P$:

$$
\nabla_b J = -\bold{1}_P^T (\bold{d} - f(\bold{u}))
$$

and the gradient descent learning algorithm becomes:

$$
\begin{aligned}
\bold{w} &\leftarrow \bold{w} + \alpha \bold{X}^T (\bold{d} - f(\bold{u})) \\
b &\leftarrow b + \alpha \bold{1}_P^T (\bold{d} - f(\bold{u}))
\end{aligned}
$$

## Procedure for GD for Logistic Regression Neuron

Given training data $(\bold{X}, \bold{d})$, we set a learning rate $\alpha$, and initialize $\bold{w}$ and $b$ randomly. Then, iterate until convergence:

$$
\begin{aligned}
\bold{u} &= \bold{Xw} + b\bold{1}_P \\
f(\bold{u}) &= \frac{1}{1 + e^{-u}} \\
\bold{w} &\leftarrow \bold{w} + \alpha \bold{X}^T (\bold{d} - f(\bold{u})) \\
b &\leftarrow b + \alpha \bold{1}_P^T (\bold{d} - f(\bold{u}))
\end{aligned}
$$

## Limitations of Logistic Regression

As long as the neuron is a linear combiner, followed by a non-linear activation function, then regardless of the form of non-linearity used, the neuron can perform pattern classification only on **linearly-separable functions**

Linear separability requires that the patterns to be classified must be sufficiently separated from each other to ensure that the decision boundaries are hyperplanes

![](http://www.statistics4u.com/fundstat_eng/img/hl_classif_separation.png)

# Effects of Learning Rates

- At higher learning rates, convergence is faster, but may not be stable
- Optimal learning rate is the largest rate at which learning does not diverge
- Generally, SGD converges to a better solution (lower error) as it capitalizes on the randomness of data. SGD takes a longer time to converge
- Usually, GD can use a higher learning rate compared to SGD; The time required for one add/multiply computation per weight update is less when patterns are trained in a batch
- In practice, mini-batch SGD is used. Then, the time to train a network is dependent upon
  - Learning rate
  - Batch size

# Local Minima Problem in GD Learning

Algorithm may get stuck in local minimum of error function depending on initial weights. GD gives a suboptimal solution, and does not guarantee the optimal solution

# Generative Adversarial Networks

Unlike discriminative models, which only model $p(y | \bold{x})$, generative models can model $p(y | \bold{x})$ and generate new data/images

# Basics of GANs

![](https://sthalles.github.io/assets/dcgan/GANs.png)

GANs samples a noise $\bold{z} \sim \mathcal{N}(0, 1)$ or $\bold{z} \sim \mathcal{U}(-1, 1)$ and utilises a network generator $G$ to create an image $\bold{x} = G(\bold{z})$

In GANs, we add a discriminator to distinguish whether an image $\bold{x}$ is real or generated

$$
D(\bold{x}) = \begin{cases}
    1 & \bold{x} \text{ is real} \\
    0 & \bold{x} \text{ is generated}
\end{cases}
$$

Generative model $G$:

-   Captures data distribution
-   Generates an image $G(\bold{z})$ such that $D(G(\bold{z})) = 1$ (Discriminator thinks that generated image is real)
-   $\bold{z}$ is some random gaussian/uniform noise, which can be thought as the latent representation of the data

Discriminative model $D$

-   Distinguishes between real and fake samples
-   $D(\bold{x}) = 1$ when $\bold{x}$ is a real image, 0 otherwise

$D$ and $G$ play the following two-player minimax game with the value function $V(G, D)$:

$$
\min_G \max_D V(D, G) = \mathbb{E}_{\bold{x} \sim p_{\text{data}}(\bold{x})} [\log D(\bold{x})] + \mathbb{E}_{\bold{z} \sim p_\bold{z}(\bold{z})} [\log (1 - D(G(\bold{z})))]
$$

# Training

We use mini-batch stochastic gradient descent training of GANs. The number of steps to apply the discriminator, $k$, is a hyperparameter. We use $k = 1$ here, the least expensive option

For a number of training iterations:

1. For $k$ steps:

    - Sample minibatch of $m$ noise samples $\{ \bold{z}^{(1)}, ..., \bold{z}^{(m)} \}$ from noise prior $p_g(\bold{z})$
        - We are sampling from some uniform/gaussian noise vector
    - Sample minibatch of $m$ examples $\{ \bold{x}^{(1)}, ..., \bold{x}^{(m)} \}$ from data generating distribution $p_\text{data}(\bold{x})$
        - We get real images from the training dataset
    - Update the discriminator by **ascending** its stochastic gradient (Note we want to **maximise** the cost function here)

        $$
            \nabla_{\theta_d} \frac{1}{m} \sum_{i=1}^{m} \left[ \log D\left( \bold{x}^{(i)} \right) + \log\left(1 - D\left(G\left(\bold{z}^{(i)}\right)\right)\right) \right]
        $$

        - We find the gradient w.r.t the parameters of the discriminator $\theta_d$
        - We want our discriminator to predict $1$ for any $\bold{x}^{(i)}$ (Real images should be predicted as real (1) by the discriminator)
        - We want our discriminator to predict $0$ for any $G(z^{(i)})$ (Generated images should be predicted as generated (0) by the discriminator)
        - The higher the cost function, the better our discriminator is performing (If our discriminator is perfect, the cost function evaluates to $0$)

2. Now we freeze the discriminator, and train the generator

    - Sample minibatch of $m$ noise samples $\{ \bold{z}^{(1)}, ..., \bold{z}^{(m)} \}$ from noise prior $p_g(\bold{z})$
    - Update the generator by **descending** its stochastic gradient (Note we want to **minimise** the cost function)

    $$
        \nabla_{\theta_g} \frac{1}{m} \sum_{i=1}^{m} \log\left(1 - D\left(G\left(\bold{z}^{(i)}\right)\right)\right)
    $$

    - We find the gradient w.r.t the parameters of the generator $\theta_g$
    - We want our generator to be able to fool the discriminator ($D\left(G\left(\bold{z}^{(i)}\right)\right) = 1$)
    - The lower the cost function, the better our generator is performing (If generator is perfect, the cost function evaluates to $-\infty$)

# Deep Convolutional GAN (DCGAN)

![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-01_at_11.27.51_PM_IoGbo1i.png)

-   Note that most deconvs are batch normalised
    -   Normalise responses to have 0 mean and unit variance over the entire mini-batch
    -   Faster and more stable
-   Fractionally strided convolutions are used

![](https://d3i71xaburhd42.cloudfront.net/f19284f6ab802c8a1fcde076fcb3fba195a71723/25-Figure4.6-1.png)
Fractionally-strided convolutions

-   Zeros are inserted between input units, which make the kernel move around at a slower pace than with unit strides
-   Dashed-white cells are zero rows/columns padded between the input cells (blue), and the resultant convolution is in green

## Other Tricks Proposed by DCGAN

-   Adam optimizer (adaptive moment estimation)
    -   Similar to SGD, but with less parameter tuning
-   Momentum = 0.5 (usually 0.9, but training oscillated and was unstable)
-   Low learning rate: 0.0002

## Sanity Check by Walking on the Manifold

We can interpolate between a series of random points in $\bold{z}$

$$
\bold{z}_{\text{new}} = m \bold{z}_1 + (1 - m) \bold{z}_2
$$

We pass $\bold{z}_\text{new}$ into the generator, and should see a smooth transition between $\bold{z}_1$ and $\bold{z}_2$

![](https://www.researchgate.net/publication/348875360/figure/fig2/AS:1000512908439564@1615552274687/Interpolation-of-the-pose-latent-space.ppm)

## Sanity Check by Vector Space Arithmetic

If we have $\bold{z}_1$ generating a man with glasses, $\bold{z}_2$ generating a man, and $\bold{z}_3$ generating a woman, then $\bold{z} = \bold{z}_1 - \bold{z}_2 + \bold{z}_3$ should generate a woman with glasses

# Advantages/Disadvantages of GANs

We choose GANs because

-   Sampling/generation is straightforward
-   Training does not involve maximum likelihood estimation (finding the best model paramters that fit the training data the most), which is believed to cause poorer quality of images (e.g. blurry images)
-   Robust to overfitting, since generator never sees training data

However,

-   Probability distribution is implicit
    -   Not straightforward to compute $p(\bold{x})$
    -   Vanilla GANs are only good for sampling/generation
-   Training is hard
    -   Non-convergence
    -   Mode-collapse

## Non-convergence

Deep learning models usually involve a single player

-   Player tries to maximise its reward (minimise loss)
-   SGD with backpropagation to find optimal parameters
-   SGD has convergence guarantees (under certain conditions)
-   However, with non-convexity, we might converge to a local optima

$$
    \min_G L(G)
$$

GANs instead involve 2 or more players

-   Discriminator is trying to maximise reward
-   Generator trying to minimise discriminator's reward

    $$
        \min_G \max_D V(D, G)
    $$

-   SGD not designed to find Nash equilibrium of a game
-   Problem: Might not converge to Nash equilibrium at all

## Mode-Collapse

![](https://miro.medium.com/v2/resize:fit:1400/1*DXP3m9l82FHpuBJounBSXg.png)

Generator fails to output diverse samples

-   Generator finds one good image that fools the discriminator, and only generates that one particular image

During mode collapse,

-   Generator produces good samples, but very few
-   Hence, discriminator is unable to tag them as fake

To address this problem,

-   Let discriminator look at the entire batch instead of single examples
-   If there is a lack of diversity, it will mark the examples as fake
-   This way, generator is forced to produce diverse samples

Procedure

1. Extract features that capture diversity in the mini-batch
    - E.g. L2 norm of the difference between all pairs from the batch
2. Feed these features to the discriminator along with the image
3. Feature values will differ between diverse and non-diverse batches
    - Hence, discriminator will rely on these features for classification
4. This in turn
    - Forces generator to match those feature values with real data
    - Will generate diverse batches

## Supervision with Labels

Label inforrmation of the real data might help

-   Instead of discriminator just predicting real/fake, discriminator should predict the classes present in the data, or whether the sample is fake (dog/car/cat + fake)
-   Empirically generates much better samples

# Conditional GANs

Condition the inputs to the generator and discriminator with labels

-   Take an image from the training data $\bold{x}^{(i)}$ and its corresponding label $y^{(i)}$
-   Generate an image using some random noise $\bold{z}^{(i)}$ and a corresponding label $y^{(i)}$
-   Pass image/label pairs into the discriminator, and let discriminator predict whether image is real/fake based on both the input image $\bold{x}^{(i)}$ and its corresponding label $y^{(i)}$
-   Conditions the model on additional information for better multi-modal learning

![](https://salu133445.github.io/dan/figs/cgan.png)

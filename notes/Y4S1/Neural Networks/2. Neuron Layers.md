# Neuron Layers

# Weight Matrix of a Layer

Consider a layer of $K$ neurons. Let $\bold{w}_k$ and $b_k$ denote the weight vector and bias of the $k$-th neuron. Weights connected to a neuron layer is given by a weight matrix

$$
\bold{w} = (\bold{w_1} \ \cdots \ \bold{w}_K)
$$

where columns are given by the weight vectors of indivdual neurons. The bias vector $\bold{b}$ where each element corresponds to the bias of a neuron:

$$
\bold{b} = (b_1, ..., b_K)^T
$$

# Synaptic Input at a Layer for a Single Input

Given an input pattern $\bold{x} \in \mathbb{R}^n$ to a layer of $K$ neurons. The synaptic input $u_j$ to the $j$-th neuron:

$$
u_j = \bold{w}_j^T \bold{x} + b_j
$$

The synaptic input vector $\bold{u}$ to the layer is:

$$
\bold{u} = \begin{pmatrix} u_1 \\ \vdots \\ u_K \end{pmatrix} = \begin{pmatrix} \bold{w}_1^T \bold{x} + b_1 \\ \vdots \\ \bold{w}_K^T \bold{x} + b_K \end{pmatrix} = \bold{W}^T \bold{x} + \bold{b}
$$

- $\bold{W}$ is the weight matrix of the layer
- $\bold{b}$ is the bias vector of the layer

# Synaptic Input to a Layer for Batch Input

Given a set $\{ \bold{x}_p \}_{p = 1}^{P}$ input patterns to a layer of $K$ neurons where $\bold{x}_p \in \mathbb{R}^n$, the synaptic input $\bold{u}_p$ to the layer for an input pattern $\bold{x}_p$:

$$
\bold{u}_p = \bold{W}^T \bold{x}_p + \bold{b}
$$

The synaptic input matrix $U$ to the layer for $P$ patterns:

$$
\bold{U} = \begin{pmatrix} \bold{u}_1^T \\ \vdots \\ \bold{u}_P^T \end{pmatrix} = \begin{pmatrix} \bold{x}_1^T \bold{W} + \bold{b}^T \\ \vdots \\ \bold{x}_P^T \bold{W} + \bold{b}^T \end{pmatrix} = \begin{pmatrix} \bold{x}_1^T \\ \vdots \\ \bold{x}_P^T \end{pmatrix} \bold{W} + \begin{pmatrix} \bold{b}^T \\ \vdots \\ \bold{b}^T \end{pmatrix} = \bold{XW} + \bold{B}
$$

- Note that $(AB)^T = B^T A^T$
- $\bold{B} = \begin{pmatrix} \bold{b}^T \\ \vdots \\ \bold{b}^T \end{pmatrix}$ has bias vector propagated as rows

# Activation at a Layer for Batch Input

The synaptic input to the layer due to a batch of patterns:

$$
\bold{U} = \bold{XW} + \bold{B}
$$

where rows of $U$ corresponds to the synaptic inputs of the layer, corresponding to the individual input patterns.

The activation of the layer:

$$
f(\bold{U}) = \begin{pmatrix} f(\bold{u}_1^T) \\ \vdots \\ f(\bold{u}_P^T) \end{pmatrix} \begin{pmatrix} f(\bold{u}_1)^T \\ \vdots \\ f(\bold{u}_P)^T \end{pmatrix}
$$

where activations due to individual patterns are written as rows

# SGD for Single Layer

$J$ is the cost function. We need to compute gradients $\nabla_{\bold{W}} J$ and $\nabla_{\bold{b}} J$ to learn the weight matrix $\bold{W}$ and bias vector $\bold{b}$

Consider the $k$-th neuron at the layer:

$$
u_k = \bold{x}^T \bold{w}_k + b_k
$$

and

$$
\frac{\partial u_k}{\partial \bold{w}_k} = \bold{x}
$$

The gradient of the cost with respect to the weight connected to the $k$-th neuron:

$$
\begin{aligned}
\nabla_{\bold{w}_k} J &= \frac{\partial J}{\partial u_k} \frac{\partial u_k}{\partial \bold{w}_k} = \bold{x} \frac{\partial J}{\partial u_k} \\
\nabla_{b_k} J &= \frac{\partial J}{\partial u_k} \frac{\partial u_k}{\partial b_k} = \frac{\partial J}{\partial u_k} \\
\end{aligned}
$$

The gradient of $J$ with respect to $\bold{W} = (\bold{w}_1 \ \cdots \ \bold{w}_K)$:

$$
\begin{aligned}
\nabla_{\bold{W}} J &= (\nabla_{\bold{w}_1} J \ \cdots \nabla_{\bold{w}_K} J) \\
&= (\bold{x} \frac{\partial J}{\partial u_1} \ \cdots \ \bold{x} \frac{\partial J}{\partial u_K}) \\
&= \bold{x} (\frac{\partial J}{\partial u_1} \ \cdots \ \frac{\partial J}{\partial u_K}) \\
&= \bold{x} (\nabla_{\bold{u}} J)^T
\end{aligned}
$$

Similarly, by substitution of $\frac{\partial J}{\partial b_k} = \frac{\partial J}{\partial u_k}$, we get

$$
\nabla_{\bold{b}} J = \begin{pmatrix} \frac{\partial J}{\partial b_1} \\ \vdots \\ \frac{\partial J}{\partial b_K} \end{pmatrix} \begin{pmatrix} \frac{\partial J}{\partial u_1} \\ \vdots \\ \frac{\partial J}{\partial u_K} \end{pmatrix} = \nabla_{\bold{u}} J
$$

From these, we can update our weights and biases by doing:

$$
\begin{aligned}
\bold{W} &= \bold{W} - \alpha \bold{x} (\nabla_{\bold{u}} J)^T \\
\bold{b} &= \bold{b} - \alpha \nabla_{\bold{u}} J
\end{aligned}
$$

# GD for Single Layer

Given a set of patterns $\{(\bold{x}_p, \bold{d}_p)\}_{p=1}^{P}$ where $\bold{x}_p \in \mathbb{R}^n$ and $\bold{d}_p \in \mathbb{R}^K$ for regression, and $d_p \in \{1, ..., K\}$ for classification

The cost $J$ is given by the sum of the costs due to individual patterns

$$
J = \sum_{p=1}^{P} J_p
$$

which then

$$
\nabla_\bold{W} J = \sum_{p = 1}^{P} \nabla_\bold{W} J_p
$$

Substituting $\nabla_{\bold{W}} J = \bold{x} (\nabla_{\bold{u}} J)^T$:

$$
\begin{aligned}
    \nabla_\bold{W} J &= \sum_{p = 1}^{P} \bold{x}_p (\nabla_{\bold{u}_p} J_p)^T \\
    &= \sum_{p = 1}^{P} \bold{x}_p (\nabla_{\bold{u}_p} J)^T \\
    &= \bold{x}_1  (\nabla_{\bold{u}_1} J)^T + ... + \bold{x}_P  (\nabla_{\bold{u}_P} J)^T \\
    &= (\bold{x}_1 \ ... \ \bold{x}_P) \begin{pmatrix} (\nabla_{\bold{u}_1} J)^T \\ \vdots \\ (\nabla_{\bold{u}_P} J)^T \end{pmatrix} \\
    &= \bold{X}^T \nabla_\bold{U} J
\end{aligned}
$$

Note that $\bold{X} = \begin{pmatrix} \bold{x}_1^T \\ \vdots \\ \bold{x}_P^T \end{pmatrix}$ and $\bold{U} = \begin{pmatrix} \bold{u}_1^T \\ \vdots \\ \bold{u}_P^T \end{pmatrix}$

Similarly,

$$
\nabla_\bold{b} J = (\nabla_\bold{U} J)^T \bold{1}_P
$$

where $\bold{1}_P = (1, 1, ..., 1)^T$ is a vector of $P$ ones

Hence, the weights and biases can be updated using the following:

$$
\begin{aligned}
\bold{W} &= \bold{W} - \alpha \bold{X}^T \nabla_\bold{U} J \\
\bold{b} &= \bold{b} - \alpha (\nabla_\bold{U} J)^T \bold{1}_P
\end{aligned}
$$

# Perceptron Layer

A layer of perceptrons performs multidimensional non-linear regression and learns a multidimensional non-linear mapping:

$$
\phi: \mathbb{R}^n \to \mathbb{R}^K
$$

# SGD for Perceptron

Given a training pattern $(\bold{x}, \bold{d})$, where $\bold{x} = (x_1, ..., x_n)^T \in \mathbb{R}^n$ and $\bold{d} = (d_1, ..., d_K)^T \in \mathbb{R}^K$, the square-error cost function is given by:

$$
J = \frac{1}{2} \sum_{k = 1}^{K} (d_k - y_k)^2
$$

where $y_k = f(u_k) = \frac{1}{1 + e^{-u_k}}$ and $u_k = \bold{x}^T \bold{w}_k + b_k$

The gradient of $J$ wrt $u_k$:

$$
\begin{aligned}
\frac{\partial J}{\partial u_k} &= \frac{\partial J}{\partial y_k} \frac{\partial y_k}{\partial u_k} \\
&= -(d_k - y_k) f'(u_k)
\end{aligned}
$$

Then,

$$
\begin{aligned}
\nabla_{\bold{u}} J &= \begin{pmatrix} \nabla_{u_1} J \\ \vdots \\ \nabla_{u_K} J \end{pmatrix} \\
&= - \begin{pmatrix} (d_1 - y_1) f'(u_1) \\ \vdots \\ (d_K - y_K) f'(u_K) \end{pmatrix} \\
&= -(\bold{d} - \bold{y}) \cdot f'(\bold{u})
\end{aligned}
$$

Note that $\cdot$ denotes element-wise multiplication

## SGD for Perceptron Layer

Given a training set $\{(\bold{x}, \bold{d})\}$, set the learning parameter $\alpha$, and initialize $\bold{W}$ and $\bold{b}$ randomly. Repeat until convergence:

For every pattern $(\bold{x}, \bold{d})$,

$$
\begin{aligned}
    \bold{u} &= \bold{W}^T \bold{x} + \bold{b} \\
    \bold{y} &= f(\bold{u}) = \frac{1}{1 + e^{-u}} \\
    \nabla_\bold{u} J &= -(\bold{d} - \bold{y}) \cdot f'(\bold{u}) \\
    \bold{W} &= \bold{W} - \alpha \bold{x} (\nabla_\bold{u} J)^T \\
    \bold{b} &= \bold{b} - \alpha \nabla_\bold{u} J
\end{aligned}
$$

## GD for Perceptron Layer

Given a training dataset $\{ (\bold{x}_p, \bold{d}_p) \}_{p=1}^{P}$, where $\bold{x}_p = (x_{p1}, ..., x_{pn})^T \in \mathbb{R}^n$ and $\bold{d}_p = (d_{p1}, ..., d_{pK})^T \in \mathbb{R}^K$

The cost function $J$ is given by the sum of square errors

$$
J = \frac{1}{2} \sum_{p = 1}^{P} \sum_{k = 1}^{K} (d_{pk} - y_{pk})^2
$$

$J$ can be written as the sum of cost due to individual patterns:

$$
J = \sum_{p = 1}^{P} J_p
$$

where $J_p = \frac{1}{2} \sum_{k = 1}^{K} (d_{pk} - y_{pk})^2$ is the square error for the $p$-th pattern

Let

- $\bold{U} = \begin{pmatrix} \bold{u}_1^T \\ \vdots \\ \bold{u}_P^T \end{pmatrix}$
- $\nabla_\bold{U} J = \begin{pmatrix} (\nabla_{\bold{u}_1} J_1)^T \\ \vdots \\ (\nabla_{\bold{u}_P} J_P)^T \end{pmatrix}$

We substitute $\nabla_{\bold{u}} J = -(\bold{d} - \bold{y}) \cdot f'(\bold{u})$

$$
\begin{aligned}
\nabla_{\bold{U}} J &= -\begin{pmatrix} ((\bold{d}_1 - \bold{y}_1) \cdot f'(\bold{u}_1))^T \\ \vdots \\ ((\bold{d}_P - \bold{y}_P) \cdot f'(\bold{u}_P))^T \end{pmatrix} \\
&= -\begin{pmatrix} (\bold{d}_1^T - \bold{y}_1^T) \cdot f'(\bold{u}_1^T) \\ \vdots \\ (\bold{d}_P^T - \bold{y}_P^T) \cdot f'(\bold{u}_P^T) \end{pmatrix} \\
&= -(\bold{D} - \bold{Y}) \cdot f'(\bold{U})
\end{aligned}
$$

where $\bold{D} = \begin{pmatrix} \bold{d}_1^T \\ \vdots \\ \bold{d}_P^T \end{pmatrix}, \bold{Y} = \begin{pmatrix} \bold{y}_1^T \\ \vdots \\ \bold{y}_P^T \end{pmatrix}, \bold{U} = \begin{pmatrix} \bold{u}_1^T \\ \vdots \\ \bold{u}_P^T \end{pmatrix}$

The algorithm for learning gradient is as follows:

Given a training set $(\bold{X}, \bold{D})$, set a learning parameter $\alpha$, then intialize $\bold{W}$ and $\bold{b}$ randomly

Repeat until convergence:

$$
\begin{aligned}
    \bold{U} &= \bold{XW} + \bold{B} \\
    \bold{Y} &= f(\bold{U}) = \frac{1}{1 + e^{-\bold{U}}} \\
    \nabla_\bold{U} &= -(\bold{D} - \bold{Y}) \cdot f'(\bold{U}) \\
    \bold{W} &= \bold{W} - \alpha \bold{X}^T \nabla_\bold{U} J \\
    \bold{b} &= \bold{b} - \alpha (\nabla_\bold{U} J)^T \bold{1}_P
\end{aligned}
$$

# Softmax Layer

![](https://raw.githubusercontent.com/rohan-varma/rohan-blog/gh-pages/images/neuralnet.png)

Softmax is the extension of logistic regression to multiclass classification, known as multinomial logistic regression

Each neuron in the softmax layer corresponds to one class label

- Activation of a neuron gives the probability of the input belonging to that class label
- Output is the class label with the maximum probability

The $K$ neurons in the softmax layer performs $K$ class classification, and represents $K$ classes

Activation of each neuron $k$ estimates the probability $P(y = k | x)$ that the input $\bold{x}$ belongs to class $k$

$$
P(y = k | \bold{x}) = \frac{e^{u_k}}{\sum_{k' = 1}^{K} e^{u_{k'}}}
$$

where $u_k = \bold{w}_k^T \bold{x} + b_k$, and $\bold{w}_k$ is the weight vector, and $b_k$ is the bias of neuron $k$.

The above is the softmax activation function. The output $y$ denotes the class label of the input pattern, which is given by

$$
y = \argmax_k P(y = k | \bold{x})
$$

## SGD for Softmax

Given a training pattern $(\bold{x}, d)$ where $\bold{x} \in \mathbb{R}^n$ and $d \in \{1, ..., K\}$

The cost function for learning is by multiclass cross-entropy

$$
J = - \sum_{k = 1}^{K} 1(d = k) \log (f(u_k))
$$

where $u_k$ is the synaptic input to the $k$-th neuron

The cost function can also be written as

$$
J = - \log (f(u_d))
$$

where $d$ is the target label of input $\bold{x}$

The gradient with respect to $u_k$ is given by:

$$
\frac{\partial J}{\partial u_k} = -\frac{1}{f(u_d)} \frac{\partial f(u_d)}{\partial u_k}
$$

where

$$
\frac{\partial f(u_d)}{\partial u_k} = \frac{\partial d}{\partial u_k} \left( \frac{e^{u_d}}{\sum_{k'=1}^{K} e^{u_{k'}}} \right)
$$

We consider separately the cases where $k = d$ and $k \neq d$

If $k = d$

$$
\begin{aligned}
\frac{\partial f(u_d)}{\partial u_k} &= \frac{\partial}{\partial u_k} \left( \frac{e^{u_k}}{\sum_{k' = 1}^{K} e^{u_{k'}}} \right) \\
&= \frac{e^{u_k} \left( \sum_{k' = 1}^{K} e^{u_{k'}} \right) - e^{u_k} e^{u_k}}{\left( \sum_{k' = 1}^{K} e^{u_{k'}} \right)^2} \\
&= \frac{e^{u_k}}{\sum_{k' = 1}^{K} e^{u_{k'}}} \left( 1 - \frac{e^{u_k}}{\sum_{k' = 1}^{K} e^{u_{k'}}} \right) \\
&= f(u_k) (1 - f(u_k))
\end{aligned}
$$

If $k \neq d$

$$
\begin{aligned}
\frac{\partial f(u_d)}{\partial u_k} &= \frac{\partial}{\partial u_k} \left( \frac{e^{u_d}}{\sum_{k' = 1}^{K} e^{u_{k'}}} \right) \\
&= - \frac{e^{u_d} e^{u_k}}{\left( \sum_{k' = 1}^{K} e^{u_{k'}} \right)^2} \\
&= -f(u_d)f(u_k) \\
\end{aligned}
$$

Combining both together, we can get a single equation:

$$
\frac{\partial f(u_d)}{\partial u_k} = f(u_d)(1(k = d) - f(u_k))
$$

Now, we can calculate the gradient $J$ with respect to $u_k$:

$$
\begin{aligned}
    \nabla_{u_k} J &= \frac{\partial J}{\partial u_k} \\
    &= \frac{\partial J}{\partial f(u_d)} \frac{\partial f(u_d)}{\partial u_k} \\
    &= -\frac{1}{f(u_d)}(f(u_d)(1(d = k) - f(u_k))) \\
    &= -(1(d = k) - f(u_k))
\end{aligned}
$$

And finally, the gradient of $J$ wrt $\bold{u}$ is given by:

$$
\begin{aligned}
\nabla_{\bold{u}} J &= \begin{pmatrix} \nabla_{u_1} J \\ \vdots \\ \nabla_{u_K} J \end{pmatrix} \\
&= -\begin{pmatrix} 1(d = 1) - f(u_1) \\ \vdots \\ 1(d = K) - f(u_K) \end{pmatrix} \\
&= -(1(\bold{k} = d) - f(\bold{u    }))
\end{aligned}
$$

where $\bold{k} = (1, ..., K)^T$

Note that $1(\bold{k} = d)$ is a one-hot vector where the element corresponding to the target label $d$ is "1" else "0". Also, $f(\bold{u}) = \begin{pmatrix} f(u_1) \\ \vdots \\ f(u_K) \end{pmatrix}$

## GD for Softmax

Given a set of patterns $\{ (\bold{x}_p, d_p) \}_{p = 1}^{P}$ where $\bold{x}_p \in \mathbb{R}^n$ and $d_p \in \{1, ..., K\}$, the cost function of the softmax layer is given by the multi-class cross entropy

$$
J = -\sum_{p = 1}^{P} \left( \sum_{k = 1}^{K} 1(d_p = k) \log \left( f(u_{pk})\right)\right)
$$

where $u_{pk}$ is the synaptic input to the $k$-th neuron for input $\bold{x}_p$

The cost function $J$ can also be written as

$$
J = - \sum_{p = 1}^{P} \log f(u_{p d_p})
$$

$J$ can be written as the sum of costs of indivdual patterns:

$$
J = \sum_{p = 1}^{P} J_p
$$

where $J = -\log f(u_{p d_p})$ is the cross-entropy for the $p$-th pattern

$$
\nabla_\bold{U} J = \begin{pmatrix} (\nabla_{\bold{u_1}} J)^T \\ \vdots \\ (\nabla_{\bold{u_P}} J)^T \end{pmatrix} = \begin{pmatrix} (\nabla_{\bold{u_1}} J_1)^T \\ \vdots \\ (\nabla_{\bold{u_P}} J_P)^T \end{pmatrix}
$$

Now, substituting $\nabla_\bold{u} J = -(1(\bold{k} = d) - f(\bold{u}))$

$$
\begin{aligned}
\nabla_{\bold{U}} J &= - \begin{pmatrix} (1(\bold{k} = d_1) - f(\bold{u}_1))^T \\ \vdots \\ (1(\bold{k} = d_P) - f(\bold{u}_P))^T \end{pmatrix} \\
&= -(\bold{K} - f(\bold{U}))
\end{aligned}
$$

where $\bold{K} = \begin{pmatrix} 1(\bold{k} = d_1)^T \\ \vdots \\ 1(\bold{k} = d_P)^T \end{pmatrix}$ is a matrix, with each row being a one-hot vector

# Initialization of Weights

Random initialization is inefficient. At initialisation, it is desirable that weights are small and near zero
- To operate in the linear region of the activation function
- To preserve variance of activations and gradients

Two methods of initialising weights
1. Using a uniform distribution with specified limits
2. Using a truncated normal distribution

## Xavier/Glorat Uniform Initialisation

Uniformly draw weights from a sample $w \sim U(-a, +a)$, where $a = \text{gain} \sqrt{\frac{6}{n_{in} + n_{out}}}$

$n_{in}$ is the number of input nodes, and $n_{out}$ is the number of output nodes. For gain:

Activation | Linear | Sigmoid | Tanh | ReLU | LeakyReLU
--- | --- | --- | --- | --- | ---
gain | 1 | 1 | 5/3 | $\sqrt{2}$ | $\sqrt{\frac{1}{1 + \text{slope}^2}}$

## Truncated Normal Distribution

$$
w \sim \text{truncated\_normal}\left[\text{mean} = 0, \text{std}= \frac{\text{gain}}{\sqrt{n_{in}}}\right]
$$

In the truncated normal, samples that are 2 s.d. away from the center are discarded and resampled again

This is known as Kaiming normal initialisation
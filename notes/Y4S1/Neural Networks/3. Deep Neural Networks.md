# Deep Neural Networks

Recall chain rule for differentiation

$$
\frac{\partial J}{\partial x} = \frac{\partial J}{\partial y} \frac{\partial y}{\partial x} \\
\nabla_x J = \frac{\partial y}{\partial x} \nabla_y J
$$

In multiple dimensions,

- $\bold{x} = (x_1, ..., x_n) \in \mathbb{R}^n$
- $\bold{y} = (y_1, ..., y_K) \in \mathbb{R}^K$
- $J \in \mathbb{R}$
- $\bold{y} = f(\bold{x})$
- $J = g(\bold{y})$

Chain rule states that

$$
\nabla_\bold{x} J = \left( \frac{\partial \bold{y}}{\partial \bold{x}} \right)^T \nabla_\bold{y} J
$$

The matrix $\frac{\partial y}{\partial x}$ is the Jacobian of $f$

$$
\frac{\partial \bold{y}}{\partial \bold{x}} = \begin{pmatrix}
    \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} \cdots & \frac{\partial y_1}{\partial x_n} \\
    \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} \cdots & \frac{\partial y_2}{\partial x_n} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial y_K}{\partial x_1} & \frac{\partial y_K}{\partial x_2} \cdots & \frac{\partial y_K}{\partial x_n}
\end{pmatrix}
$$

# Feedforward Networks (FFN)

FFNs consist of several layers of neurons where activations propagate from input to output. The layers between the input and output layers are called "hidden layers"

The number of layers is the **depth** of the FFN. When a network has many hidden layers of neurons, FFNs are referred to as **deep neural networks** (DNN). Learning in DNNs is referred to as deep learning. The number of neurons in a layer is the **width** of the layer

Hidden layers are usually composed of perceptrons (sigmoidal units) or ReLU units, and the output is usually

- A linear neuron layer for regression
- A softmax layer for classification

## Forward Propagation of Activations: Single Input Pattern

Consider a 2-layer DNN

1. Input $\bold{x} = (x_1, ..., x_n)^T$
2. Hidden layer $\bold{h} = (h_1, ..., h_m)^T$
3. Output layer $\bold{y} = (y_1, ..., y_K)^T$

We also consider

- $\bold{W}, \bold{b}$: The weights and biases of the hidden layer
- $\bold{V}, \bold{c}$: The weights and biases of the output layer

Consider a single input pattern $(\bold{x}, \bold{d})$. The synaptic input $z$ to the hidden layer is

$$
\bold{z} = \bold{W}^T \bold{x} + \bold{b}
$$

Take note that

- $\bold{z} \in \mathbb{R}^m$, corresponding to the width of the hidden layer
- $\bold{x} \in \mathbb{R}^n$, corresponding to the input
- $\bold{W}^T = \begin{pmatrix} w_1^T \\ ... \\ w_m^T \end{pmatrix} \in \mathbb{R}^{m \times n}$
- $\bold{b} \in \mathbb{R}^m$, corresponding to the width of the hidden layer

The output $\bold{h} \in \mathbb{R}^m$ of the hidden layer is

$$
\bold{h} = g(\bold{z})
$$

where $g$ is the hidden layer's activation function

The synaptic input $\bold{u} \in \mathbb{R}^K$ to the output layer is

$$
\bold{u} = \bold{V}^T \bold{h} + \bold{c}
$$

- $\bold{V}^T = \begin{pmatrix} v_1^T \\ ... \\ v_K^T \end{pmatrix} \in \mathbb{R}^{K \times m}$
- $\bold{h} \in \mathbb{R}^m$, as per the previous layer
- $\bold{c} \in \mathbb{R}^K$, corresponding to the width of the output layer

The output $\bold{y} \in \mathbb{R}^K$ on the output layer

$$
\bold{y} = f(\bold{u})
$$

## Backpropagation of Gradients

Since the targets appear at the output, the error gradient at the output layer $\nabla_\bold{u} J$ is known. Therefore, output weights and biases $\bold{V}$ and $\bold{c}$ can be learnt

To learn hidden layer weights and biases, we need to backpropagate the gradients at the output layer to the hidden layer

Consider the synaptic input $u_k$ to the $k$-th neuron at the output layer, with the weight vector $v_k = (v_{k1}, ..., v_{kM})^T$ and bias $c_k$

The synaptic input $u_k$ due to $\bold{h}$ is given by:

$$
\begin{aligned}
u_k &= \bold{v}_k^T \bold{h} +  c_k \\
\frac{\partial u_k}{\partial h_j} &= v_{kj} & \forall j = 1, 2, ..., K
\end{aligned}
$$

Therefore,

$$
\frac{\partial \bold{u}}{\partial \bold{h}} = \begin{pmatrix}
    \frac{\partial u_1}{\partial h_1} & \frac{\partial u_1}{\partial h_2} & \cdots & \frac{\partial u_1}{\partial h_M} \\
    \frac{\partial u_2}{\partial h_1} & \frac{\partial u_2}{\partial h_2} & \cdots & \frac{\partial u_2}{\partial h_M} \\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial u_K}{\partial h_1} & \frac{\partial u_K}{\partial h_2} & \cdots & \frac{\partial u_K}{\partial h_M} \\
\end{pmatrix} = \begin{pmatrix}
    v_{11} & v_{12} & \cdots & v_{1M} \\
    v_{21} & v_{22} & \cdots & v_{2M} \\
    \vdots & \vdots & \ddots & \vdots \\
    v_{K1} & v_{K2} & \cdots & v_{KM} \\
\end{pmatrix} = \bold{V}^T
$$

Now consider

$$
\bold{y} = f(\bold{u})
$$

Considering the $k$-th neuron: $y_k = f(u_k)$,

$$
\frac{\partial \bold{y}}{\partial \bold{u}} = \begin{pmatrix}
    \frac{\partial y_1}{\partial u_1} & \frac{\partial y_1}{\partial u_2} & \cdots & \frac{\partial y_1}{\partial u_K} \\
    \frac{\partial u_2}{\partial u_1} & \frac{\partial u_2}{\partial u_2} & \cdots & \frac{\partial u_2}{\partial u_K} \\
    \vdots & \vdots & \ddots & \vdots \\
    \frac{\partial u_K}{\partial u_1} & \frac{\partial u_K}{\partial u_2} & \cdots & \frac{\partial u_K}{\partial u_K} \\
\end{pmatrix} = \begin{pmatrix}
    f'(u_1) & 0 & \cdots & 0 \\
    0 & f'(u_2) & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & f'(u_K) \\
\end{pmatrix} = diag(f'(\bold{u}))
$$

Considering the output layer

$$
\nabla_\bold{u} J = \begin{cases}
    -(\bold{d} - \bold{y}) & \text{for linear layer} \\
    -(1(\bold{k} = d) - f(\bold{u})) & \text{for softmax layer} \\
\end{cases}
$$

From chain rule,

$$
\nabla_{\bold{h}} J = \left(\frac{\partial \bold{u}}{\partial \bold{h}}\right)^T \nabla_\bold{u} J = \bold{V} \nabla_\bold{u} J
$$

And then

$$
\nabla_\bold{z} J = \left( \frac{\partial \bold{h}}{\partial \bold{z}} \right)^T \nabla_{\bold{h}} J = diag(g'(\bold{z})) \bold{V} \nabla_\bold{u} J = \bold{V} \nabla_\bold{u} J \cdot g'(\bold{z})
$$

- Note the $\cdot$ represents element-wise multiplication

$$
\begin{aligned}
diag(f'(\bold{u})) \bold{x} &= \begin{pmatrix}
    f'(u_1) & 0 & \cdots & 0 \\
    0 & f'(u_2) & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & f'(u_K)
\end{pmatrix} \begin{pmatrix}
    x_1 \\ x_2 \\ \vdots \\ x_K
\end{pmatrix} \\
&= \begin{pmatrix}
    f'(u_1) x_1 \\
    \vdots \\
    f'(u_K) x_K
\end{pmatrix} \\
&= \bold{x} \cdot f'(\bold{u}) \\
&= f'(\bold{u}) \cdot \bold{x}
\end{aligned}
$$

In conclusion

$$
\nabla_\bold{z} J = \bold{V} \nabla_\bold{u} J \cdot g'(\bold{z})
$$

- The gradients at the output layer is multiplied by $\bold{V}$, and backpropagated to the hidden layer
- Note that hidden-layer activations are multiplied by $\bold{V}^T$ in forward propagation ($\bold{u} = \bold{V}^T \bold{h} + \bold{c}$) and the gradients are multiplied by $\bold{V}$ in back-propagation

## SGD for 2-layer FFN

Given a training set $\{(\bold{x}, \bold{d})\}$, set a learning rate $\alpha$, and initialise $\bold{W}, \bold{b}, \bold{V}, \bold{c}$

Repeat until convergence:

For every pattern $(\bold{x}, \bold{d})$

- $\bold{z} = \bold{W}^T \bold{x} + \bold{b}$
- $\bold{h} = g(\bold{z})$
- $\bold{u} = \bold{V}^T \bold{h} + \bold{c}$
- $\bold{y} = f(\bold{u})$
- $\nabla_\bold{u} J = \begin{cases} 
    -(\bold{d} - \bold{y}) & \text{for linear layer} \\
    -(1(\bold{k} = d) - f(\bold{u})) & \text{for softmax layer} \\
\end{cases}$
- $\nabla_\bold{z} J = \bold{V} \nabla_\bold{u} J \cdot g'(\bold{z})$
- $\bold{V} \leftarrow \bold{V} - \alpha \bold{h} (\nabla_\bold{u} J)^T$
- $\bold{c} \leftarrow \bold{c} - \alpha \nabla_\bold{u} J$
- $\bold{W} \leftarrow \bold{W} - \alpha \bold{x} (\nabla_\bold{z} J)^T$
- $\bold{b} \leftarrow \bold{b} - \alpha \nabla_\bold{z} J$

## Forward Propagation of Activations: Batch of Inputs

Synaptic input $\bold{Z}$ to the hidden layer

$$
\bold{Z} = \bold{XW} + \bold{B}
$$

The output $\bold{H}$ of the hidden layer:

$$
\bold{H} = g(\bold{Z})
$$

The synaptic input $\bold{U}$ to the outer layer

$$
\bold{U} = \bold{HV} + \bold{C}
$$

The output $\bold{Y}$ of the output layer:

$$
\bold{Y} = f(\bold{U})
$$

## Backpropagation of Gradients: Batch of Patterns

$$
\nabla_{\bold{U}} J = \begin{cases}
    -(\bold{D} - \bold{Y}) & \text{for linear layer} \\
    -(\bold{K} - f(\bold{U})) & \text{for softmax layer}
\end{cases}
$$

$$
\begin{aligned}
    \nabla_\bold{Z} J &= \begin{pmatrix}
        (\nabla_\bold{z_1} J)^T \\
        \vdots \\
        (\nabla_\bold{z_P} J)^T
    \end{pmatrix} \\
    &= \begin{pmatrix}
        (\bold{V} \nabla_{\bold{u}_1} J \cdot g'(\bold{z}_1)^T) \\
        \vdots \\
        (\bold{V} \nabla_{\bold{u}_P} J \cdot g'(\bold{z}_P)^T)
    \end{pmatrix} \\
    &= \begin{pmatrix}
        (\nabla_{\bold{u}_1} J)^T \bold{V}^T \cdot (g'(\bold{z}_1))^T \\ \vdots \\ (\nabla_{\bold{u}_P} J)^T \bold{V}^T \cdot (g'(\bold{z}_P))^T
    \end{pmatrix} \\
    &= \begin{pmatrix}
        (\nabla_{\bold{u}_1} J)^T \\
        \vdots \\
        (\nabla_{\bold{u}_P} J)^T
    \end{pmatrix} \bold{V}^T \cdot \begin{pmatrix}
        (g'(\bold{z}_1))^T \\
        \vdots \\
        (g'(\bold{z}_P))^T
    \end{pmatrix} \\
    &= (\nabla_\bold{U} J) \bold{V}^T \cdot g'(\bold{Z})
\end{aligned}
$$

## GD for 2-Layer FFN

Given a training set $(\bold{X}, \bold{D})$, set learning parameter $\alpha$, and initialize $\bold{W}, \bold{b}, \bold{V}, \bold{c}$. Repeat until convergence:

Forward propagation:

$$
\begin{aligned}
    \bold{Z} &= \bold{XW} + \bold{B} \\
    \bold{H} &= g(\bold{Z}) \\
    \bold{U} &= \bold{HV} + \bold{C} \\
    \bold{Y} &= f(\bold{U})
\end{aligned}
$$

Backward propagation:

$$
\begin{aligned}
    \nabla_{\bold{U}} J &= \begin{cases}
        -(\bold{D} - \bold{Y}) & \text{for linear layer} \\
        -(\bold{K} - f(\bold{U})) & \text{for softmax layer}
    \end{cases} \\
    \nabla_\bold{Z} J &= (\nabla_\bold{U} J) \bold{V}^T \cdot g'(\bold{Z})
\end{aligned}
$$

Updating parameters:

$$
\begin{aligned}
    \bold{V} &\leftarrow \bold{V} - \alpha \bold{H}^T \nabla_\bold{U} J \\
    \bold{c} & \leftarrow \bold{c} - \alpha (\nabla_\bold{U} J)^T \bold{1}_P \\
    \bold{W} & \leftarrow \bold{W} - \alpha \bold{X}^T \nabla_\bold{Z} J \\
    \bold{b} & \leftarrow \bold{b} - \alpha (\nabla_\bold{Z} J)^T \bold{1}_P
\end{aligned}
$$

# Normalization of Inputs

If inputs have similar variations, a better approximation of inputs or prediction of outputs is achieved. Mainly, there are 2 approaches to normalisation of inputs

Suppose the $i$-th input $x_i \in [x_{i, min}, x_{i, max}]$ and has a mean $\mu_i$ and standard deviation $\sigma_i$

Let $\tilde{x}_i$ denote the normalised input

1. Scaling the inputs such that $\tilde{x}_i \in [0, 1]$:

   $$
       \tilde{x}_i = \frac{x_i - x_{i, min}}{x_{i, max} - x_{i, min}}
   $$

2. Normalising the input to have the same normal distributions $\tilde{x}_i \sim N(0, 1)$

   $$
       \tilde{x}_i = \frac{x_i - \mu_i}{\sigma_i}
   $$

# Normalisation of Outputs

Linear activation function:

- The convergence is usually improved if each output is normalised to have zero mean and unit standard deviation: $\tilde{y}_k \sim N(0, 1)$

  $$
      \tilde{y}_k = \frac{y_k - \mu_k}{\sigma_k}
  $$

Sigmoid activation function:

- If output $y_k \in [y_{k, min}, y_{k, max}]$, the output layer activation function:

  $$
      f(u_k) = \frac{a}{1 + e^{-u_k}} + b
  $$

  - where $a = y_{k, max} - y_{k, min}$
  - $b = y_{k, min}$

Alternatively, you can scale $y_k \in [0, 1]$

# Width of Hidden Layers

- The number of parameters of the network increases with the width of the layers
- Therefore, the network attempts to remember the training patterns with increasing number of parameters
- In other words, the network aims to minimise the training error at the expense of its generalisation ability on unseen data

As the number of hidden units increases, the test error decreases initially, but tends to increase at some point. The optimal number of hidden units is often determined empirically (by trial and error)

# Deep Neural Networks

## Notation

$l$ denotes the layer's index

For the input layer $l = 0$,

- Width = $n$
- Input $\bold{x} \in \mathbb{R}^n$, $\bold{X} = \begin{pmatrix} \bold{x}_1^T \\ \vdots \\ \bold{x}_P^T \end{pmatrix} \in \mathbb{R}^{P \times n}$
  - Note that there are $P$ training data

For hidden layers, $l = 1, 2, ..., L - 1$

- Width of $l$-th layer: $n_l$
- Weight matrix $\bold{W}^l = (\bold{w}_1 \cdots \bold{w}_{n_l}) \in \mathbb{R}^{n_{l - 1} \times n_l}$, bias vector $\bold{b}^l = (b_1, ..., b_{n_l})^T \in \mathbb{R}^{n_l}$
- Synaptic input $\bold{u}^l \in \mathbb{R}^{n_l}, \bold{U}^l = \begin{pmatrix} (\bold{u}_1^l)^T \\ \vdots \\ (\bold{u}_P^l)^T \end{pmatrix} \in \mathbb{R}^{P \times n_l}$
- Activation function $f^l$
- Output $\bold{h}^l \in \mathbb{R}^{n_l}, \bold{H}^l = \begin{pmatrix} (\bold{h}_1^l)^T \\ \vdots \\ (\bold{h}_P^l)^T \end{pmatrix} \in \mathbb{R}^{P \times n_l}$

The output layer $l = L$

- Width = $K$
- Synaptic input $\bold{u}^L \in \mathbb{R}^{n_L}, \bold{U}^L = \begin{pmatrix} (u_1^L)^T \\ \vdots \\ (u_P^L)^T \end{pmatrix} \in \mathbb{R}^{P \times n_L}$
- Activation function $f^L$
- Output $\bold{y} \in \mathbb{R}^L, \bold{Y} = \begin{pmatrix} (\bold{y}_1^L)^T \\ \vdots \\ (\bold{y}_P^L)^T \end{pmatrix} \in \mathbb{R}^{P \times n_L}$
- Desired output $\bold{d}, \bold{D}$

## Forward Propagation: Single Pattern

For an input pattern $(\bold{x}, \bold{d})$

$$
\bold{u}^1 = (\bold{W}^1)^T \bold{x} + \bold{b}^1
$$

For layers $l = 1, 2, ..., L - 1$:

$$
\begin{aligned}
    \bold{h}^l &= f^l(\bold{u}^l) \\
    \bold{u}^{l + 1} &= (\bold{W}^{l + 1})^T \bold{h}^l + \bold{b}^{l + 1}
\end{aligned}
$$

For the output layer,

$$
\bold{y} = f^L(\bold{u}^L)
$$

## Backpropagation of Gradients: Single Pattern

If $l = L$ (in the output layer), the gradients with respect to the synaptic input $\bold{u}$ are:

$$
\nabla_{\bold{u}^L} J = \begin{cases}
    -(\bold{d} - \bold{y}) & \text{for linear layer} \\
    -(1(\bold{k} = d) - f^L(\bold{u}^L)) & \text{for softmax layer}
\end{cases}
$$

Else,

$$
\nabla_{\bold{u}^l} J = \bold{W}^{l + 1} (\nabla_{\bold{u}^{l + 1}} J) \cdot f^l(\bold{u}^l)
$$

- The gradients of layer $l$, $\nabla_{\bold{u}^l} J$, is based off
  1. The gradients of the next layer $l + 1$, $\nabla_{\bold{u}^{l + 1}} J$
  2. The weights of the next layer $\bold{W}^{l + 1}$
  3. The current layer's activation function with the current layer's input $f^l(\bold{u}^l)$

The gradients of the current layer with respect to the weights $\bold{W}$ and biases $\bold{b}$ are:

$$
\begin{aligned}
    \nabla_{\bold{W}^l} J &= \bold{h}^{l - 1} (\nabla_{\bold{u}^l} J)^T \\
    \nabla_{\bold{b}^l} J &= \nabla_{\bold{u}^l} J
\end{aligned}
$$

Gradients are backpropagated from the output to the input layer

## Forward Propagation of Activation in DNN: Batch of Patterns

For an input $(\bold{X}, \bold{D})$,

$$
\bold{U}^1 = \bold{XW}^1 + \bold{B}^1
$$

For layers $l = 1, 2, ..., L - 1$:

$$
\begin{aligned}
    \bold{H}^l &= f^l (\bold{U}^l) \\
    \bold{U}^{l + 1} &= \bold{H}^l \bold{W}^{l + 1} + \bold{B}^{l + 1}
\end{aligned}
$$

At the output,

$$
\bold{Y} = f^L (\bold{U}^L)
$$

## Backpropagation of gradients in DNN: Batch of Patterns

If $l = L$ (at output layer), the gradients of the current layer with respect to the synaptic input $\bold{U}^l$:

$$
\nabla_{\bold{U}^L} J = \begin{cases}
    -(\bold{D} - \bold{Y}) & \text{for linear layer} \\
    -(\bold{K} - f^L(\bold{U}^L)) & \text{for softmax layer}
\end{cases}
$$

Else (at hidden layer):

$$
\nabla_{\bold{U}^l} J = (\nabla_{\bold{U}^{l + 1}} J) (\bold{W}^{l + 1})^T \cdot (f^l)'(\bold{U}^l)
$$

The graidients of layer $l$ with respect to its weights $\bold{W}^l$ and biases $\bold{b}^l$ are:

$$
\begin{aligned}
    \nabla_{\bold{W}^l} J &= (\bold{H}^{l - 1})^T (\nabla_{\bold{U}^l} J) \\
    \nabla_{\bold{b}^l} J &= (\nabla_{\bold{U}^l} J)^T \bold{1}_P
\end{aligned}
$$

Gradients are backpropagated from output to input layer

# Depth of DNN

The deep networks extract features at different levels of complexity for regression or classification. However, the depth (number of layers) that you can have for the networks depend on the number of training patterns available.

- Deep networks have more parameters to learn, hence require more data to train
- Deep networks can learn complex mappings accurately if sufficient training data is available

The optimal number of layers is determined empirically. The optimal architecture minimises the error (training, test and validation)

# Mini-Batch SGD

In practice, gradient descent is performed on mini-batch updates of gradients within a batch or block of data of size $B$. In mini-batch SGD, the data is divided into blocks, and the gradients are evaluated on blocks in an epoch in random order

- $B = 1$: Stochastic gradient descent
- $B = P$ (size of training data): Gradient descent
- $1 < B < P$: Mini-batch stochastic gradient descent

When $B$ increases, more add-multiply operations occur per second, taking advantage of parallelism and matrix computations. On the other hand, as $B$ increases, the number of computations per update (of weights and biases) increases as well

Therefore, the curve of the time for weight update against batch size usually takes a U-shaped curve. There exists an optimal value $B$, which depends on the sizes of the caches as well

## Selection of Batch Size

For SGD, it is deesirable to randomly sample the patterns from training data in each epoch. In order to efficiently sample blocks, the training patterns are shuffled at the beginning of every training epoch, and then blocks are sequentially fetched from memory

Typical batch sizes are in powers of 2: 16, 32, 64, 128, 256

The batch size is dependent on the size of caches for the CPU and GPU

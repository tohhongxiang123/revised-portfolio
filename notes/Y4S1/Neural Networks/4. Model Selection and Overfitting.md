# Model Selection and Overfitting

# Model Selection

In neural networks, there exists several free parameters

-   Learning rate
-   Batch size
-   Number of layers
-   Number of neurons
-   Etc

Every set of parameters of the network leads to a specific model

How do we determine the optimum parameters or model for a given regression/classification problem?

# Performance Estimates

How do we measure the performance of networks? Some metrics include

1. Mean-square/Root-mean-square errors for regression

    $$
    \begin{aligned}
        MSE &= \frac{1}{P} \sum_{p = 1}^{P} \sum_{k = 1}^{K} (d_{pk} - y_{pk})^2 \\

        RMSE &= \sqrt{\frac{1}{P} \sum_{p = 1}^{P} \sum_{k = 1}^{K} (d_{pk} - y_{pk})^2}
    \end{aligned}
    $$

2. Classification error of a classifier

    $$
        E = \sum_{p = 1}^{P} 1(d_p \neq y_p)
    $$

where $d_p$ is the target, and $y_p$ is the predicted output of pattern $p$. $1(.)$ is the indicator function

# True Error vs Apparent Error

**Apparent error** (training error): The error on the training data. What the learning algorithm tries to optimise

**True error**: The error that will be obtained in use (i.e. over the whole sample space). What we want to optimise, but unkonwn

However, the apparent error is not always a good estimate fo the true error. It is just an optimistic measure of it

**Test error** (out of sample error): An estimate of the true error obtained by testing the network on some independent data

Generally, a larger test set helps provide a greater confidence on the accuracy of the estimate

## Estimation of True Error

Intuition: Choose the model with the best fit to the data

Meaning: Choose the model that provides the lowest error rate on teh entire sample population. Of course, that error rate is the true error rate

-   However, to choose a model, we must first know how to estiamte the error of a model
-   The entire sample population is often unavailable and only example data is available

## Validation

In real applications, we only have a finite set of examples

Validation is the approach to use the entire example data avialable to build the model and estimate the error rate. The validation uses a part of the data to select the model, which is known as the validation test

Validation attempts to solve fundamental problems encountered

-   The model tends to overfit the training data. It is not uncommon to have 100% correct classification on training data
-   There is no way of knowing how well the model performs on unseen data
-   The error rate estimate is usually overly optimistic (usually lower than the true error rate). We need an unbiased estimate

## Validation Methods

An effective approach is to split the entire data into subsets (Training/Validation/Test) datasets

Some validation methods

-   The holdout (1/3 - 2/3 rule for test and train partitions)
-   Re-sampling methods
    -   Random subsampling
    -   K-fold cross validation
    -   Leave one out cross validation
-   Three-way data splits (train-validation-test partitions)

### Holdout Method

Split the entire dataset into 2 sets

1. Training set: Used to train the classifier
2. Testing set: Used to estimate the error rate of trained classifier on unseen data samples

The holdout method has 2 basic drawbacks

-   By setting some samples for testing, the training dataset becomes smaller
-   Use of a single train-and-test experiment could lead to misleading estimates if an unfortunate split happens (training data may not cover the space of testing data)

### Random Sampling Methods

Limitations of holdout can be overcome with a family of resampling methods at the expense of more computations

-   Random subsampling
-   K-Fold cross validation
-   Leave one out (LOO) cross-validation

### K-Data Splits Random Subsampling

Random subsampling performs $K$ data splits of the dataset for training and testing

Each split randomly selects a number of samples. For each data split, we retrain the classifier from scratch with the training data. Let the error estiamte obtained for the $i$-th split be $e_i$

The average test error is

$$
E = \frac{1}{K} \sum_{i = 1}^{K} e_i
$$

We choose the model with the lowest average test error

### K-Fold Cross-Validation

Create a $K$-fold partition of the dataset

-   For each of the $K$ experiments, use $K - 1$ folds for training, and the remaining one-fold for testing

Similarly, the corss-validation error is the average of the error estimate for each of the experiments

$$
E = \frac{1}{K} \sum_{i = 1}^{K} e_i
$$

$K$-fold cross validation is similar to random subsampling. The advantage of $K$-fold cross validation is that all examples in the dataset are eventually used for both training and testing

### Leave-One-Out (LOO) Cross-Validation

LOO is the degenerate case of $K$-fold cross validation, where $K$ is chosen as the total number of examples

-   For a dataset with $N$ examples, perform $N$ experiments (i.e. $N = K$)
-   For each experiment, use $N - 1$ examples for training and the remaining one example for testing

The LOO CV error is given by the average CV error over all folds

$$
E = \frac{1}{K} \sum_{i = 1}^{K} e_i
$$

### Three-Way Data Splits Method

If model selection and true error estimates are to be computed simulatneously, the data needs to be divided into 3 disjoint sets

1. Training set: Examples used for learning to fit the parameters of several possible classifiers. For DNN, we would use teh training set to find the optimal weights with gradient descent
2. Validation set: Examples to determine the error $J_m$ of different models $m$, using the validation set. The optimal model $m^*$ is given by

    $$
        m^* = \argmin_m J_m
    $$

3. Training + Validation set: Combine examples used to retrain/redesign model $m^*$, and find the new optimal weights and biases
4. Test set: Examples used only to assess the performance of a trained model $m^*$. We use the test data to estimate the error rate after we have trained the final model with train + validation data

Why separate test and validaiton sets?

-   Error rate estimate of the final model on the validation data will be biased (smaller than true error rate) since the validation set is also used to select in the process of final model selection
-   After assessing the final model, an independent test set is required to estimate the performance of the final model

## Model Complexity

Complex models: Models with many adjustable weights and biases will

-   More likely be able to solve the required task
-   More likely memorise the training data without solving the task (overfitting)

Simple models: The simpler the model, the more likely it is to generalise over the entire sample space. However, this model may not learn the problem well

The fundamental trade-off

-   Too simple: Cannot do the task because not enough parameters to learn (underfitting)
-   Too complex: Cannot generalise from small and noisy datasets well (overfitting)

### Overfitting

The network learns to respond correctly to the training inputs by remembering them too much, but is unable to generalise to produce correct outputs to novel inputs

Overfitting happens when the amount of training data is inadequate in comparison to the number of network parameters to learn

Overfitting occurs when the weights and biases become too large, and are fine-tuned to remember the training patterns too much. Even if the model is correct, too much training can cause overfitting

#### Methods to Overcome Overfitting

1. Early stopping
2. Regularisation of weights
3. Dropouts

Early stopping

-   Training of network is stopped when validation error starts increasing.
-   Early stopping can be used in test/validation by stopping when the validation error is minimum

Regularisation of weights

-   During overfitting, some weights attain very large values to reduce training error, jeopardising its ability to generalise.
-   To avoid this, a penalty term (regularisation term) is added to teh cost function

For a network with weights $\bold{W} = \{ w_{ij} \}$ and bias $\bold{b}$, the penalised (or regularised) cost function $J_1(\bold{W}, \bold{b})$ is defined as

$$
J_1 = J = \beta_1 \sum_{i, j} |w_{ij}| + \beta_2 \sum_{i, j} (w_{ij})^2
$$

-   $J(\bold{W}, \bold{b})$ is the standard cost function (i.e. MSE or cross-entropy)
-   $\sum_{i, j} |w_{ij}|$ is the $L_1$ norm
-   $\sum_{i, j} (w_{ij})^2$ is the $L_2$ norm
-   $\beta_1$ and $\beta_2$ are the $L_1$ and $L_2$ regularisation (penalty) constants. These penalties discourage weights from attaining large values

$L_2$ regularisation of weights

-   Regularisation is usually not applied on bias terms. $L_2$ regularisation is most popular on weights
    $$
        J_1 = J + \beta_2 \sum_{i, j} (w_{ij})^2
    $$
-   The gradient of the regularised cost w.r.t. weights:

    $$
        \nabla_{\bold{W}} J_1 = \nabla_\bold{W} J + \beta_2 \frac{\partial \sum_{i, j} w_{ij}^2}{\partial \bold{W}}
    $$

-   We can easily calculate $\frac{\partial \sum_{i, j} w_{ij}^2}{\partial \bold{W}}$

    $$
        \begin{aligned}
        \sum_{i, j} w_{ij}^2 &= w_{11}^2 + \cdots + w_{nK}^2 \\
        \frac{\partial \sum_{i, j} w_{ij}^2}{\partial \bold{W}} &= \begin{pmatrix}
            \frac{\partial \sum(w_{ij})^2}{\partial w_{11}} & \frac{\partial \sum(w_{ij})^2}{\partial w_{12}} & \cdots & \frac{\partial \sum(w_{ij})^2}{\partial w_{1K}} \\
            \frac{\partial \sum(w_{ij})^2}{\partial w_{21}} & \frac{\partial \sum(w_{ij})^2}{\partial w_{22}} & \cdots & \frac{\partial \sum(w_{ij})^2}{\partial w_{2K}} \\
            \vdots & \vdots & \ddots & \vdots \\
            \frac{\partial \sum(w_{ij})^2}{\partial w_{n1}} & \frac{\partial \sum(w_{ij})^2}{\partial w_{n2}} & \cdots & \frac{\partial \sum(w_{ij})^2}{\partial w_{nK}} \\
        \end{pmatrix} = 2\bold{W}
        \end{aligned}
    $$

-   Now, we can substitute

    $$
        \nabla_{\bold{W}} J_1 = \nabla_\bold{W} J + 2 \beta_2 \bold{W}
    $$

-   For gradient descent, we can use the following

    $$
    \begin{aligned}
        \bold{W} \leftarrow \bold{W} - \alpha \nabla_\bold{W} J_1 \\
        \bold{W} \leftarrow \bold{W} - \alpha (\nabla_\bold{W} J + \beta \bold{W})
    \end{aligned}
    $$

    -   $\beta = 2 \beta_2$
    -   $\beta$ is known as the weight decay parameter

### Dropout

Deep neural networks with large numbers of parameters are powerful learning machines. Overfitting can be avoided by training only a fraction of weights in each iteration. The key idea of dropout is to randomly drop neurons (along with their connections) from the network during training. This prevents neurons from co-adapting, and thereby reduces overfitting

The units (neurons) are present with probability $p$, and presented to the next layer with weights $\bold{W}$ to the next layer at training time.

However in testing, the weights are always present, and presented to the network with weights multiplied by probability $p$ (dropout rate). The output at the test time is same as the expected output during training time

Applying dropout results in a "thinned" network, that consists only of neurons that survived, which minimises redundancy in the network

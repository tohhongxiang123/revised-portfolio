# Convolutional Neural Networks

Consider the first layer of a MLP, taking a 28x28 image as an input, to the next layer with 100 neurons. There are many issues
- Too many parameters: 100 * 28 * 28 + 100. This increases exponentially for even larger images (640 * 480 * 3 image)
  - Fully connected networks are not feasible for signals of large resolutions, and are computationally expensive in feedforward and backpropagation computations
- Spatial organisation of input is destroyed
  - Network is not invariant to transformations such as translation or rotation

To solve this, we use convolutions

For example, in Lenet5:

![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTXoLkh3J1OIt25WEFGVMqaJwqeop7ozxPswYuB0Apq&s)

- As we go deeper (left to right), the height and width goes down, and the number of channels increase
- Common layer arrangement: Conv -> pool -> Conv -> pool -> Fully connected -> Output

## Convolution Layer

We use a filter to go through an image
- Weights of filters are learned using backpropagation
- We convolve the filter with the image (slide over the image spatially and compute dot product)
- The activations are obtained by convolving filters (weights) with input activations. The output activation produced by a particular filter is known as an activation map/feature map
- If we had $n$ filters, we will get $n$ separate activation maps
  - For e.g., if we had a 3x32x32 image, and 6 3x5x5 filters, we will get 6 activation maps, for a "new image" of 6x28x28
  - In general, if we had $N \times D_1 \times H_1 \times W_1$ batch of images, with $D_2 \times D_1 \times F \times F$ filters, then we get $N \times D_2 \times H_2 \times W_2$ batch of activation/feature maps
  - $N$: Number of images in the batch, $D_1 \times H_1 \times W_1$ is the original dimensions of the image ($D_1$ is the number of channels, $H_1$ is the original height, and $W_1$ is the original width)
  - We have $N$ filters, each with dimensions $D_1 \times F \times F$ (Note the first dimension of the filter is equal to the first dimension of the image (number of channels are equal))
  - After running the batch of $N$ images on the batch of $D_2$ filters, we are left with $N \times D_2 \times H_2 \times W_2$, where the dimensions fo the new image is $H_2 \times W_2$

CNN is a sequence of convolutional layers, interspersed with activation functions (CONV -> ReLU -> CONV -> ReLU ...)

Consider a kernel $\bold{w} = \{w(l, m)\}$ which has a size of $L \times M, L = 2a + 1, M = 2b + 1$

$$
\bold{w} = \begin{Bmatrix}
    w(-a, -b) & \cdots & w(-a, 0) & \cdots & w(-a, b) \\
    \vdots & \ddots & \vdots & \ddots & \vdots \\
    w(0, -b) & \cdots & w(0, 0) & \cdots & w(0, b) \\
    \vdots & \ddots & \vdots & \ddots & \vdots \\
    w(a, -b) & \cdots & w(a, 0) & \cdots & w(a, b) \\
\end{Bmatrix}
$$

The synaptic input at location $p = (i, j)$ of the first hidden layer due to a kernel is given by:

$$
u(i, j) = \sum_{l=-a}^{a} \sum_{m=-b}^{b} x(i + l, j + m) w(l, m) + \theta
$$

The output of the neuron at $(i, j)$ of the convolution layer

$$
y(i, j) = f(u(i, j))
$$

where $f$ is an activation function. Typically for deep CNNs, we use ReLU: $f(x) = \max(0, x)$

Note that one weight tensor $\bold{w}_k = \{w_k(l, m)\}$ creates one feature map $\bold{y}_k = \{y_k(i, j)\}$. If there are $K$ weight vectors $(\bold{w}_k)_{k=1}^{K}$, then the convolutional layer is formed by $K$ feature maps $\bold{y} = (\bold{y}_k)_{k=1}^{K}$

### Spatial Dimensions of Convolution Layer

Stride: The factor with which the output is subsampled. That is, the distance between adjacent centers of the kernel
- Stride = 1: Every convolution, shift the kernel 1 to the right
- Stride = 2: Every convolution, shuft the kernel 2 to the right

For an $N \times N$ input image, and an $F \times F$ filter with stride $S$, the output size is $\frac{N - F}{S} + 1$
- For a $32 \times 32$ image with a $5 \times 5$ kernel and stride 1, we get $(32-5)/1 + 1 = 28$, hence $28 \times 28$ activation map
- Note that the feature map is smaller than the input after convolution
- Without zero-padding, the width of the representation shrinks by $F - 1$ at each layer. To avoid rapid shrinking, small filters are used

Zero padding
- By zero-padding in each layer, we prevent the representation from shrinking with depth. In practice, we zero-pad the border
- For example, for a $7 \times 7$ input, with a $3 \times 3$ filter with stride 1, and padded with a 1-pixel border, the output shape is still $7 \times 7$
- For an $N \times N$ image with an $F \times F$ kernel and a $P$-pixel padding, our output size is now $\frac{N - F + 2P}{S} + 1$
- In general, if we want to preserve size spatially (for stride 1), filters of $F \times F$ use a zero-padding with $(F - 1) / 2$

For a $D \times F \times F$ filter, the number of parameters we have is $D \times F \times F + 1$ ($D \times F \times F$ weights, and 1 bias)

### Summary

A convolution layer
- Accepts a volume of size $D_1 \times H_1 \times W_1$
- Requires 4 hyperparameters
  - Number of filters $K$
  - Spatial extent $F$ (size of filter)
  - Stride $S$
  - Amount of zero-padding $P$
- Produces a feature map of size $D_2 \times H_2 \times W_2$, where
  - $D_2 = K$
  - $H_2 = \frac{H_1 - F + 2P}{S} + 1$
  - $W_2 = \frac{W_1 - F + 2P}{S} + 1$
- With parameter sharing, it introduces $F \times F \times D_1$ weights per filter, for a total of $F \times F \times D_1 \times K$ weights, and $K$ biases
- In the output volume, the $d$-th depth slice of size $H_2 \times W_2$ is the result of a valid convolution of the $d$-th filter over the input volume with a stride $S$ and then offset by the $d$-th bias

## Pooling Layer

- Operates over each activation map independently
- Either "max" or "average" pooling is used in the pooling layer. That is the convolved features are divided into disjoint regions, and pooled by taking either the max, or the average
- Pooling is intended to subsample the convolution layer. Default stride for pooling is equal to the filter width

Consider pooling with non-overlapping windows ${(l, m)}_{l, m = -L/2, -M/2}^{L/2, M/2}$ of size $L \times M$

The max pooling output is the maximum of the activaiton inside the pooling window. Pooling of a feature map $\bold{y}$ at $p = (i, j)$ produces a pooled feature:

$$
z(i, j) = \max_{l, m} y(i + l, j + m)
$$

The average pooling output is the mean of activations in the pooling window

$$
z(i, j) = \frac{1}{LM} \sum_l \sum_m y(i + l, j + m)
$$

Why use pooling?
- A function $f$ is invariant to $g$ if $f(g(\bold{x})) = f(\bold{x})$
- Pooling layers are used for building inner activations that are slightly more invariant to small translations in the input
- Invariance to local translation is helpful if we care more about the presence of a pattern rather than its exact position

### Summary

A pooling layer
- Accepts a volume of size $D_1 \times H_1 \times W_1$
- Requires 2 hyperparameters
  - Spatial extent $F$
  - Stride $S$
- Produces a volume of size $D_2 \times H_2 \times W_2$, where
  - $W_2 = (W_1 - F) / S + 1$
  - $H_2 = (H_1 - F) / S + 1$
  - $D_2 = D_1$
- Introduces zero parameters because it computes a fixed function of the input
- Note that is is not common to use zero-padding for pooling layers

## Fully Connected Layer

The fully connected layer (FC) operates on a flattened input where each input is connected to all neurons
- If present, FC layers are usually found towards the end of CNN architectures and can be used to optimize objectives such as class scores

We take a $D \times H \times W$ feature map, and flatten it into a $1 \times (DHW)$ vector

# Training Classifier

We consider multi-class classification

CIFAR-10:
- 10 classes
- 6000 images per class
- 60000 images - 50000 training images, 10000 test images
- Each image is $3 \times 32 \times 32$ (3-channel color image $32 \times 32$)

## Softmax

The softmax step is a generalised logistic function that takes a input vector of scores $\bold{z} \in \mathbb{R}^n$, and outputs a vector of output probability $\bold{p} \in \mathbb{R}^n$ through a softmax function at the end of the architecture

$$
S(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

Softmax's input is the output of the fully connected layer immediately preceding it, and it outputs the final output of the entire neural network. This output is a **probability distribution** of all the label class candidates

## Cross Entropy Loss

For multi-class classification, we use cross entropy loss (multinomial logistic regression)
- Occurs right after the softmax function
- It takes in the input from the softmax function output, and the true label

$$
L(\hat{\bold{y}}, \bold{y}) = - \sum_i y_i \log \hat{y_i}
$$

- Minimum loss: 0 ($1 \log_2 1$)
- Maximum loss tends to infinity ($1 \log_2 0$)

# Data Preprocessing

From the original input data, we normalize the data

$$
X_n = \frac{X - \mu}{\sigma}
$$

Before normalisation, classification loss is very sensitive to changes in the weight matrix, hence hard to optimise. After normalisation, less sensitive to small changes in weights, easier to optimize

There are multiple ways for normalising
- Subtract the mean image (Mean image is [32, 32, 3] array, e.g. AlexNet)
- Subtract per-channel mean (Mean along each channel: 3 numbers, e.g. VGGNet)
- Subtract per-channel mean, and divide by per-channel std (Mean along each channel: 3 numbers, e.g. ResNet)

# Optimisation

A CNN is a composition of functions

$$
f_\bold{w}(\bold{x}) = f_L(...(f_2(f_1(\bold{x}; \bold{w}_1); \bold{w}_2))...; \bold{w}_L)
$$

We have parameters 

$$
\bold{w} = (\bold{w}_1, ..., \bold{w}_L)
$$

The empirical loss function is the mean of the losses of all input data

$$
L(\bold{w}) = \frac{1}{n} \sum_i l(y_i, f_\bold{w}(\bold{x}_i))
$$

We want to learn the weights $\bold{w}^*$ that minimises the loss

$$
\bold{w}^* = \argmin_\bold{w} L(\bold{w})
$$

## Gradient Descent

We iteratively step in the direction of the negative gradient

$$
\bold{w}^{t + 1} = \bold{w}^t - \alpha \frac{\partial f(\bold{w}^t)}{\partial \bold{w}}
$$

Batch gradient descent:
- Take a batch of inputs, calculate the loss based on this batch, and then update the weights accordingly
- Full sum is expensive when the batch size is large

Stochastic Gradient Descent
- Approximate the sum using a minibatch of examples
- 32/64/128 is a common minibatch size
- Additional hyperparameter on the batch size

### GD with Momentum

DNNs have very complex error profiles. The method of momentum is designed to accelerate learning, especially in the face of high curvature, small but consistent gradients, or noisy gradients

When the error function has the form of a shallow ravine, leading to the optimum and steep walls on the side, SGD tends to oscillate near the optimum. This leads to very slow converging rates. A typical deep learning architectural problem

Momentum is a method of speeding convergence along a narrow ravine

![](https://www.scaler.com/topics/images/vanilla_gd.webp)

Momentum update is given by

$$
\begin{aligned}
    \bold{V} &\leftarrow \gamma \bold{V} - \alpha \nabla_\bold{W} J \\
    \bold{W} &\leftarrow \bold{W} + \bold{V}
\end{aligned}
$$

$\bold{V}$ is known as the velocity term, and has the same dimension as the weight vector $\bold{W}$

Momentum parameter $\gamma \in [0, 1]$ indicates how many iterations the previous gradients are incorporated into the current update
- The higher the value of $\gamma$, the slower the gradients decay

Momentum algorithm accumulates an exponentially decaying moving average of past gradients, and continues to move in their direction

Often, $\gamma$ is initially set to 0.1, until learning stabilizes, and increases to 0.9 thereafter


### Learning Rate

Learning rate, often noted as $\alpha$ or sometimes $\eta$ indicates the pace at which the weights are updated
- Can be fixed or adaptively changes
- Low learning rate: Converges very slowly
- High learning rate: Converges quickly, but can easily oscillate around optimum instead of converging

The current most popular method is called **Adam**, which is a method that adapts the learning rate
- Letting the learning rate vary when training a model can reduce the training time and improve the numerical optimal solution
- While Adam optimizer is the most commonly used technique, others can also be useful

Algorithms with adaptive learning rate
- AdaGrad
- RMSprop
- Adam

### Annealing

One way to adapt the learning rate is to use an annealing schedule: To start with a large learning factor, and gradually reduce it

A possible annealing schedule ($t$: The iteration count)

$$
\alpha(t) = \frac{\alpha}{\epsilon + t}
$$

- $\alpha$ and $\epsilon$ are 2 positive constants
- Initial learning rate $\alpha(0) = \frac{\alpha}{\epsilon}$ and $\alpha(\infty) = 0$

### AdaGrad

Adaptive learning rates with annealing usually works with convex cost functions

Learning trajectory of a neural network minimising non-convex cost functions passes through many different structures, and eventually arrives at a region that is locally convex

AdaGrad individually adapts the learning rates of all model parameters, by scaling them inversely proportional to the square root of the sum of all historical squared values of the gradient. This improves learning rates especially in the convex regions of the error function

$$
\begin{aligned}
    \bold{r} &\leftarrow \bold{r} + (\nabla_{\bold{W}} J)^2 \\
    \bold{W} &\leftarrow \bold{W} - \frac{\alpha}{\epsilon + \sqrt{\bold{r}}} \cdot (\nabla_\bold{W} J)
\end{aligned}
$$

In other words, the learning rate

$$
\tilde{\alpha} = \frac{\alpha}{\epsilon + \sqrt{\bold{r}}}
$$

and $\alpha$ and $\epsilon$ are 2 parameters

Notes:
- $\bold{r}$ accumulates the historical squared values of the gradient, hence $\bold{r}_t = \sum_{\tau=1}^{t} (\nabla_\bold{W} J_\tau)^2$
- $\epsilon$ is used to prevent division by zero errors
- $\frac{\alpha}{\epsilon + \sqrt{\bold{r}}} \cdot (\nabla_\bold{W} J)$ refers to element-wise multiplication between $\frac{\alpha}{\epsilon + \sqrt{\bold{r}}}$ and $\nabla_\bold{W} J$
- Fully expanded, 

    $$
        \frac{\alpha}{\epsilon + \sqrt{\bold{r}}} = \alpha \left( \epsilon I + \sqrt{r} \right)^{-1}
    $$

    where $I$ is the identity matrix with dimensions equal to $\sqrt{\bold{r}}$

### RMSprop

RMSprop improves upon AdaGrad algorithms uses an exponentially decaying average to discard the history from the extreme past, so that it can converge rapidly after finding a convex region

$$
\begin{aligned}
    \bold{r} & \leftarrow \rho \bold{r} + (1 - \rho) (\nabla_{\bold{W}} J)^2 \\
    \bold{W} & \leftarrow \bold{W} - \frac{\alpha}{\sqrt{\epsilon + \bold{r}}} \cdot (\nabla_{\bold{W}}J)
\end{aligned}
$$

The decay constant $\rho$ controls the length of the moving average of gradients, with default value $\rho = 0.9$

RMSprops has been shown to be an effective and practical optimisation algorithm for DNNs

### Adam

Adam combines RMSprop and momentum methods. Adam is generally regarded as fairly robust to hyperparameters and works well on many applications

- The momentum term: $\bold{s} \leftarrow \rho_1 \bold{s} + (1 - \rho_1) \nabla_\bold{W} J$
- The learning rate term: $\bold{r} \leftarrow \rho_2 \bold{r} + (1 - \rho_2) (\nabla_\bold{W} J)^2$

$$
\begin{aligned}
    \bold{s} \leftarrow \frac{\bold{s}}{1 - \rho_1} \\
    \bold{r} \leftarrow \frac{\bold{r}}{1 - \rho_2} \\
    \bold{W} \leftarrow \bold{W} - \frac{\alpha}{\epsilon + \sqrt{\bold{r}}} \cdot \bold{s}
\end{aligned}
$$

Note that $\bold{s}$ adds the momentum and $\bold{r}$ contributes to the adaptive learning rate. Suggested default parameters
- $\alpha = 0.001$
- $\rho_1 = 0.9$
- $\rho_2 = 0.999$
- $\epsilon = 10^{-8}$

# Resources

- https://cs231n.github.io/convolutional-networks/#pool
- https://medium.com/konvergen/an-introduction-to-adagrad-f130ae871827
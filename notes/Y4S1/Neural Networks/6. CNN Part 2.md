# CNN Part 2

# CNN Architectures

- As the years pass, CNNs get deeper, with more intricate and different connectivity structures
- Depth is the key to high classification accuracy

## AlexNet

![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_6.35.45_PM.png)

- Split within image is split between 2 GPUs
- Trained for a week on 2 Nvidia GTX 580 3GB GPU
- 60 million parameters
- Input size 227x227x3
- 8 layers deep: 5 convolution and pooling layers, and 3 fully connected layers
- 96 kernel learned by first convolution layer, 48 learned by each GPU
- Escape from a few layers
  - ReLU nonlinearity for solving gradient vanishing
  - Data augmentation
  - Dropout
  - Outperformed all previous models on ILSVRC by 10%

## GoogLeNet

![](https://media.geeksforgeeks.org/wp-content/uploads/20200429201549/Inceptionv1_architecture.png)

- An important lesson: Go deeper
- Inception structures to reduce parameters
- Batch normalisation
  - Normalisation of the activation for each training mini-batch
  - Allows us to use much higher learning rates, and be less careful about initialisation

## VGG

![](https://raw.githubusercontent.com/blurred-machine/Data-Science/master/Deep%20Learning%20SOTA/img/network.png)

- Go deeper
- 140M parameters
- Now commonly used for computing perceptual loss

## ResNet

![](https://miro.medium.com/v2/resize:fit:1200/1*6hF97Upuqg_LdsqWY6n_wg.png)

- Escape from 100 layers
  - Residual learning: Drives the new layer to learn something different
    ![](https://d2l.ai/_images/resnet-block.svg)

# More on Convolution

- Spatial size: $(N - F + 2P) / S + 1$, for
  - $N$: Image dimension
  - $F$: Kernel dimension
  - $P$: Amount of padding
  - $S$: Stride
- Number of parameters: If we had $N$ $D \times F \times F$ kernels, we would have $N(DF^2 + 1)$ parameters
  - $DF^2$ weights per kernel
  - 1 bias per kernel
- How many multiplication operations?
  $$
  u(i, j) = \sum_{l = -a}^{a} \sum_{m = -b}^{b} x(i + l, j + m) w(l, m) + b
  $$
  - For a $D \times F \times F$ filter:
    - We have $DF^2$ multiplications
    - We have $DF^2 - 1$ additions
    - We add once more for the bias
    - Hence, for one channel we require $DF^2 + DF^2 - 1 + 1 = 2DF^2$
  - Recall that a $D_1 \times H_1 \times W_1$ image convolved with a filter of $D_2 \times D_1 \times F \times F$ produces a $D_2 \times H_2 \times W_2$ output
    - The number of FLOPs: $(2DF^2) (D_2 H_2 W_2)$

Therefore, for a filter size $F$, accepting an input volume of $D_1 \times H_1 \times W_1$, producing an output volume of $D_2 \times H_2 \times W_2$, the FLOPs of the convolution layer is given by

$$
FLOPs = [(D_1 F^2) + (D_1 F^2 - 1) + 1] D_2 H_2 W_2 = 2 D_1 F^2 D_2 H_2 W_2
$$

# Pointwise Convolution

![](https://www.researchgate.net/publication/333506478/figure/fig2/AS:801366712778752@1568072121875/A-depthwise-and-a-pointwise-convolutions.jpg)

Using a 1x1 filter size

- Change the size of channels
- Blend information among channels through linear combination
- This is known as inception structure, used in GoogLeNet
- 4M parameters in LeNet vs 60M parameters in AlexNet

# Standard Convolution

- Input and output are locally connected in spatial domain
- In channel domain, they are fully connected

# Depthwise Convolution

![](https://production-media.paperswithcode.com/methods/1_yG6z6ESzsRW-9q5F_neOsg_eaJuoa5.png)

Convolution is performed independently for each input channel

# Combining Convolutions

We have a 3x32x32 image, with 64x3x5x5 filters. This produces a 64x28x28 feature map through standard convolution

- Number of parameters: $64 \times 3 \times 5 \times 5 + 64 = 4864$

We can also have a 3x32x32 image, with 3x1x5x5 filter, doing a depthwise convolution produces a 3x28x28 feature map. Then, use 64x3x1x1 filters on the result and perform pointwise convolution to produce a final 64x28x28 feature map

- Number of parameters = $(3 \times 1 \times 5 \times 5 + 3) + (64 \times 3 \times 1 \times 1 + 64) = 334$

Large parameter reduction

- Computation cost reduction as well, only at a cost of a small reduction in accuracy
- Good to deploy small networks on CPU
- Used in networks such as MobileNet

| Convolution | Filter dimension                   | Size change                                               | Cost                      |
| ----------- | ---------------------------------- | --------------------------------------------------------- | ------------------------- |
| Standard    | $D_2 \times D_1 \times F \times F$ | $D_1 \times W_1 \times H_1 \to D_2 \times W_2 \times H_2$ | $(2 D_1 F^2) D_2 H_2 W_2$ |
| Depthwise   | $D_1 \times 1 \times F \times F$   | $D_1 \times W_1 \times H_1 \to D_1 \times W_2 \times H_2$ | $(2 F^2) D_1 H_2 W_2$     |
| Pointwise   | $D_2 \times D_1 \times 1 \times 1$ | $D_1 \times W_1 \times H_1 \to D_2 \times W_2 \times H_2$ | $(2D_1) D_2 H_2 W_2$      |

Reduction = (cost of depthwise convolution + const of pointwise convolution) / (cost of standard convolution) = $\frac{1}{D_2} + \frac{1}{F^2}$

# Batch Normalisation

Main idea: Normalise the outputs of a layer, so they have zero mean and unit variance

- This allows higher learning rates, reducing the strong dependence on initialisation

Minibatch: A subset of all data during one iteration to compute the gradient

- We have full data of 10000 images
- We split into minibatches of 16 images
- For each iteration, we take 1 batch

For batch normalisation, we want to normalize the values across each training example in the batch, so that the values of the column have 0 mean and unit variance

- $\mu_j = \frac{1}{N} \sum_{i=1}^{N} x_{i,j}$: Per-channel mean, shape is $1 \times D$
- $\sigma_j^2 = \frac{1}{N} \sum_{i=1}^{N} (x_{i,j} - \mu_j)^2$: Per-channel variance, shape is $1 \times D$
- $\hat{x_{i,j}} = \frac{x_{i, j} - \mu_j}{\sqrt{\sigma_j^2 + \epsilon}}$: Normalise values, shape is $N \times D$
- $y_{i, j} = \gamma_j \hat{x_{i, j}} + \beta_j$: We scale and shift normalised values to add flexibility. Output shape is $N \times D$. We have learnable parameters $\gamma$ and $\beta \in \mathbb{R}^{1 \times D}$

Note that $\mu_j$ and $\sigma_j^2$ cannot be done at test-time, only during training time, as these estimates depend on the minibatch

| Batch Normlisation                          | Fully Connected                            | Convolutional                              |
| ------------------------------------------- | ------------------------------------------ | ------------------------------------------ |
| Input size $\bold{x}$                       | $N \times D$                               | $N \times C \times H \times W$             |
| Normalise with $\mu, \sigma, \gamma, \beta$ | $1 \times D$                               | $1 \times C \times 1 \times 1$             |
| $y$                                         | $\gamma  \frac{(x - \mu)}{\sigma} + \beta$ | $\gamma  \frac{(x - \mu)}{\sigma} + \beta$ |

Batch normlisation is usually done after a fully-connected/convolutional layer and before a non-linearity layer

- Zero overhead at test-time, can be fused with conv
- Allows higher learning rates, faster convergence
- Networks becomes more robust to initialisation
- However, not well-understood theoretically (yet)
- Behaves differently during training and testing, very common source of bugs

# Overfitting

Happens when our model is too complex, and too specialised on a small number of training data

- Increase size of data
- Remove outliers in data
- Reduce complexity of model
- Reduce feature dimension

## Transfer Learning

- When trained on images, DNNs tend to learn first-layer features that resemble eitehr Gabor filters or color blobs. These first-layer features are general
- In a network with N-dimensional softmax output layer that has been successfully trained toward a supervised classification objective, each output unit will be specific to a particular class

To do transfer learning:

1. Pre-train on large scale dataset e.g. ImageNet
2. Use pre-trained network as initialisation

- Remove last layer, freeze pre-trained layer

3. Fine-tuning

- Add new layer corresponding to the target class number
- Train on new data

## Data Augmentation

- Random transforms
  - RandomFlip
  - RandomCrop
  - RandomScale
  - Translation, rotation, stretching, shearing, lens distortion, etc
- Color jitter
  - Simple:
    - Randomize contrast and brightness
  - Complex
    - Apply PCA to all R, G, B in training set
    - Sample a color offset along principal component direction
    - Add offset to all pixels in training image

# Attention

- Make predictions by selectively attending to a given set of data
- Amount of attention is quantified by learned weights

$$
\text{Attention} = \bold{WV}
$$

# Transformers

![](https://sebastianraschka.com/images/blog/2023/self-attention-from-scratch/transformer.png)

- Uses attention to model long range dependencies in data, a seq2seq model
- Model of choice in NLP
- Built on self-attention mechanisms, without using sequence-aligned recurrent architecture
  - Self-attention is a type of attention mechanism where the model makes predictions for one part of a data sample, using other parts of the observation about the same sample

Transformer architecture:
![](https://miro.medium.com/v2/resize:fit:1400/1*V2435M1u0tiSOz4nRBfl4g.png)

- Encoding: Stack of encoders
- Decoding: Stack of decoders, same number of encoders and decoders

Each encoder is broken down into 2 sublayers:

1. Self-attention layer
2. Feed forward neural network

Each decoder has 3 sublayers

1. Self-attention layer
2. Encoder-decoder attention
3. Feed-forward neural network

![](https://jalammar.github.io/images/t/Transformer_decoder.png)

## Self-Attention

![](https://production-media.paperswithcode.com/methods/35184258-10f5-4cd0-8de3-bd9bc8f88dc3.png)

Self-attention is known as scaled dot-product attention, and it is defined as:

$$
\text{Attention}(\bold{Q}, \bold{K}, \bold{V}) = \text{Softmax}\left( \frac{\bold{Q}\bold{K}^T}{\sqrt{d_k}} \right) \bold{V}
$$

- Output of self-attention is a weighted sum of the values, where the weights assigned to each value is determined by the dot product between the query and the keys

1. For each of the encoder's input vectors, create 3 vectors: Query $q_i$, key $k_i$, and value $v_i$

   $$
   \begin{aligned}
       \bold{X} &= \begin{pmatrix}
           x_1 \\ \vdots \\ x_t
       \end{pmatrix} \\
       \bold{Q} &= \bold{X} \bold{W}^Q \\
       \bold{K} &= \bold{X} \bold{W}^K \\
       \bold{V} &= \bold{X} \bold{W}^V \\
   \end{aligned}
   $$

2. Calculate the score

   $$
       \bold{Q} \bold{K}^T
   $$

   - Determines how much focus to place on other parts of the input sentence, given a specific word

3. Divide by square root of dimension of the keys $d_k$ (the last dimension of $\bold{W}^K$). This allows more stable gradients

   $$
       \frac{\bold{Q}\bold{K}^T}{\sqrt{d_k}}
   $$

4. Softmax to normalize

   $$
       \text{Softmax}\left( \frac{\bold{Q}\bold{K}^T}{\sqrt{d_k}} \right)
   $$

5. Multiply each vector by softmax score

   $$
       \text{Softmax}\left( \frac{\bold{Q}\bold{K}^T}{\sqrt{d_k}} \right) \bold{V}
   $$

6. Sum up weighted value vectors to get the output for the self-attention layer at this position

Example:

- Assume we have 2 sequences $(1, 2, 3), (4, 5, 6)$
- We have

$$
\bold{W}^Q = \begin{pmatrix}
    0.01 & 0.03 \\ 0.02 & 0.02 \\ 0.03 & 0.01
\end{pmatrix} \
\bold{W}^K = \begin{pmatrix}
    0.05 & 0.05 \\ 0.06 & 0.05 \\ 0.07 & 0.05
\end{pmatrix} \
\bold{W}^V = \begin{pmatrix}
    0.02 & 0.02 \\ 0.01 & 0.02 \\ 0.01 & 0.01
\end{pmatrix}
$$

Compute the output of the scaled-dot product attention

Note, $\bold{X} = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{pmatrix}$

1. Find $\bold{Q}, \bold{K}, \bold{V}$:

   $$
   \begin{aligned}
       \bold{Q} &= \bold{X} \bold{W}^Q = \begin{pmatrix}
           0.14 & 0.10 \\ 0.32 & 0.28
       \end{pmatrix} \\
       \bold{K} &= \bold{X} \bold{W}^K = \begin{pmatrix}
           0.38 & 0.30 \\ 0.92 & 0.75
       \end{pmatrix} \\
       \bold{V} &= \bold{X} \bold{W}^V = \begin{pmatrix}
           0.07 & 0.09 \\ 0.19 & 0.24
       \end{pmatrix}
   \end{aligned}
   $$

2. Find $\text{Attention}(\bold{Q}, \bold{K}, \bold{V})$

   $$
   \begin{aligned}
       \text{Softmax}\left( \frac{\bold{Q}\bold{K}^T}{\sqrt{d_k}} \right) \bold{V} &= \text{Softmax} \left( \frac{\begin{pmatrix}
           0.14 & 0.10 \\ 0.32 & 0.28
       \end{pmatrix} \begin{pmatrix}
           0.38 & 0.30 \\ 0.92 & 0.75
       \end{pmatrix}^T }{\sqrt{2}}\right) \begin{pmatrix}
           0.07 & 0.09 \\ 0.19 & 0.24
       \end{pmatrix} \\
       &= \text{Softmax} \left( \begin{pmatrix}
           0.0588 & 0.1441 \\ 0.1454 & 0.3566
       \end{pmatrix}\right) \begin{pmatrix}
           0.07 & 0.09 \\ 0.19 & 0.24
       \end{pmatrix} \\
       &= \begin{pmatrix}
           0.4784 & 0.5213 \\ 0.4474 & 0.5526
       \end{pmatrix} \begin{pmatrix}
           0.07 & 0.09 \\ 0.19 & 0.24
       \end{pmatrix} \\
       &= \begin{pmatrix}
           0.1326 & 0.1682 \\ 0.1363 & 0.1729
       \end{pmatrix}
   \end{aligned}
   $$

   - Row-wise softmax
   - Note that $\sqrt{d_k} = 2$ ($\bold{W}^K \in \mathbb{R}^{2 \times 2}$)
   - Consider the top left element before softmaxing

     $$
         \frac{e^{z_i}}{\sum_j e^{z_j}} = \frac{e^{0.0588}}{e^{0.0588} + e^{0.1441}} = 0.4787
     $$

## Multi-Head Self-Attention

![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-08_at_12.17.05_AM_st5S0XV.png)

- Run scaled dot product attention multiple times in parallel
- Due to different linear mappings, each head is presented with different versions of keys, queries and values
- Independent attention outputs are simply concatenated and linearly transformed into the expected dimensions
- Allows the model to jointly attend to information from different representation subspaces at different positions
  - E.g. **Deep learning** is a part of **machine learning methods** based on artificial neural networks with **representation learning**
  - Given "representation learning", first head atttends to "Deep learning", second head attends to "machine learning methods"

## Positional Encoding

- Self-attention layers work on a set of vectors, and does not know the order of the vectors it is processing
- To add positional information, we add a positional encoding to each vector
  - Embedding + Positional encoding = Embedding with time signal
- Positional encoding has the same dimension as input embedding

Example of positional encoding:

$$
\begin{aligned}
    PE_{(pos, 2i)} &= \sin (pos / 10000^{2i/d_{model}}) \\
    PE_{(pos, 2i + 1)} &= \cos (pos / 10000^{2i/d_{model}})
\end{aligned}
$$

- Use $\sin$ for even indices, $\cos$ for odd indices

E.g. given the following sinusoidal positional encoding, calculate the $PE(pos=1)$ fo the first 5 dimensions: [0, 1, 2, 3, 4], assume $d_{model} = 512$

Given $pos = 1$, $d_{model} = 512$:

- At dimension 0, $2i = 0 \implies i = 0 \implies PE_{(pos, 2i)} = PE_{(1, 0)} = \sin(1/10000^{0/512})$
- At dimension 1, $2i + 1 = 0 \implies i = 0 \implies PE_{(pos, 2i + 1)} = PE_{(1, 1)} = \cos(1/10000^{0/512})$
- At dimension 2, $2i = 2 \implies i = 1 \implies PE_{(pos, 2i)} = PE_{(1, 2)} = \sin(1/10000^{2/512})$
- At dimension 3, $2i + 1 = 3 \implies i = 1 \implies PE_{(pos, 2i + 1)} = PE_{(1, 3)} = \cos(1/10000^{2/512})$
- At dimension 4, $2i = 4 \implies i = 2 \implies PE_{(pos, 2i)} = PE_{(1, 4)} = \sin(1/10000^{4/512})$

## Transformer Encoder

![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png)

- Each encoder layer has a multi-head self-attention layer, and a simple position-wise fully connected FFN

$$
\text{FFN}(x) = \max(0, xW_1 + b_1) W_2 + b_2
$$

- Linear transformations are same across different positions, they use different parameters from layer to layer
- Each sub-layer adopts a residual connection and a layer normalisation

![](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-19_at_4.24.42_PM.png)
Layer Normalisation

- Pixels along shaded are normalized by the same mean/variance, computed by aggregating the values of these pixels
- Batch normalisation is found to be unstable in transformers
- Layer normalisation works well with RNN, and is now being used in transformers

Outputs of (final) encoder go to the sub-layers of the decoders as well

![](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)

## Transformer Decoder

- Encoder starts by processing input sequence
- Output of last (top) decoder is then transformed into a set of attention vectors $\bold{K}$ and $\bold{V}$
- These are to be used by each decoder in its encoder-decoder attention layer, which helps the decoder focus on appropriate places in the input sequence
- Output of each step is fed to the bottom decoder in the next time step
- Embed and add positional encoding to those decoder inputs. Process those inputs
- Repeat process until a special token (`END`) is reached, indicating the transformer decoder has completed its output
  - In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence
  - This is done by masking future positions (set to -inf) before the softmax step in the self-attention calculation

# Vision Transformer

![](https://miro.medium.com/v2/resize:fit:640/format:webp/0*97Y6wjHcHg5Th-lZ)

- Does not have decoder
- Reshape image $\bold{x} \in \mathbb{R}^{H \times W \times C}$ into a sequence of flattened 2D patches $\bold{x} \in \mathbb{R}^{N \times (P^2 \cdot C)}$
  - $(H, W)$ is the resolution of the original image
  - $C$ is the number of channels
  - $(P, P)$ is the resolution of each image patch
  - $N = HW/P^2$ is the number of resulting patches

For inference using ViT:

1. Prepend a learnable embedding $\bold{z}_0^0 = \bold{x}_{\text{class}}$ to the sequence of embedded patches

   $$
       \bold{z}_0 = [\bold{x}_{\text{class}}; \bold{x}_p^1 \bold{E}; \bold{x}_p^2 \bold{E}; \cdots ; \bold{x}_p^N \bold{E}] + \bold{E}_{\text{pos}}
   $$

   - Patch embedding: Linearly embed each of the patches to $D$ dimension with a trainable linear projection $\bold{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}$
   - Add learnable position embeddings $\bold{E}_{\text{pos}} \in \mathbb{R}^{(N + 1) \times D}$ to retain positional information

2. Feed the resulting sequence of vectors $\bold{z}_0$ to a standard transformer encoder

   $$
   \begin{aligned}
       \bold{z}_0 &= [\bold{x}_{\text{class}}; \bold{x}_p^1 \bold{E}; \bold{x}_p^2 \bold{E}; \cdots ; \bold{x}_p^N \bold{E}] + \bold{E}_{\text{pos}} \\
       \bold{z}'_l &= \text{MSA}(\text{LN}(\bold{z}_{l - 1})) + \bold{z}_{l - 1} \\
       \bold{z}_l &= \text{MLP}(\text{LN}(\bold{z}'_l)) + \bold{z}'_l \\ \\

       \bold{y} &= \text{LN}(\bold{z}^0_L)
   \end{aligned}
   $$

   - $\bold{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}$ is an embedding matrix (project image patch onto embedding)
   - $\bold{E}_{\text{pos}} \in \mathbb{R}^{(N + 1) \times D}$ is a positional embedding
   - $l = 1, ..., L$ is the layer's index (index of encoder inside full transformer encoder)
   - $\text{MSA}$ is multi-self-attention, and $\text{LN}$ is layer normalisation

## Inductive Bias

The inductive bias of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not yet encountered

- ViT has much less image-specific inductive bias than CNNs
- Inductive bias in CNN
  - Locality
  - 2-D neighborhood structure
  - Translation equivariance
- ViT
  - Only MLP layers are local and translationally equivariant. Self-attention layer is global
  - 2D neighborhood is used sparingly
    - Only at the beginning, where we cut image into patches
    - Learnable position embeddings (spatial relations) have to be learnt from scratch

## Performance of ViT

- ViT performs worse than CNN equivalent when trained on ImageNet (1M images) as it overfits
- ViT performs comparably to CNN on ImageNet-21K (14M images), and outperforms CNN on JFT (300M images)
- ViT overfits ImageNet task due to its lack of inbuilt knowledge about images
